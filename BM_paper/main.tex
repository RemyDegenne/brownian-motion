\documentclass[lean]{AFM}

\input{preamble}

\addbibresource{biblio.bib}
\usepackage{tikz}
\usetikzlibrary{arrows, arrows.meta}
% \newcommand{\mathlib}{\lstinline{mathlib}\xspace}
\definecolor{ghpurple}{rgb}{0.4, 0.22, 0.73}

% \hypersetup{
%   colorlinks = true,
%   citecolor = teal,
%   linkcolor = teal,
%   urlcolor = ghpurple
% }

\newcommand{\changeurlcolor}[1]{\hypersetup{urlcolor=#1}}
\newcommand{\lsthref}[2]{\changeurlcolor{ghpurple}{\ttfamily\href{#1}{#2}}}
\newcommand{\leanok}{\qed}
\newcommand{\uses}[1]{Uses #1}
\newcommand{\mathlibok}{}
\renewcommand{\lean}[1]{\verb| #1|}

\lstset{escapeinside={(*}{*)}, language=lean}

\title[Formalization of Brownian motion in Lean]{Formalization of Brownian motion in Lean}
\author[R. Degenne, M. Himmel, D. Ledvinka, E. Marion, P. Pfaffelhuber]{
  Rémy Degenne, Markus Himmel, David Ledvinka, Etienne Marion, Peter Pfaffelhuber}


\authorinfo[R. Degenne]{Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189-CRIStAL, F-59000 Lille, France}{remy.degenne@inria.fr}
\authorinfo[M. Himmel]{University of XYZ, Country}{markus.himmel@example.com}
\authorinfo[D. Ledvinka]{University of XYZ, Country}{david.ledvinka@example.com}
\authorinfo[E. Marion]{University of XYZ, Country}{etienne.marion@example.com}
\authorinfo[P. Pfaffelhuber]{University of Freiburg, Germany}{p.p@stochastik.uni-freiburg.de}

% please fill in
\msc{TODO}
\keywords{Formalization, Mathlib, Lean, Brownian motion}

%update
\VOLUME{TODO}
\YEAR{TODO}
\NUMBER{TODO}
\firstpage{1}
\DOI{TODO}
\receiveddate{TODO}
\finaldate{TODO}
\accepteddate{TODO}


\begin{abstract}
Brownian motion is a building block in modern probability theory. In this paper, we describe a formalization of Brownian motion using the Lean theorem prover. We build on the existing measure-theoretic foundations in Lean's mathematical library, Mathlib, and we develop several key components needed for the construction of Brownian motion, including the Carathéodory and Kolmogorov extension theorems, Gaussian measures in Banach spaces, and the Kolmogorov-Chentsov theorem for path continuity.
\end{abstract}

\begin{document}

\href{https://github.com/RemyDegenne/brownian-motion}{github}

% Example of Lean code:
% \begin{lean}
% /-- A process `X : T → Ω → E` has independent increments if for any `n ≥ 2` and `t₁ ≤ ... ≤ tₙ`,
% the random variables `X t₂ - X t₁, X t₃ - X t₂, ...` are independent. -/
% def HasIndepIncrements {Ω T E : Type*} {mΩ : MeasurableSpace Ω} [Sub E]
%     [Preorder T] [MeasurableSpace E] (X : T → Ω → E) (P : Measure Ω) : Prop :=
%   ∀ n, ∀ t : Fin (n + 2) → T, Monotone t →
%     iIndepFun (fun i : Fin (n + 1) ↦ X (t i.succ) - X (t i.castSucc)) P
% \end{lean}

\section{Introduction}

\subsection{Mathematical background}
\sloppy Brownian motion is arguably one of the most important stochastic processes (e.g.\ \cite{karatzas1991brownian, morters2010brownian}), and is used as a modeling tool across all sciences (physics: e.g.\ \cite{einstein1906theorie, bian2016111}; biology: e.g.\ \cite{erban2014molecular}; finance: e.g.\ \cite{davis2006louis}). Mathematically, Brownian Motion led to Wiener measure \cite{wiener1923differential}, which is the first instance of a probability measure on a function space, but also to developments such as Stochastic (Partial) Differential equations (see e.g.\ \cite{hairer2009introduction}).

Let us briefly describe Brownian motion $X = (X_t)_{t\geq 0}$. There are several interesting features (considering the one-dimensional version and if $X_0=0$):
\begin{itemize}
\item It has a normal distirbution at all times; more precisely, for any $t_1,...,t_n > 0$, we have that $(X_{t_1},...,X_{t_n})$ has a multivariate normal distribution, with $\mathbf E[X_{t_i}] = 0$ and $\mathbf{COV}(X_{t_i}, X_{t_j}) = t_i \wedge t_j$ (in other words, $X$ is a Gaussian process);
\item Its states after time $t$ are independent of states before $t$ given $X_t$, or $\mathbb P(X_{t+s} \in . | (X_r)_{r\leq t}) = \mathbb P(X_{t+s} \in . | X_t)$ for $s,t \ge 0$, i.e.\ Brownian Motion is a Markov process;
\item It has independent increments, which only depend on the evolved time, i.e.\ $X_s$ and $X_t - X_s$ are independent, and $X_t - X_s$ is a function of $t-s$, i.e.\ it is a Lévy process;
\item It describes a fair game (usually denoted a martingale), i.e.\ $\mathbb E[X_{t+s} | (X_r)_{r\leq t}] = X_t$ for $s,t \geq 0$; interestingly, it is the only stochastic process with continuous paths such that $(X_t)_{t\geq 0}$ and $(X_t^2 - t)_{t\geq 0}$ are martingales;
\item It has continuous paths (almost surely); more precisely, it has Hölder-continuous paths for any coefficient $\beta < \tfrac 12$; as a consequence, its paths are nowhere differentiable, almost surely;
\end{itemize}
The goal of the present paper is to describe a formalization project of Brownian Motion using Lean \cite{lean}, building on its mathematical library. Since Mathlib \cite{mathlib} has a decent amount of measure theory already included, but is still lacking some fundamentals, we describe our way to a full formalization using the following steps:
\begin{itemize}
\item {\em The Carathéodory and Kolmogorov Extension Theorems:} Constructing a probability measure on an uncountable infinite product (i.e.\ the distribution of a stochastic process) is not straight-forward, since the corresponding product of $\sigma$-algebras is too large. Rather, one uses the $\sigma$-algebra defined by all finite (or countably infinite) projections, and constructs a probability measure by extension from all finite dimensional possibilities, which is the content of the Kolmogorov Extension Theorem. This result is in fact an application of a more general result, the Carathèodory Extension Theorem. In this step, we need that the family of finite dimensional distirbutions has a consistency property (i.e.\ it forms a projective family).
\item {\em Gaussian Measures and characteristic functions:} Defining a one-dimensional normal distribution is straight-forward using its density with respect to Lebesgue measure, and concrete calulations are often possible using the density. In the multi-dimensional case, which we have to work with extensively, it is often much more convenient to use characteristic functions, which are known to characterize probability measures uniquely. In particular, they can be used to show that linear maps (e.g.\ projections) of Gaussian measures are Gaussian. This approach is fundamental to show that finite dimensional distirbutions of Brownian Motion indeed form a projective family.
\item {\em The Kolmogorov-Chentsov Theorem:} We are aiming for a stochastic process with continuous paths. The classical Kolmogorov Chentsov theorem gives a criterion in terms of some moment bounds if the set of times is a subset of $\mathbb R^d$; see e.g.\ \cite{kallenberg2021}. Here, we use a modern version for existence of a modification of a stochastic process with continuous paths, based on a more general set of times \cite{kratschmer2023kolmogorov}.
\item {\em Construction of Brownian motion and Wiener measure on $\R_+$:} Putting all of the above together gives a continuous Gaussian stochastic process with the correct distribution, i.e.\ Brownian Motion. From this, we can also define a probability measure on (the Polish space) of continuous functions $\mathbb R_+ \to \mathbb R$. This is usually called the Wiener measure.
\end{itemize}
It is important to realize that probability theory comes with a duality between random variables (or stochastic processes) and their distributions (or laws), which are probability measures. In the case of stochastic processes, the latter are defined on the (often uncountable infinite) product on the state space. While some properties of a stochastic process are only dependent on their distribution (e.g.\ being a Gaussian process), other properties depend on more refined properties (e.g.\ having continuous paths). We account for this duality in our code by introducing in the assumptions that a certain law is required.

\subsection{Related work}
In the literature, there are some formalizations of stochastic processes. Within Isabelle/HOL, the Kolmogorov extension theorem has been previously formalized \cite{Immler2012}. This formalization only works on Polish spaces (rather than on spaces where every finite measure is inner regular with respect to compact sets, see below), and only in the case where the state spaces for all times are identical. As for the Kolmogorov-Chentson-Theorem (showing continuity of paths), there is an Isabelle/HOL formalization as well \cite{Kolmogorov_Chentsov-AFP}. It uses the index set  $\R_+$ (rather than a more general type) with the usual dyadics proof, as e.g. outlined in \cite{kallenberg2021}. Last,
Brownian motion in implemented in Isabelle/HOL at \cite{laursen2024brownian} (the code is on \href{https://github.com/cplaursen/Brownian_Motion}{github}). As the authors say, dependent type theory would have facilitated their work in places.

For the Lean theorem prover, \cite{ying2023formalization} formalized martingales (see e.g.\ \cite{kallenberg2021}) and was the first implementation of stochastic processes. In our work, however, we do not touch martingales, and rather build on implementations of measure theory, as implemented in \cite{mathlib}. We benefitted from recent efforts to write code and a description of the proofs using the blueprint tool \cite{Monticone_LeanProject_2025}.

\section{The formalization}
Let us start with our notation. Any stochastic process has an index set $\iota$ (usually denoted {\em time}, usually uncountably infinite), and a state space. Since we use dependent type theory, the state space may depend on the time, i.e.\ the state space at time $t$ is $\alpha t$. Probability measures are denoted $P_.$.

All references to results in \lstinline|mathlib| are accurate for commit {\tt xxx}, August 22, 2025.

\subsection{The Carathéodory and Kolmogorov Extension Theorems}
The usual approach to construct (the distribution of) a stochastic process works as follows: describe properties of the distribution of the stochastic process $P_J$ at some arbitrary but finite number of times $J = \{t_1,...,t_n\} \subseteq \iota$.
The resulting family of probability measures $(P_J)_{J \subseteq \iota \text{ finite}}$ has to be {\em projective} in the sense that the projection of $P_J$ to $H\subseteq J$ has to be equal to $P_H$. In other words, when describing the distribution of the stochastic process at all times in $J$, and then forgetting all properties for times in $J\setminus H$, results in the description of properties at times in $H$. One may then ask if this already gives a complete description of the process for all times.

It is the achievement of Kolmogorov that these finite-dimensional distributions in fact provide a unique description of the distribution of a stochastic process, even if $\iota$ is uncountable, as long as the underlying family of state spaces $(\alpha_t)_{t\in\iota}$ is nice enough (Polish, i.e.\ a separable topological space which can be metrized by a complete metric, for example) \cite{kolmogoroff1933grundbegriffe}.
The resulting measure is defined on the product-$\sigma$-field $\mathcal F :=\bigotimes_{t\in\iota} \mathcal B(\alpha_t)$ (where $\mathcal B(\alpha_t)$ is the Borel $\sigma$-algebra on $\alpha_t$). Here, $\mathcal F$ is generated by finite projections and hence any element of $\mathcal F$ may only depend on at most countably many $t\in \iota$, making this a rather coarse $\sigma$-algebra. (In particular, note that this is not the Borel $\sigma$-algebra of the product topology for infinite $\iota$.)

The Kolmogorov extension theorem is on the interface between measure theory and probability theory. Here, we rely on a decent amount of formalized mathematics in the measure-theory part of {\tt mathlib} (outer measures, above all), while not requiring any specific previous formalization of probability theory. (In fact, most of our results are formulated in terms of finite rather than probability measures.)

We are going to formulate the main result in a modern fashion, as e.g.\ found in Theorem 2.2 of \cite{rao1971projective}, Theorem 7.7.1 of Volume~2 of \cite{bogachev2007measure}, Theorem 15.26 of \cite{guide2006infinite}, or \cite{border1998expository}. Note that these formulations split general assumptions on the underlying space(s) (e.g.\ a metric property) from the property which is needed in the proof (inner regularity with respect to compact sets). Other -- highly readable -- references such as \cite{Billingsley1995} state the extension theorem only in special cases such as $\alpha_t = \mathbb R$ for all $t$.


\subsection{Gaussian Measures and characteristic functions}
\label{ss:char}
Our goal in this subsection is to define the finite-dimensional distributions of Brownian Motion. For times $t := (t_1, ..., t_n)$, this is given by the multi-dimensional Gaussian distribution $N(0, C_t)$, where $0$ is the vector of expectations, and $C_t$ is the covariance matrix, given by $C_{ij} = t_i \wedge t_j$. In order to do so, we rely on (i) the implementation of the one-dimensional normal distribution (xxx ref to mathlib) and characteristic functions of probability (or finite) measures.

For any probability measure $P$ on $\mathbb R^n$, its characteristic function is given by $\psi: t \mapsto \int e^{it^\top x} P(dx)$, where the integral takes values in $\mathbb C$. We use the fact that $\psi$ characterizes $P$ uniquely (xxx ref to mathlib). For the standard normal distribution $N(0,1)$ this is $\psi_{N(0,1)}(t) = \exp(-t^2/2)$. Moreover, by independence, we can as well define the $n$-fold product measure $N(0, I_n)$, where $I_n$ is the unit matrix with characteristic function $\psi_{N(0,I_n)}(t) = \exp\big(-\tfrac 12 t^\top I_n t\big)$. In addition, we call a probability measure $P$ on $\R^n$ Gaussian if there is some non-negative definite matrix $C$ with $\psi_P(t) = \exp\big( - \tfrac 12 t^\top C t\big)$. For any such $C$, such a measure exists, since there is $A$ with $C = A^\top A$ (xxx ref to mathlib) and there is the image of $N(0,I_n)$ under the map $f : x\mapsto Ax$. It has the caracteristic function
\begin{align} \label{eq:gausslin}
\psi_{f_\ast N(0,I_n)}(t) = \int e^{it^\top x} f_\ast N(0,I_n) dx = \int e^{it^\top A x} N(0,I_n) dx = \exp\big( - \tfrac 12 t^\top C t\big).
\end{align}
Using the same transformation, we can show that Gaussian measures are closed under linear maps.

So, we can define the finite dimensional distributions of Brownian motion given that we show that $C \in \R^n$ with entries $C_{ij} = t_i \wedge t_j$ is non-negative definite. For this, we rely on Gram matrices, which are based on inner product spaces. In such a space $E$ (with scalar product $\langle .,. \rangle$), and $v_i,...,v_n \in E$, define $C_{ij} := \langle v_i, v_j\rangle$. Then, for $t :=(t_1,...,t_n) \in \R^n$,
$$ t^\top C t = \sum_{i,j} t_i \langle v_i, v_j\rangle t_j = \Big\langle \sum_i t_i v_i, \sum_j t_j v_j\Big\rangle \geq 0.$$
Let $v_i = 1_{[0,t_i]}$ in the space of $L^2$ integrable functions with respect to Lebesgue integral. Then,
$$ \langle v_i, v_j \rangle = \int 1_{[0,t_i]}(x) 1_{[0,t_j]}(x) dx = \int 1_{[0,t_i \wedge t_j]}(x) dx = t_i \wedge t_j.$$
So, $C$ from above is a Gram matrix, which is non-negative definite by general theory, and we have constructed the finite dimensional distributions for Brownian motion.

\cite{hairer2009introduction}

\subsection{The Kolmogorov-Chentsov Theorem}
Let us describe briefly the Kolmogorov-Chentsov Theorem in a simple case here, before we dive deeper in the next sections. The result states that for a stochastic process $X$ (with $\iota = [0,1])$, assume we find $\alpha, \beta, C > 0$ satisfying
\begin{align}
\label{eq:cs}
  \mathbf E[r(X_s, X_t)^\alpha] \leq C|t-s|^{\beta + 1}, \qquad 0\leq s,t\leq 1.
\end{align}
Then there exists $Y = (Y_t)_{t\in [0,1]}$ with $\mathbf P(X_t = Y_t) = 1$ for all $t\in [0,1]$ and $Y$ has almost surely Hölder continuous paths with coefficient $\gamma$ for any $\gamma < \tfrac \beta \alpha$.

In order to see this, set $D_n := \{k/2^n: k=0,...,2^n\}$ and $D := \bigcup_{n\in\mathbb N} D_n$. Start by showing summability of $\mathbf P\Big( \sup_{s,t\in D_n, |t-s| = 2^{-n}} r(X_s, X_t) \geq 2^{-\gamma n} \Big)$ using \eqref{eq:cs}. By the Borel-Cantelli Lemma, this shows that $\sup_{s,t\in D_n, |t-s| = 2^{-n}} r(X_s, X_t) \leq 2^{-\gamma n}$ holds for $n$ large enough. From this, we see that $X$ is locally Hölder-$\gamma$ continuous on $D$. From here, define some Hölder continuous $Y$ coinciding with $X$ on $D$. Finally, fix $t \in [0,1]$ and $t_1, t_2,...\in D$ with $t_n \xrightarrow{n\to\infty} t$. Using \eqref{eq:cs}, we see that $X_{t_n} \xrightarrow{n\to\infty} X_t$ in probability, as well as $Y_{t_n} \xrightarrow{n\to\infty} Y_t$ almost surely due to continuity of $Y$. Therefore, $X_t = Y_t$ almost surely.

We will use a more general version of this statement, replacing $\iota = [0,1]$ by a metric space with a property restricting the number of balls needed to cover $\iota$. This is based on recent work of \cite{talagrand2022upper} and \cite{kratschmer2023kolmogorov}.

\subsection{Construction of Brownian motion and Wiener measure on $\R_+$}
In order to finally construct Brownian Motion, we need to put everything together. First, the projectivity property of the finite-dimensional distributions as defined in Section~\ref{ss:char} follows from the fact that Gaussian measures are closed under linear maps, and -- as in \eqref{eq:gausslin} -- the characteristic function of the image of $N(0,C)$ under the linear map $\Pi : x \mapsto \pi x$ is $\Psi_{\Pi_\ast N(0,C)}(t) = \exp\Big( -\tfrac 12 t^\top \pi^\top C \pi t \Big)$. So, for $t = (t_1,...,t_n)$ and $C(t) \in \mathbb R^{n\times n}$ with $C(t)_{ij} = t_i \wedge t_j$ and the projection $\Pi: \mathbb R^n \to \mathbb R^m$ with $m < n$ and $\Pi(t) = (s_1,...,s_m)$, this shows that $\Pi_\ast N(0,C(t)) = N(0,C(s))$, which is the desired projectivity. Therefore, we have constructed a probability measure on $\mathbb R_+^{\mathbb R}$, the law of Brownian motion.\\
Finally, the assumption in the Kolmogorov-Chentsov Theorem can e.g.\ be verified by using that
\[ \mathbf E[|X_t - X_s|^4] = \mathbf E[X_{t-s}^4] = (t-s)^2 \mathbf E[X_{1}^4] < \infty.\]
So, we have constructed a process with continuous paths and the correct finite dimensional distributions, which we call Brownian Motion. Since this also gives a distribution on $\mathcal C(\mathbb R_+, \mathbb R)$, we call this distribution Wiener measure.

\section{The Carathéodory and Kolmogorov Extension Theorems}
\label{S:extension}
We describe our implementation of the Kolmogorov Extension Theorem, and rely on basic notions from topology, such as a metric space, and teh Borel $\sigma$-algebra. Recall, however, that a pseudo-metric $r(.,.)$ can have $r(x,y) = 0$ for $x\neq y$.


\begin{theorem}[\boldmath $\mu$-measurable sets are a
    $\sigma$-algebra]\label{T:cara} Let $\mu$ be an outer measure on
  $E$ and $\mathcal F$ the set of $\mu$-measurable sets,
  i.e.\ the set of sets $A$ satisfying
  \begin{align*}
    \mu(B) = \mu(B\cap A) + \mu(B\cap A^c), \qquad B \subseteq E.
  \end{align*}
  Then, $\mathcal F$ is a $\sigma$-Algebra and $\mu|_{\mathcal F}$ is
  a measure. In addition, $\{N\subseteq \Omega:
  \mu(N)=0\}\subseteq \mathcal F$, i.e.\ $\mathcal F$ is complete.
\end{theorem}

Note that we extend the standard assumption that all $\alpha_t$ are
separable, complete metric spaces (or Polish, i.e.\ separable and
metrizable through a complete metric) to cover the case of extended
pseudo-metric spaces. Such spaces do not satisfy the frequently used
Hausdorff (or t2) property, i.e.\ there can be $x\neq y$ such that all
open balls around $x$ and $y$ overlap.  This generalization was only
possible since underlying results in \leaninline{mathlib} were already
provided on the same level of generality.  More precisely,
\leaninline{isCompact_iff_totallyBounded_isComplete}, which shows that a
set $A \subseteq \alpha$ is compact iff it is complete and totally
bounded (see note~\ref{note:tot}), requires $\alpha$ to be a uniform
space (Remark~\ref{rem:uniform}, recall that every metric space is
uniform).  We also require the underlying space(s) to be
second-countable (used in the proof of Lemma~\ref{L:relcoPol}).  A
second-countable uniform space can be made into an (extended)
pseudo-metric space (\leaninline{UniformSpace.pseudoMetrizableSpace});
see also Remark~\ref{rem:uniform} for some more details.

\subsection{Extending a set function}
In the formulation of Theorem~\ref{T1}, we extend $P$, which is
defined on a union of $\sigma$-algebras. However, unions of
$\sigma$-algebras in general are not $\sigma$-algebras, but they can
be used to define the $\sigma$-algebra generated by the union. So, we
need to extend $P$ to the $\sigma$-algebra generated by $\mathcal
A$. This is exactly what Carathéodory's extension theorem is made
for. In fact, we implemented this result in greater generality than
needed for the proof of the extension theorem. Note that $\mathcal A$
in Theorem~\ref{T1} is a ring of sets (see the next definition)
containing the whole set. (This is sometimes called a field of sets.)
We will work with the weaker semi-ring as introduced next in the
formulation of Carathéodory's extension theorem; see
e.g.\ \cite[Definition~1.9]{Klenke2013}.

\begin{definition}[Semi-ring, ring]\label{def:semi}
  Let $\alpha$ be some set. We call $\mathcal H \subseteq 2^\alpha$ a
  \emph{semi-ring}, if it is (i) a $\pi$-system (i.e.\ closed under
  $\cap$) and (ii) for all $A, B \in\mathcal H$ there is $\mathcal K
  \subseteq_f \mathcal H$ with\footnote{We write $A\uplus B$ for
    $A\cup B$ if $A\cap B=\emptyset$.}  $B\setminus A = \biguplus_{K
    \in \mathcal K} K$.  \\ We call $\mathcal H \subseteq 2^\alpha$ a
  \emph{ring}, if it is closed under $\cup$ and under set-differences.
\end{definition}

\noindent
Any ring is a semi-ring since $A\cap B = A \setminus (A \setminus B)$,
i.e.\ every ring is a $\pi$-system.
We define a semi-ring on a type \leaninline{α} as follows:

% Let us remark that (i) \leaninline{Lean} indicates a coercion by
% \leaninline{↑}, which in this case is from \leaninline{Finset} to
% \leaninline{Set} and (ii) we have \leaninline{PairwiseDisjoint (s : Set ι) (f : ι → α)}
% iff the images of any distinct elements of \leaninline{ι}
% under \leaninline{f} are different. (Hence, if \leaninline{f = id}, the
% usual definition of pairwise disjoint sets unfolds.)

\noindent
We do not show the formalization of rings here.
The important ring of sets in our formalization is the ring of measurable cylinders on a product space.
A cylinder is defined as follows:

%\begin{minted}[highlightlines={1}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
%def cylinder
%    (s : Finset ι) (S : Set (∀ i : s, α i)) :
%    Set (∀ i : ι, α i) :=
%  (fun (f : ∀ j : ι, α j) => fun i : s => f i) ⁻¹' S
%\end{minted}

\noindent
In this definition, \leaninline{s} is a finite subset of the index set
\leaninline{ι}, and for some $S$ in the finite product $\prod_{i \in s}
\alpha_i$, consider the projection $\pi_s : \prod_{i \in \iota}
\alpha_i \to \prod_{i \in s} \alpha_i$, and consider the preimage of
$S$ (This is what the last line in previous definition gives.).
We can define \leaninline{cylinders α}, the set of all measurable cylinders.

% \begin{minted}[highlightlines={1}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% def cylinders : Set (Set (∀ i : ι, α i)) :=
%   ⋃ (s) (S) (_ : MeasurableSet S), {cylinder s S}
% \end{minted}

\noindent
Membership of that set is characterized as follows:
% \begin{minted}[highlightlines={1}, framesep = 0pt, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% theorem mem_cylinders
%     (t : Set (∀ i : ι, α i)) :
%     t ∈ cylinders α ↔
%       ∃ (s S : _) (_ : MeasurableSet S), t = cylinder s S
% \end{minted}
\noindent
The finite set \leaninline{s} of the potential coordinates where
the cylinder deviates from the whole set need not be unique.
From the $\exists$-statement of \leaninline{mem_cylinders}, we however choose an
arbitrary such \leaninline{Finset ι}, called \leaninline{cylinders.finset}.


The set-system of cylinders is in fact a field, hence a ring and
a semi-ring. This means we can prove the following:

% \begin{minted}[highlightlines={1-4}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% theorem setField_cylinders : SetField (cylinders α)
% theorem setRing_cylinders :  SetRing (cylinders α)
% theorem setSemiringCylinders
%   : SetSemiring (cylinders α)
% \end{minted}

From the field/ring/semi-ring of cylinders, we have to define the
generated $\sigma$-algebra. This uses the following:
% \begin{minted}[highlightlines={1}, framesep = 0pt, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% theorem generateFrom_cylinders :
%     MeasurableSpace.generateFrom (cylinders α)
%     = MeasurableSpace.pi

% instance MeasurableSpace.pi
%     [m : ∀ a, MeasurableSpace (π a)] :
%     MeasurableSpace (∀ a, π a) :=
%   iSup a, (m a).comap fun b => b a
% \end{minted}

%Here, \leaninline{comap {β : Type} (f : α → β) (m : MeasurableSpace β)}
is the measurable space consisting of pre-images of measurable subsets
of \leaninline{β}, and the set of $\sigma$-algebras on the product
$\prod_{i\in\iota} \alpha_i$ (or on any other space) is a complete
lattice, i.e.\ the subset defined by the \leaninline{comap}s has a
supremum, which defines the $\sigma$-algebra generated by the
cylinders.

Let us state an important lemma on semi-rings:

\begin{lemma}\label{l1}
  Let $\mathcal H$ be a semi-ring, $\mathcal I \subseteq_f \mathcal H$,
  $A \in \mathcal H$. Then, there is $\mathcal K \subseteq_f \mathcal
  H$ such that $\mathcal K$ contains pairwise disjoint sets and $A
  \setminus \bigcup_{I \in \mathcal I} I = \biguplus_{K\in \mathcal K}
  K$.
\end{lemma}

\begin{proof}
  We proceed by induction on $\mathcal I$. If $\mathcal I$ is a
  singleton, the assertion is true by the definition of a semi-ring. If
  it holds for some $\mathcal I$ (i.e.\ there is $\mathcal K
  \subseteq_f \mathcal H$ with $A \setminus \bigcup_{I \in \mathcal I}
  I = \biguplus_{K\in \mathcal K} K$), let us consider $\mathcal I' =
  \{J\} \cup \mathcal I$ for some $J \notin \mathcal I$. For each $K
  \in \mathcal K$, Write $K \setminus J = \biguplus_{J_K \in \mathcal
    J_K} J_K$ for some $\mathcal J_K \subseteq_f \mathcal H$ (which
  exists by the definition of a semi-ring). Then, write
  \begin{align*}
    A \setminus \bigcup_{I' \in \mathcal I'} I & = \Big(A \setminus
    \bigcup_{I \in \mathcal I} I\Big) \setminus J = \biguplus_{K\in
      \mathcal K} K \setminus J = \biguplus_{K\in \mathcal K}
    \biguplus_{J_K \in \mathcal J_K} J_K.
  \end{align*}
  This concludes the proof, since the latter disjoint union is over a
  finite set.
\end{proof}

The proof as well as its formalization is somewhat straight-forward,
but requires induction over finite sets.

% \begin{minted}[highlightlines={1}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% theorem exists_disjoint_finset_diff_eq
%     (hC : SetSemiring C) (hs : s ∈ C)
%     (I : Finset (Set α)) (hI : ↑I ⊆ C) :
%     ∃ (J : Finset (Set α)) (_h_ss : ↑J ⊆ C)
%     (_h_dis : PairwiseDisjoint (J : Set (Set α)) id),
%     s \ ⋃₀ I = ⋃₀ J
% \end{minted}

\noindent
The existence-statement of the above lemma actually gives rise to a
definition. Here, we use \leaninline{Exists.choose} in order to extract
an element from an $\exists$-statement. In addition, we do not allow
$\emptyset \in \mathcal K$ without loss of generality:

% \begin{minted}[highlightlines={1}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% def diff₀ (hC : SetSemiring C)
%     (hs : s ∈ C) (I : Finset (Set α)) (hI : ↑I ⊆ C)
%     [DecidableEq (Set α)] : Finset (Set α) :=
%   (hC.exists_disjoint_finset_diff_eq hs I hI).choose \ {∅}
% \end{minted}

\noindent
Extending Lemma~\ref{l1}, we would like to write a finite union of
members of a semi-ring as a finite union of disjoint sets.

\begin{lemma}\label{l2}
  Let $\mathcal H$ be a semi-ring and $A_1,...,A_m \in \mathcal
  H$. Then, there are $\mathcal K_1,...,\mathcal K_m \subseteq_f
  \mathcal H$ disjoint such that $\bigcup_{n=1}^m \mathcal K_n$
  contains disjoint sets and $\bigcup_{m=1}^n A_m = \biguplus_{m=1}^n
  \biguplus_{K \in \mathcal K_n} K$.
\end{lemma}

\begin{proof}
  Indeed, we may write $ \bigcup_{m=1}^n A_m = \biguplus_{n=1}^m
  \Big(A_n \setminus \bigcup_{i=1}^{n-1}A_i\Big).$ Then, the result
  follows by applying Lemma~\ref{l1} to $A_n \setminus
  \bigcup_{i=1}^{n-1}A_i$, $n=1,...,m$.
\end{proof}

\noindent
\subsection{Carathéodory's extension theorem}
Measures are set functions defined on a $\sigma$-algebra, satisfying
some properties which we recall below. Frequently,
constructively defining such a measure on the full $\sigma$-algebra
is not possible, but defining a set function on a semi-ring is
possible. As an example, consider Lebesgue-measure on $\mathbb R$,
with the Borel $\sigma$-algebra on $\mathbb R$, which is
$\sigma(\mathcal O)$, where $\mathcal O$ is the set of open sets, and
$\sigma(\mathcal O)$ is the smallest $\sigma$-algebra containing all
open sets. Since this set system is defined only abstractly, it is
hard to know which volume each of these sets should be assigned to at
first sight. However, the volume of an interval is easy, since it may
be defined by the length of the interval. So, in order to construct
measures from a set function $m$ on a (semi-)ring $\mathcal H$
(e.g.\ the set of all semi-open intervals), it has been a fundamental
insight of Carathéodory that one may start by defining a set function
(outer measure) $\mu$ on $2^\alpha$, and then show $\mu$ extends $m$
and defines a measure on the $\sigma$-algebra of
  measurable sets (see Theorem~\ref{T:cara} below), which contains
$\sigma(\mathcal H)$.
%(Note that the set of semi-open intervals
%generates the Borel $\sigma$-algebra on $\mathbb R$.)
We will follow this abstract construction, and start by stating some
basic concepts.

\begin{definition}\label{def:content}
  For some set $\alpha$, let $\mathcal H \subseteq 2^\alpha$
  and call any $m : \mathcal H \to [0,\infty]$ a
    content.
  \begin{enumerate}
  \item $m$ is called additive if for $\mathcal K \subseteq_f \mathcal
    H$ pairwise disjoint and $\bigcup_{K \in \mathcal K} K \in
    \mathcal H$, we have $m \Big(\bigcup_{K \in \mathcal K} K \Big) =
    \sum_{K \in \mathcal K} m(K)$. If the same holds for\footnote {We
      write $A \subseteq_c B$ if $A$ is a countable subset of
      $B$.}$\mathcal K \subseteq_c \mathcal H$ pairwise disjoint, we
    say that $m$ is $\sigma$-additive.
  \item The set-function $m$ is called sub-additive if for $\mathcal K
    \subseteq_f \mathcal H$ and $\bigcup_{K \in \mathcal K} K \in
    \mathcal H$, we have $m \Big(\bigcup_{K \in \mathcal K} K \Big)
    \leq \sum_{K \in \mathcal K} m(K)$. (Note that elements of
    $\mathcal K$ need not be disjoint.) Here, $\sigma$-sub-additivity
    is defined in the obvious way using $\mathcal K\subseteq_c
    \mathcal H$.
  \item If $m(A) \leq m(B)$ for $A\subseteq B$ and $A,B\in\mathcal H$,
    we say that $m$ is monotone.
  \item If $\mathcal H$ is a $\sigma$-algebra and $m$ is
    $\sigma$-additive with $m(\emptyset) = 0$, we call
    $m$ a measure.
  \item If $\mathcal H = 2^\alpha$ and $m$ is monotone and
    $\sigma$-sub-additive with $m(\emptyset)=0$, we call $m$ an outer
    measure.
  \end{enumerate}
\end{definition}

\noindent
For additive contents, we need two definitions.
The structure \leaninline{AddContent} defines
an additive set-function on \leaninline{Set α}.
The definition \leaninline{extendContent} takes an additive function defined only on a semi-ring of sets, and extends it to an additive content on all sets by choosing the value infinity outside of the semi-ring. We use this because total functions are easier to use in subsequent code.

% \begin{minted}[highlightlines={1, 10}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% structure AddContent (C : Set (Set α)) where
%   toFun : Set α → ℝ≥0∞
%   empty' : toFun ∅ = 0
%   add' : ∀ (I : Finset (Set α))
%     (_h_ss : ↑I ⊆ C) (_h_dis : PairwiseDisjoint
%       (I : Set (Set α)) id)
%     (_h_mem : ⋃₀ ↑I ∈ C),
%     toFun (⋃₀ I) = ∑ u in I, toFun u

% def extendContent
%     (hC : SetSemiring C)
%     (m : ∀ s : Set α, s ∈ C → ℝ≥0∞)
%     (m_empty : m ∅ hC.empty_mem = 0)
%     (m_add :
%       ∀ (I : Finset (Set α)) (h_ss : ↑I ⊆ C)
%       (_h_dis : PairwiseDisjoint (I : Set (Set α)) id)
%       (h_mem : ⋃₀ ↑I ∈ C),
%       m (⋃₀ I) h_mem = ∑ u : I, m u (h_ss u.prop)) :
%     AddContent C
% \end{minted}

For the concrete application we have in mind, we introduce a
definition which uses a specific semi-ring, the measurable cylinders,
based on a projective family of measures.  That is the goal of the two
definitions \leaninline{kolmogorovFun} and
\leaninline{kolContent} below:

% \begin{minted}[highlightlines={1,7}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% def kolmogorovFun
%     (P : ∀ J : Finset ι, Measure (∀ j : J, α j))
%     (s : Set (∀ i, α i))
%     (hs : s ∈ cylinders α) : ℝ≥0∞ :=
%   P (cylinders.finset hs) (cylinders.set hs)

% def kolContent
%     (hP : IsProjectiveMeasureFamily P) :
%     AddContent (cylinders α) :=
%   extendContent setSemiringCylinders
%     (kolmogorovFun P) (kolmogorovFun_empty hP)
%     (kolmogorovFun_additive hP)
% \end{minted}

Here, \leaninline{kolmogorovFun_empty} and
\leaninline{kolmogorovFun_⌋} \leaninline{additive} give proofs for
\leaninline{m_empty} and \leaninline{m_add} in
\leaninline{extendContent} as applied to a
\leaninline{kolmogorovFun}.

\noindent
From the next lemma, we will need monotonicity and sub-additivity, as
well as $\sigma$-additive $\Rightarrow$ $\sigma$-sub-additive in the
proof of the Carathéodory extension,
Theorem~\ref{T:masseind}. We do not show any details
  about its formalization here.

\begin{lemma}[Set-functions on semi-rings]\label{l:halbextRing}
  Let $\mathcal H$ be a semi-ring and $m: \mathcal H\to [0,\infty]$
  additive. Then, $m$ is monotone and sub-additive. In addition, $m$
  is $\sigma$-additive iff it is $\sigma$-sub-additive.
\end{lemma}

\begin{proof}
  We start by monotonicity. Let $A, B\in\mathcal H$ with $A\subseteq
  B$ and $\mathcal K \subseteq_f \mathcal H$ with $B \setminus A =
  \biguplus_{K\in\mathcal K} K$. Therefore, we can write $m(A) \leq
  m(A) + \sum_{K\in\mathcal K} m(K) = m(B)$.

  Next, we claim that for $\biguplus_{I\in\mathcal I} I \subseteq A$
  with all sets belonging to $\mathcal H$, we have $\sum_{I \in \mathcal
    I} m(I) \leq m(A)$. For this, write $A \setminus \biguplus_{I\in
    \mathcal I} I = \biguplus_{K\in\mathcal K} K$ as in
  Lemma~\ref{l1}. Then,
  $$ m(A) = m\Big(\biguplus_{I\in\mathcal I} I \uplus
  \biguplus_{K\in\mathcal K} K\Big) = \sum_{I\in\mathcal I} m(I) +
  \sum_{K\in\mathcal K} m(K) \geq \sum_{I\in\mathcal I} m(I). $$

  For sub-additivity, let $\mathcal I \subseteq_f \mathcal H$ with
  $\bigcup_{I \in \mathcal I} I \in \mathcal H$. Without loss of
  generality, we write $\mathcal I = \{I_1,...,I_n\}$ for some $n$. We
  need to show $m\Big(\bigcup_{k=1}^n I_i\Big) \leq \sum_{k=1}^n
  m(I_k)$. For $k=2,...,n$, we write
  $$ \bigcup_{k=1}^n I_k = \biguplus_{k=1}^n \Big(I_k \setminus
  \bigcup_{j=1}^{k-1} I_j\Big) = \biguplus_{k=1}^n \biguplus_{K_k \in
    \mathcal K_k} K_k$$ with $\mathcal K_k$ as in Lemma~\ref{l1}. So,
  since $\biguplus_{K_k \in \mathcal K_k} K_k \subseteq I_k \in
  \mathcal H$,
  $$ m\Big(\bigcup_{k=1}^n I_i\Big) = \sum_{k=1}^n \sum_{K_k \in
    \mathcal K_k} m(K_k) \leq \sum_{k=1}^n m(I_k).$$

  \noindent
  Now, we show that $m$ is $\sigma$-additive $\iff$ it is
  $\sigma$-sub-additive.

  \noindent
  '$\Rightarrow$': Here, just copy the proof of sub-additivity, but
  using countable $\mathcal I$, i.e.\ $n=\infty$. For '$\Leftarrow$',
  let $\mathcal I \subseteq_c \mathcal H$ and consist of
  disjoint sets with $A = \biguplus_{I \in \mathcal I} I \in\mathcal
  H$. Since $m$ is monotone and for any $\mathcal I' \subseteq_f
  \mathcal I$, we have $\biguplus_{I\in\mathcal I'} I \subseteq A$
  (hence $\sum_{I \in \mathcal I'} m(I') \leq m(A)$),
  $$ \sum_{I \in \mathcal I} m(I) = \sup_{\mathcal I' \subseteq_f
    \mathcal I} \sum_{I \in \mathcal I'} m(I) \leq m(A) \leq \sum_{I
    \in \mathcal I} m(I)$$ by $\sigma$-sub-additivity. So,
  $\sigma$-additivity follows.
\end{proof}

\noindent
Although some material on outer measures was covered in {\tt mathlib}
already, the classical extension theorem (extending a set function $m$
on a semi-ring $\mathcal H$)) was not provided yet. In particular, this
result states that the outer measure coincides with $m$ on $\mathcal
H$. All statements on equality of $\mu$ and $m$ present in {\tt
  mathlib} at the time of writing have too many requirements: they all require $m$ to be a $\sigma$-additive function defined on a $\sigma$-algebra.

The next result extends a set function on a semi-ring to an outer measure.

\begin{proposition}[Outer measure induced by a set function on a semi-ring] %\mbox{}
  Let \label{P:auss} $\mathcal H$ be a semi-ring and $m: \mathcal
  H\to\mathbb R_+$ additive. For $A\subseteq E$ let
  $$ \mu(A) := \inf_{\mathcal G \in \mathcal U(A)}
    \sum_{G\in\mathcal G} m(G)$$ where
  $$ \mathcal U(A) := \big\{\mathcal G \subseteq_c \mathcal H,
    A\subseteq \bigcup_{G\in\mathcal G} G\big\}$$ is the set of
    countable coverings of $A$. Then, $\mu$ is an outer measure.
\end{proposition}

\sloppy Let us now formulate the classical results by
Carathéodory. The first leads to the definition of the measurable
space \leaninline{OuterMeasure.caratheodory}, covered in
\leaninline{mathlib}. See e.g.\ \cite[Theorem 2.1]{Kallenberg2020}

\begin{theorem}[\boldmath $\mu$-measurable sets are a
    $\sigma$-algebra]\label{T:cara} Let $\mu$ be an outer measure on
  $E$ and $\mathcal F$ the set of $\mu$-measurable sets,
  i.e.\ the set of sets $A$ satisfying
  \begin{align*}
    \mu(B) = \mu(B\cap A) + \mu(B\cap A^c), \qquad B \subseteq E.
  \end{align*}
  Then, $\mathcal F$ is a $\sigma$-Algebra and $\mu|_{\mathcal F}$ is
  a measure. In addition, $\{N\subseteq \Omega:
  \mu(N)=0\}\subseteq \mathcal F$, i.e.\ $\mathcal F$ is complete.
\end{theorem}

\noindent
\sloppy The second result states that for an outer measure induced by
an additive content on a semi-ring, we have $\sigma(\mathcal H)
\subseteq \mathcal F$. In particular, we then have defined a measure
on $\sigma(\mathcal H)$. This result is not yet covered in {\tt
  mathlib}. However, for product spaces, in
\leaninline{MeasureTheory.Constructions.Pi}, there is
\leaninline{pi_caratheodory}, which shows that $\sigma(\mathcal H)
\subseteq \mathcal F$ in the construction of a product
  measure for a finite index set. In addition, there is
\leaninline{pi_pi_aux}, which shows that $\mu$ extends $m$ on $\mathcal
H$ in the same special case. See also \cite[Theorem
  2.5]{Kallenberg2020}.

\begin{theorem}[Carathéodory extension]\label{T:masseind}
  Let $\mathcal H$ be a semi-ring and $m: \mathcal H\to\mathbb R_+$
  $\sigma$-finite and $\sigma$-additive. Furthermore, let $\mu$ be the
  induced outer measure from Proposition~\ref{P:auss} and $\mathcal F$
  the $\sigma$-algebra from Theorem~\ref{T:cara}. Then,
  $\sigma(\mathcal H)\subseteq\mathcal F$ and $\mu$ coincides with $m$
  on $\mathcal H$.
\end{theorem}

\begin{proof}
  First, $m$ is $\sigma$-sub-additive by
  Lemma~\ref{l:halbextRing}.

  ~

  \noindent\emph{Step 1: $\mu|_{\mathcal H} = m$:} Let $A\in\mathcal
  H$. Choose $\mathcal K \subseteq_c \mathcal H$ with
  $A\subseteq \bigcup_{K\in\mathcal K} K$ and
  $$\mu(A) \geq \sum_{K\in\mathcal K} m(K) - \varepsilon.$$ By $ A =
  \bigcup_{K\in\mathcal K} K\cap A$,
  \begin{align*}
    \mu(A) \leq m(A) \leq \sum_{K\in\mathcal K} m(K\cap A) \leq
    \sum_{K\in\mathcal K} m(K) \leq \mu(A) + \varepsilon,
  \end{align*}
  where we have used $\sigma$-sub-additivity of $m$ in the second and
  monotonicity of $m$ in the third $\leq$ (see
  Lemma~\ref{l:halbextRing}).  With $\varepsilon\to 0$ we find that
  $\mu(A) = m(A)$.

  ~

  \noindent\emph{Step 2: $\sigma(\mathcal H)\subseteq \mathcal F$:}
  Let $B \subseteq E$, $A\in\mathcal H$ and $\varepsilon>0$. Choose
  $\mathcal K \subseteq_c \mathcal H$ such that $B \subseteq
  \bigcup_{K \in \mathcal K} K$ and $\mu(B) \geq \sum_{K\in\mathcal K}
  m(K) - \varepsilon.$ Then, by additivity of $m$,
  \begin{align*}
    \mu(B)+\varepsilon & \geq \sum_{K\in\mathcal K} \mu(K) =
    \sum_{K\in\mathcal K} \mu(K\cap A) + \sum_{K\in\mathcal K}
    \mu(K\cap A^c) \\ & \geq \mu(B\cap A) + \mu(B\cap A^c).
  \end{align*}
  By sub-additivity of $\mu$, we find that $\mu(B) \leq \mu(B\cap A) +
  \mu(B \cap A)$, so letting $\varepsilon\to 0$ leads to $\mu(A) =
  \mu(E\cap A) + \mu(E\cap A^c)$. This implies that $A$ is $\mathcal
  F$-measurable, and we have shown $\sigma(\mathcal H) \subseteq
  \mathcal F$, since $\mathcal F$ is a $\sigma$-algebra.
\end{proof}

\noindent
Here is a formalization of the measure resulting from
Theorem~\ref{T:masseind}:

% \begin{minted}[highlightlines={1}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% def Measure.ofAddContent
%     [mα : MeasurableSpace α] (hC : SetSemiring C)
%     (hC_gen : MeasurableSpace.generateFrom C = mα)
%     (m : AddContent C)
%     (m_sigma_subadd : ∀ ⦃f : ℕ → Set α⦄
%       (hf : ∀ i, f i ∈ C) (hf_Union : (⋃ i, f i) ∈ C),
%       m (⋃ i, f i) hf_Union ≤ ∑' i, m (f i) (hf i)) :
%     Measure α
% \end{minted}

\subsection{$\sigma$-additivity of set functions}
For the proof of Kolmogorov's extension Theorem, note that $P$ as
given in Theorem~\ref{T1} is a finite additive set function on the
ring $\mathcal A$. In order to use Theorem~\ref{T:masseind}, we
therefore have to show $\sigma$-additivity. For this, we will use
Lemma~\ref{l:stetigcompact} below, i.e.\ inner regularity of $P$ with
respect to compact sets. Before, we will show useful alternative
conditions for $\sigma$-additivity, which do not make use of any
topological structure of the underlying space.

\begin{lemma}[$\sigma$-additivity and continuity at $\emptyset$]\label{P:stetigmass}
  Let $\mathcal R$ be a ring and $m:\mathcal R\to \mathbb R_+$
  additive. Then, the following are equivalent:
  \begin{enumerate}
    \item $m$ is $\sigma$-additive;
    \item $m$ is $\sigma$-sub-additive;
    \item $m$ is continuous from below, i.e.\ for $A, A_1, A_2,... \in
      \mathcal R$ with $A_1 \subseteq A_2 \subseteq \cdots$ and $A =
      \bigcup_{n=1}^\infty A_n$, we have $m(A) = \lim_{n\to\infty}
      m(A_n)$.
    \item $m$ is continuous from above in $\emptyset$, i.e.\ for $A_1,
      A_2,...\in\mathcal R$ with $A_1 \supseteq A_2 \supseteq ...$ and
      $\bigcap_{n=1}^\infty A_n = \emptyset$, we have
      $\lim_{n\to\infty} m(A_n)=0$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  1.$\Leftrightarrow$2.\ was already shown in
  Lemma~\ref{l:halbextRing}, since $\mathcal R$ is a semi-ring.

  ~

  \noindent 1.$\Rightarrow$3.: For $A_1, A_2,...$ as in 3., we write
  (with $A_0 := \emptyset$)
  \begin{align*}
     m(A) & % = m\Big( \biguplus_{n=1}^\infty \Big(A_n \setminus
     %\Big(\bigcup_{l=1}^{k-1} A_l\Big)\Big)\Big)
     = \lim_{n\to\infty}
     m\Big( \biguplus_{k=1}^n A_k \setminus \Big(\Big(\bigcup_{l=1}^{k-1}
     A_l\Big)\Big)\Big) = \lim_{n\to\infty} m(A_n).
  \end{align*}

  \noindent 3.$\Rightarrow$1.: Let $A_1, A_2,... \in \mathcal R$ be
  disjoint and $A = \biguplus_{n=1}^\infty A_n$. Set $B_n :=
  \bigcup_{k=1}^n A_k$, such that $B_1, B_2,...$ satisfy 3. Hence,
  \begin{align*}
    m\Big( \biguplus_{n=1}^\infty A_n\Big) & = m\Big(
    \bigcup_{n=1}^\infty B_n\Big) = \lim_{n\to\infty} m\Big(
    \bigcup_{k=1}^n B_n\Big) \\ & = \lim_{n\to\infty} m\Big(
    \biguplus_{k=1}^n A_n\Big) = \sum_{n=1}^\infty m(A_n).
  \end{align*}

  \noindent 3.$\Rightarrow$4.: Let $A_1, A_2,\dots \in\mathcal R$ as
  in 4. Set $B_n := A_1\setminus A_n$. Then, $B=A_1,B_1,B_2,\dots
  \in\mathcal R$ satisfy 3., and therefore
  $$m(A_1) = \lim_{n\to\infty} m(B_n) = m(A_1) -
  \lim_{n\to\infty}m(A_n),$$ and 4.\ follows.

  ~

  \noindent 4.$\Rightarrow$3.: Let $A,A_1,A_2,\dots \in\mathcal R$ as
  in 3. Let $B_n := A\setminus A_n \in \mathcal R, n\in\mathbb
  N$. Then, $\bigcap_{n=1}^\infty B_n = \emptyset$, so $$0 =
  \lim_{n\to\infty} \mu(B_n) = \mu(A) - \lim_{n\to\infty} \mu(A_n),$$
  and 3.\ follows.
\end{proof}

\noindent
As an example, we give the formalization of $4.\Rightarrow 1.$:

% \begin{minted}[highlightlines={1}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% theorem sigma_additive_addContent_of_tendsto_zero
%     (hC : SetRing C)
%     (m : AddContent C)
%     (hm_ne_top : ∀ {s} (_ : s ∈ C), m s ≠ ∞)
%     (hm : ∀ ⦃s : ℕ → Set α⦄ (_ : ∀ n, s n ∈ C),
%       Antitone s → (⋂ n, s n) = ∅ →
%       Tendsto (fun n => m (s n)) atTop (nhds 0))
%       ⦃f : ℕ → Set α⦄ (hf : ∀ i, f i ∈ C) (hUf : (⋃ i, f i) ∈ C)
%       (h_disj : Pairwise (Disjoint on f)) :
%     m (⋃ i, f i) = ∑' i, m (f i)
% \end{minted}

\noindent
Next, we will be extending our analysis to the case of a topological
space. We define inner (and outer) regularity of set functions.

\begin{definition}[Inner regularity] \label{def:innerreg}
  Let $\alpha$ be some set, equipped with a topology, and $m$ be a
  set-function on some $\mathcal H \subseteq 2^\alpha$.
  \begin{enumerate}
  \item Let $p, q : 2^\alpha \to \{\text{true, false}\}$. Then, $m$ is
    called inner regular with respect to $p$ and $q$, if
    $$ m(A) = \sup\{m(F) : p(F) = \text{true}, F \subseteq A\}$$ for
    all $A \in \mathcal H$ with $q(A) = \text{true}$.
  \item If $q(A) = \text{true}$ iff $A$ is measurable, we neglect the
    {\em and $q$}. If $p(A) = \text{true}$ iff $A$ is closed (compact,
    closed and compact), we say that $m$ is inner regular with respect
    to closed (compact, compact and closed) sets.
  \end{enumerate}
\end{definition}

\noindent
The above definition closely resembles its formalization in
\leaninline{mathlib}:

% \begin{minted}[highlightlines={1}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% def MeasureTheory.Measure.InnerRegular
%     {α : Type u_1}  {_ : MeasurableSpace α}
%     (μ : Measure α) (p q : Set α → Prop) :=
%   ∀ ⦃U⦄, q U → ∀ r < μ U, ∃ K, K ⊆ U ∧ p K ∧ r < μ K
% \end{minted}

For the next result, recall that for compact sets $C_1, C_2,...$ with
$\bigcap_{n=1}^\infty C_n = \emptyset$, there is some $N$ with
$\bigcap_{n=1}^N C_n = \emptyset$. More generally, compact sets form a
compact system, which is defined as follows:

\begin{definition}
  Let $\mathcal C \subseteq 2^\alpha$. If, for all $C_1, C_2,...$ with
  $\bigcap_{n=1}^\infty C_n = \emptyset$, there is some $N$ with
  $\bigcap_{n=1}^N C_n = \emptyset$, we call $\mathcal C$ a {\em
    compact system}.
\end{definition}

\noindent
Here is the formalization:

% \begin{minted}[highlightlines={1}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% def IsCompactSystem (p : Set α → Prop) : Prop :=
%     ∀ C : ℕ → Set α, (∀ i, p (C i)) → ⋂ i, C i = ∅ →
%     ∃ (s : Finset ℕ), ⋂ i ∈ s, C i = ∅
% \end{minted}

\noindent
Such compact systems are important since they allow a
proof of $\sigma$-additivity of a content on a ring, which is the
missing piece for applying the Carathéodory Theorem in the proof of
the Kolmogorov extension theorem.

\begin{lemma}\label{l:stetigcompact}
  Let $\alpha$ be a topological space and $\mu$ be an additive set
  function on a ring $\mathcal R$ contained by the Borel
  $\sigma$-algebra, and which is inner regular with respect to a
  compact system. Then, $\mu$ is $\sigma$-additive.
\end{lemma}

\begin{proof}
  According to Lemma~\ref{P:stetigmass}, we need to show continuity of
  $\mu$ in $\emptyset$, so let $A_1 \supseteq A_2 \supseteq... \in
  \mathcal R$ satisfy $\bigcap_{n=1}^\infty A_n = \emptyset$ and
  $\varepsilon>0$. Let $\delta_1, \delta_2,... >0$ with
  $\sum_{n=1}^\infty \delta_n < \varepsilon$. For each $n$, let
  $\mathcal C \ni C_n \subseteq A_n \in \mathcal R$ with $\mu(A_n)
  \leq \mu(C_n) + \delta_n$. We have that $\bigcap_{n=1}^\infty C_n
  \subseteq \bigcap_{n=1}^\infty A_n = \emptyset$, so there is $N \in
  \mathbb N$ with $\bigcap_{n=1}^N C_n = \emptyset$ since $\mathcal C$
  is a compact system. So, for any $m>N$, we have that
  \begin{align*}
     \mu(A_m) & = \mu\Big( \Big(\bigcap_{n=1}^m A_n\Big) \setminus
     \Big(\bigcap_{n=1}^m C_n\Big)\Big) \leq \sum_{n=1}^m \mu(A_n
     \setminus C_n) \\ & \leq \sum_{n=1}^m \delta_n < \varepsilon.
  \end{align*}
  This concludes the proof.
\end{proof}

In order to apply this result and show the extension theorem, we need
to show that
$\{\pi_J^{-1} C: C \in \prod_{j\in J} \alpha_j \text{ compact and
  closed}\}$ is a compact systems. Note that compact sets are closed
in Hausdorff spaces, but we do not have this property since we are
working with pseudo-metric spaces. Since Lemma~\ref{l:stetigcompact}
gives the $\sigma$-additivity of a \leaninline{kolContent}, which
is defined through the projective family \leaninline{P}, we have:

% \begin{minted}[highlightlines={1}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% theorem kolContent_sigma_additive_of_innerRegular
%     (hP : IsProjectiveMeasureFamily P)
%     (hP_inner : ∀ J,
%       (P J).InnerRegular (fun s => IsCompact s ∧ IsClosed s)
%       MeasurableSet)
%     ⦃f : ℕ → Set (∀ i, α i)⦄ (hf : ∀ i, f i ∈ cylinders α)
%     (hf_Union : (⋃ i, f i) ∈ cylinders α)
%     (h_disj : Pairwise (Disjoint on f)) :
%     kolContent hP (⋃ i, f i) = ∑' i, kolContent hP (f i)
% \end{minted}

Note that the assumption \leaninline{hP_inner} from above translates
directly to inner regularity of the \leaninline{kolContent}, which
is defined through the projective family \leaninline{P}. Moreover, since
the Carathéodory extension theorem requires $\sigma$-subadditivity
(rather than $\sigma$-additivity), we can use
\leaninline{sigma_subadditive_of_sigma_additive} (see
  Lemma~\ref{P:stetigmass}, 1.$\Rightarrow$2.) in order to show that
the assumptions of the last result also imply $\sigma$-subadditivity;
this gives
\leaninline{kolContent_sigma_subadditive_of_innerRegular}.

What remains to be done is to show conditions under which the
projective family is inner regular with respect to compact (and
closed) sets. For this, we need some assumption on the underlying
spaces.

The next result is already implemented in \leaninline{mathlib}: Use
\leaninline{Measure.InnerRegular.of_pseudoEMetricSpace} in order to show
that every open set is inner regular with respect to closed sets, and
\leaninline{Measure.InnerRegular.measurableSet_of_open} to show that
every measurable set is inner regular with respect to closed sets.
The claimed outer regularity is part of
\leaninline{MeasureTheory.Measure.InnerRegular.weaklyRegular_of_finite}.

\begin{lemma}\label{l:pseude1}
  Let $r$ be an extended pseudo-metric on $\alpha$, and the topology
  on $\alpha$ be given by $r$. If $\mu$ is a finite measure on the
  Borel $\sigma$-algebra $\mathcal B$ on $\alpha$, it is inner regular
  with respect to closed sets. In fact, we have also outer regularity
  with respect to open sets, i.e.\ for all $A\in \mathcal B$,
  $$ \mu(A) = \inf \{ \mu(O) : A \subseteq O \text{ open}\}.$$
\end{lemma}

\begin{proof}
  It suffices to prove that
  $$ \mathcal A := \Big\{A \in \mathcal B : \mu(B) = \sup_{F\subseteq
    B \text{ closed}} \mu(F) = \sup_{B\subseteq O \text{ open}}
  \mu(O)\Big\} $$ is a $\sigma$-algebra containing all closed
  sets. So, we proceed in two steps.  \\ {\em Step 1: $\mathcal A$ is
    a $\sigma$-algebra:} Since $\mu$ is finite, we have that $\mu(A^c)
  = \mu(\alpha) - \mu(A)$ for all $A$. Using this, we already have
  that $\mathcal A$ is closed under complements. So, we are left with
  showing that $\mathcal A$ is closed under countable unions. For
  this, let $A_1, A_2,... \in \mathcal A$ and $A :=
  \bigcup_{n=1}^\infty A_n$. Fix $\varepsilon > 0$ and a sequence
  $\delta_1, \delta_2,... > 0$ with $\sum_{n=1}^\infty \delta_n <
  \varepsilon$. For each $n$, let $F_n \subseteq A_n \subseteq O_n$
  with $F_n$ closed, $O_n$ open and $\mu(O_n \setminus F_n) \leq
  \delta_n$. Then, $O := \bigcup_{n=1}^\infty O_n$ is open and
  $$ 0 \leq \mu(O) - \mu(A) \leq \sum_{n=1}^\infty \mu(O_n \setminus
  A) \leq \sum_{n=1}^\infty \mu(O_n \setminus A_n) < \varepsilon.$$
  This shows outer regularity of $\mu$ with respect to open sets at
  $A$. It remains to show inner regularity with respect to closed
  sets. For this, note that $\bigcup_{n=1}^N F_n$ is closed for all
  $N$ and recall from Proposition~\ref{P:stetigmass} that $\mu$ is
  continuous from below. Therefore, setting $F := \bigcup_{n=1}^\infty
  F_n$,
  \begin{align*}
    0 & \leq \mu(A) - \lim_{N\to\infty} \mu\Big(\bigcup_{n=1}^N
    F_n\Big) = \mu(A \setminus F) \leq \sum_{n=1}^\infty \mu(A_n
    \setminus F) \\ & \leq \sum_{n=1}^\infty \mu(A_n \setminus F_n) <
    \varepsilon.
  \end{align*}
  This shows inner regularity of $\mu$ with respect to closed sets at
  $A$.


  \noindent
  {\em Step 2: $\mathcal A$ contains all closed sets:} Let
  $\varepsilon_ n\downarrow 0$ and, for $\varepsilon>0$ and some $A
  \subseteq \alpha$, $A^\varepsilon := \{x : \exists y \in A,
  r(x,y)<\varepsilon\}$. Then, for $A$ closed, we have that $A =
  \bigcap_{n=1}^\infty A^{\varepsilon_n}$ and $A^{\varepsilon_n}$ is
  open for all $n$. Clearly, $\mu(A) = \sup\{\mu(F): F\text{ closed},
  F \subseteq A\}$, since $A$ is closed. By continuity of $\mu$, we
  find that $\mu(A^{\varepsilon_n}) \xrightarrow{n\to\infty} \mu(A)$,
  therefore $\mu(A) = \inf\{\mu(O): F\text{ open}, A \subseteq O\}$,
  and we have shown that $A \in \mathcal A$.
\end{proof}

In the next lemma, we will need that a closed subset of a compact set
is compact. (See \leaninline{isCompact_of_isClosed_subset}.) We describe
the formalization of the next lemma only in conjunction with
Lemma~\ref{L:relcoPol}. We omit the proof, since the
  only interesting part is an application of inner regularity from
  Lemma~\ref{l:pseude1}.

\begin{lemma}\label{l:tight}
  Let $r$ be an extended pseudo-metric on $\alpha$, and the topology
  on $\alpha$ be given by $r$. If $\mu$ is a finite measure on the
  Borel $\sigma$-algebra $\mathcal B$ on $\alpha$, the following are
  equivalent:
  \begin{enumerate}
    \item For all $\varepsilon>0$, there is some closed and compact
      $K$ with $\mu(K^c) < \varepsilon$.
    \item For all $\varepsilon>0$ and $A \in \mathcal B$, there is
      some closed and compact $K\subseteq A$ with $\mu(A \setminus K)
      < \varepsilon$.
  \end{enumerate}
\end{lemma}

%% \begin{proof}
%%   2. $\Rightarrow$ 1.: This is clear since $\alpha \in \mathcal B$.
%%   1. $\Rightarrow$ 2.: Fix $\varepsilon>0$ and $A \in \mathcal B$. Let
%%   $K$ be closed and compact with $\mu(K^c) < \varepsilon/2$. By
%%   Lemma~\ref{l:pseude1}, there is some closed $F \subseteq A$
%%   with $\mu(A) < \mu(F) + \varepsilon/2$. Now, we have that $F \cap K
%%   \subseteq A$ is closed and compact, and with $A \setminus (F\cap K)
%%   = (A \setminus F) \cup (A \setminus K)$,
%%   $$\mu(A \setminus (F\cap K)) \leq \mu(A \setminus F) + \mu(A
%%   \setminus K) < \varepsilon/2 + \mu(K^c) < \varepsilon. $$
%% \end{proof}

We apply this result by using that 1.\ is satisfied for complete and
separable extended pseudo-metric spaces. Here, for the generalization
to extended pseudo-metric spaces, we use that any subset of such a
space is compact iff it is complete and totally
bounded\footnote{\label{note:tot}A subset of a pseudo-metric space is
  totally bounded iff, for all $\varepsilon>0$, it can be covered by
  finitely many balls of radius $\epsilon$.} (see
\leaninline{isCompact_iff_totallyBounded_isComplete} in {\tt
  mathlib}). Since closed subsets of a complete space are complete,
the closure of a totally bounded set is (still totally bounded, hence)
compact. (This fact is used in the proof below.)

\begin{lemma}\label{L:relcoPol}
  Let $\alpha$ be a complete and separable, extended pseudo-metric
  space, and $\mu$ a finite measure on its Borel $\sigma$-algebra
  $\mathcal B$. Then, for any $\varepsilon>0$, there is some
  $K\subseteq \alpha$ with compact closure and\footnote{We write $\bar
    K$ for the closure of $K$.}  $\mu((\bar K)^c)<\varepsilon$.
\end{lemma}

\begin{proof}
 Since $\alpha$ is separable, there is a subset $\{x_1, x_2,...\}
 \subseteq_c \alpha$ with closure $\alpha$. By separability, for each
 $n\in \mathbb N$, $\alpha = \bigcup_{k=1}^\infty
 B_{\varepsilon_n}(x_k)$. Since $\mu$ is continuous from above in
 $\emptyset$ (see Lemma~\ref{P:stetigmass})
  \[
  0 = \mu\Big(\Big(\bigcup_{k=1}^\infty B_{\varepsilon_n}(x_k)\Big)^c
  \Big) = \lim_{N\to\infty} \mu\Big( \Big(\bigcup_{k=1}^N
  B_{\varepsilon_n}(x_k)\Big)^c\Big).
  \]
  Let $\delta_n\downarrow 0$ be summable with $\sum_{n=1}^\infty
  \delta_n = 1$ (e.g.\ $\delta_n = 2^{-n}$). Then, there is some
  $N_n\in\mathbb N$ with $\mu\Big( E\setminus \bigcup_{k=1}^{N_n}
  B_{\varepsilon_n}(x_k)\Big) < \varepsilon \delta_n$. Now take
  \[
    A := \bigcap_{n=1}^\infty \bigcup_{k=1}^{N_n}
    B_{\varepsilon_n}(x_k),
  \]
  which by construction is totally bounded (for $r>0$ take $n$ such
  that $\varepsilon_n < r$, such that $B_r(x_1),...,B_r(x_{N_n})$
  cover $A$), and therefore has a compact closure as noted
  in the lines directly preceding the lemma. Finally, by $\sigma$-sub-additivity of $\mu$,
  \begin{align*}
     \mu((\overline A)^c) & \leq \mu(A^c) = \mu\Big(
     \bigcup_{n=1}^\infty \Big(\Big(\bigcup_{k=1}^{N_n}
     B_{\varepsilon_n}(x_k)\Big)^c\Big)\Big) \\ & \leq
     \sum_{n=1}^\infty \mu\Big( \Big(\bigcup_{k=1}^{N_n}
     B_{\varepsilon_n}(x_k)\Big)^c\Big)< \varepsilon
     \: .
  \end{align*}
\end{proof}

Combining the last two results, we obtain the desired statement that a measure on a second countable,
complete extended pseudo-metric space is inner regular with respect to
closed compact sets.

% \begin{minted}[highlightlines={1-2}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% theorem innerRegular_isCompact_isClosed_
%     measurableSet_of_complete_countable
%     [PseudoEMetricSpace α] [CompleteSpace α]
%     [SecondCountableTopology α]
%     [BorelSpace α] (P : Measure α) [IsFiniteMeasure P] :
%     P.InnerRegular (fun s => IsCompact s ∧ IsClosed s)
%       MeasurableSet
% \end{minted}

\subsection{Proof of Kolmogorov's extension theorem}
We now describe the proof of Kolmogorov's extension theorem as well as
its formalization: For the proof of Theorem~\ref{T1}, we
\begin{enumerate}
  \item apply Theorem~\ref{T:masseind} for the ring (hence semi-ring)
    $\mathcal A$ and the set-function $P$ as given in
    Theorem~\ref{T1};
  \item show $\sigma$-additivity (as assumed in
    Theorem~\ref{T:masseind}) of $P$ by using
    Lemma~\ref{l:stetigcompact}. For the latter, we need to show that
    $P$ is inner regular with respect to a compact system. Here, note
    that $\{\pi_J^{-1}C : C \in \prod_{j\in J} \alpha_j \text{ compact
      and closed}\}$ is a compact system and $P(\pi_J^{-1}C) =
    P_J(C)$, so we need to show that $P_J$ is inner regular with
    respect to compact closed sets, $J\subseteq_f \iota$;
  \item use Lemma~\ref{L:relcoPol} in combination with
    Lemma~\ref{l:tight} (1.$\Rightarrow$2.) and the properties of
    completeness, separability of the underlying extended
    pseudo-metric spaces in order to see that every $P_J$ has the
    desired property of being inner regular wrt compact and closed
    sets. This concludes the proof of Theorem~\ref{T1}.
\end{enumerate}
The formalization of this proof resembles these arguments. We leave
out all instances in the reformulation of the result and its proof
(see below Theorem~\ref{T1} for a full formulation):

% \begin{minted}[highlightlines={1}, mathescape, numbersep=5pt, framesep=5mm, bgcolor=mygray]{Lean}
% def projectiveLimitWithWeakestHypotheses
%     (P : ∀ J : Finset ι, Measure (∀ j : J, α j))
%     (hP : IsProjectiveMeasureFamily P) :
%     Measure (∀ i, α i) :=
%   Measure.ofAddContent setSemiringCylinders
%     generateFrom_cylinders (kolContent hP)
%     (kolContent_sigma_subadditive_of_innerRegular hP
%       fun J => innerRegular_isCompact_isClosed_
%       measurableSet_of_complete_countable (P J))
% \end{minted}
\sloppy Let us look closer at the formalized proof:
\leaninline{kolContent_sigma_subadditive_of_innerRegular} has the
same hypotheses as
\leaninline{kolContent_sigma_additive_of_innerRegular},
except \leaninline{h_disj}. So, the lemma in the last brackets shows
$\sigma$-sub-additivity of the content $P$ from
Theorem~\ref{T1}. Then, \leaninline{generateFrom_cylinders} gives
the \leaninline{MeasurableSpace} on which we define the
\leaninline{addContent}.


\section{Gaussian Measures and characteristic functions}

\color{blue}

\subsection{Real Gaussian measures}

\begin{definition}[Real Gaussian measure]\label{def:gaussianReal}
  % \mathlibok
  % %  \lean{ProbabilityTheory.gaussianReal}
  The real Gaussian measure with mean $\mu \in \mathbb{R}$ and variance $\sigma^2 > 0$ is the measure on $\mathbb{R}$ with density $\frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right)$ with respect to the Lebesgue measure.
  The real Gaussian measure with mean $\mu \in \mathbb{R}$ and variance $0$ is the Dirac measure $\delta_\mu$.
  We denote this measure by $\mathcal{N}(\mu, \sigma^2)$.
\end{definition}


\begin{lemma}\label{lem:charFun_gaussianReal}
  % %  \uses{def:gaussianReal, def:charFun}
  % \mathlibok
  % %  \lean{ProbabilityTheory.charFun_gaussianReal}
The characteristic function of a real Gaussian measure with mean $\mu$ and variance $\sigma^2$ is given by
$x \mapsto \exp\left(i \mu x - \frac{\sigma^2 x^2}{2}\right)$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:centralMoment_two_mul_gaussianReal}
  %  \uses{def:gaussianReal}
  \leanok
%  %  \lean{ProbabilityTheory.centralMoment_two_mul_gaussianReal}
The central moment of order $2n$ of a real Gaussian measure $\mathcal{N}(\mu, \sigma^2)$ is given by
\begin{align*}
  \mathbb{E}[(X - \mu)^{2n}] = \sigma^{2n} (2n - 1)!! \: ,
\end{align*}
in which $(2n - 1)!! = (2n - 1)(2n - 3) \cdots 3 \cdot 1$ is the double factorial of $2n - 1$.
\end{lemma}

\begin{proof}\leanok
\begin{align*}
	\mathbb{E}[(X - \mu)^{2n}] &= \int_{-\infty}^\infty (x - \mu)^{2n} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}} \mathrm dx \\
	&= \int_{-\infty}^\infty x^{2n} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{x^2}{2 \sigma^2}} \mathrm dx \\
	&= 2 \int_{0}^\infty x^{2n} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{x^2}{2 \sigma^2}} \mathrm dx \\
	&= 2 \int_{0}^\infty {\sqrt{2 \sigma^2 x}}^{2n} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-x)} \frac{\sigma^2}{\sqrt{2 \sigma^2 x'}} \mathrm dx \\
	&= \frac{\sigma^{2n} 2^n}{\sqrt{\pi}} \int_{0}^\infty x^{n - 1/2} e{-x} \mathrm dx \\
	&= \frac{\sigma^{2n} 2^n}{\sqrt{\pi}} \Gamma(n + 1/2) \\
	&= \frac{\sigma^{2n} 2^n}{\Gamma(1/2)} \left( \prod_{k=0}^{n-1} (k + 1/2) \right) \Gamma(1/2) \\
	&= \sigma^{2n} \prod_{k=0}^{n-1} (2k + 1) \\
	&= \sigma^{2n} (2n - 1)!!
\end{align*}
\end{proof}


\subsection{Gaussian measures on a Banach space}

That kind of generality is not needed for this project, but we happen to have results about Gaussian measures on a Banach space in Mathlib, so we will use them.
The main reference for this section is \cite{hairer2009introduction}.

Let $F$ be a separable Banach space.

\begin{definition}[Gaussian measure]\label{def:IsGaussian}
  % %  \uses{def:gaussianReal}
  % \mathlibok
  % %  \lean{ProbabilityTheory.IsGaussian}
A measure $\mu$ on $F$ is Gaussian if for every continuous linear form $L \in F^*$, the pushforward measure $L_* \mu$ is a Gaussian measure on $\mathbb{R}$.
\end{definition}


\begin{lemma}\label{lem:IsGaussian.IsProbabilityMeasure}
  % %  \uses{def:IsGaussian}
  % \mathlibok
A Gaussian measure is a probability measure.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{theorem}\label{thm:isGaussian_iff_charFunDual_eq}
  % %  \uses{def:IsGaussian, def:charFunDual}
  % \mathlibok
  % %  \lean{ProbabilityTheory.isGaussian_iff_charFunDual_eq}
A finite measure $\mu$ on $F$ is Gaussian if and only if for every continuous linear form $L \in F^*$, the characteristic function of $\mu$ at $L$ is
\begin{align*}
  \hat{\mu}(L) = \exp\left(i \mu[L] - \mathbb{V}_\mu[L] / 2\right) \: ,
\end{align*}
in which $\mathbb{V}_\mu[L]$ is the variance of $L$ with respect to $\mu$.
\end{theorem}

\begin{proof}
  %%  \uses{thm:ext_of_charFunDual, lem:charFun_gaussianReal}\leanok

\end{proof}



\paragraph{Transformations of Gaussian measures}

\begin{lemma}\label{lem:isGaussian_map}
  %  \uses{def:IsGaussian}
  \mathlibok
%  %  \lean{ProbabilityTheory.isGaussian_map}
Let $F, G$ be two Banach spaces, let $\mu$ be a Gaussian measure on $F$ and let $T : F \to G$ be a continuous linear map.
Then $T_*\mu$ is a Gaussian measure on $G$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:isGaussian_add_const}
%  %  \uses{def:IsGaussian}
  \leanok
  % This is an instance without name in the code, hence we don't give a %  \lean{...}.
Let $\mu$ be a Gaussian measure on $F$ and let $c \in F$.
Then the measure $\mu$ translated by $c$ (the map of $\mu$ by $x \mapsto x + c$) is a Gaussian measure on $F$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:isGaussian_conv}
 % %  \uses{def:IsGaussian}
  %\mathlibok
  %%  \lean{ProbabilityTheory.isGaussian_conv}
The convolution of two Gaussian measures is a Gaussian measure.
\end{lemma}

\begin{proof}\leanok

\end{proof}



\paragraph{Fernique's theorem}


\begin{theorem}\label{thm:exists_integrable_exp_sq_of_map_rotation_eq_self}
  %\leanok
  % In Mathlib PR #26291
Let $\mu$ be a finite measure on $F$ such that $\mu \times \mu$ is invariant under the rotation of angle $-\frac{\pi}{4}$.
Then there exists $C > 0$ such that the function $x \mapsto \exp (C \Vert x \Vert ^ 2)$ is integrable with respect to $\mu$.
\end{theorem}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:IsGaussian.map_rotation_eq_self}
  %%  \uses{def:IsGaussian}
  \leanok
  % In Mathlib PR #26291
For a Gaussian measure $\mu$, $\mu \times \mu$ is invariant by rotation.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:isGaussian_conv}

\end{proof}


\begin{theorem}[Fernique's theorem]\label{thm:IsGaussian.exists_integrable_exp_sq}
  %  \uses{def:IsGaussian}
  \leanok
  % In Mathlib PR #26291
For a Gaussian measure, there exists $C > 0$ such that the function $x \mapsto \exp (C \Vert x \Vert ^ 2)$ is integrable.
\end{theorem}

\begin{proof}\leanok
  %  \uses{thm:isGaussian_iff_charFunDual_eq, lem:IsGaussian.IsProbabilityMeasure, thm:exists_integrable_exp_sq_of_map_rotation_eq_self, lem:IsGaussian.map_rotation_eq_self}

\end{proof}


\begin{lemma}\label{lem:IsGaussian.memLp_id}
  %  \uses{def:IsGaussian}
  \leanok
  %  \lean{ProbabilityTheory.IsGaussian.memLp_id}
A Gaussian measure $\mu$ has finite moments of all orders.
In particular, there is a well defined mean $m_\mu := \mu[\mathrm{id}]$, and for all $L \in F^*$, $\mu[L] = L(m_\mu)$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{thm:IsGaussian.exists_integrable_exp_sq}

\end{proof}

A Gaussian measure has finite second moment by Lemma~\ref{lem:IsGaussian.memLp_id}, hence its covariance bilinear form is well defined.


\subsection{Gaussian measures on a finite dimensional Hilbert space}

We specialize directly from Banach space to finite dimensional Hilbert space since that's what we need in this project, although there are results for Gaussian measures on infinite dimensional Hilbert spaces that would worth stating.

\begin{lemma}\label{lem:isGaussian_iff_charFun_eq}
  %  \uses{def:IsGaussian, def:charFunDual, def:charFun}
  \leanok
  %  \lean{ProbabilityTheory.isGaussian_iff_charFun_eq}
A finite measure $\mu$ on a Hilbert space $E$ is Gaussian if and only if for every $t \in E$, the characteristic function of $\mu$ at $t$ is
\begin{align*}
  \hat{\mu}(t) =  \exp\left(i \mu[\langle t, \cdot \rangle] - \mathbb{V}_\mu[\langle t, \cdot \rangle] / 2\right) \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{thm:isGaussian_iff_charFunDual_eq}
By Theorem~\ref{thm:isGaussian_iff_charFunDual_eq}, $\mu$ is Gaussian iff for every continuous linear form $L \in E^*$, the characteristic function of $\mu$ at $L$ is
\begin{align*}
  \hat{\mu}(L) = \exp\left(i \mu[L] - \mathbb{V}_\mu[L] / 2\right) \: .
\end{align*}
Every continuous linear form $L \in E^*$ can be written as $L(x) = \langle t, x \rangle$ for some $t \in E$, hence we have that $\mu$ is Gaussian iff for every $t \in E$,
\begin{align*}
  \hat{\mu}(t) = \exp\left(i \mu[\langle t, \cdot \rangle] - \mathbb{V}_\mu[\langle t, \cdot \rangle] / 2\right) \: .
\end{align*}
\end{proof}

Let $E$ be a separable Hilbert space. We denote by $\langle \cdot, \cdot \rangle$ the inner product on $E$ and by $\Vert \cdot \Vert$ the associated norm.

\begin{lemma}\label{lem:IsGaussian.charFun_eq}
  %  \uses{def:IsGaussian, def:charFun, def:covInnerBilin}
  \leanok
  %  \lean{ProbabilityTheory.IsGaussian.charFun_eq}
The characteristic function of a Gaussian measure $\mu$ on $E$ is given by
\begin{align*}
  \hat{\mu}(t) = \exp\left(i \langle t, m_\mu \rangle - \frac{1}{2} C'_\mu(t, t)\right) \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:isGaussian_iff_charFun_eq, lem:IsGaussian.memLp_id, lem:covarianceBilin_same_eq_variance}
By Lemma~\ref{lem:isGaussian_iff_charFun_eq}, for every $t \in E$,
\begin{align*}
  \hat{\mu}(t) = \exp\left(i \mu[\langle t, \cdot \rangle] - \mathbb{V}_\mu[\langle t, \cdot \rangle] / 2\right) \: .
\end{align*}
By Lemma~\ref{lem:IsGaussian.memLp_id}, $\mu$ has finite first moment and $\mu[\langle t, \cdot \rangle] = \langle t, m_\mu \rangle$. By the same lemma, $\mu$ has finite second moment and for any $t$ we have $\mathbb{V}_\mu[\langle t, \cdot\rangle] = C'_\mu(t, t)$.
\end{proof}

\begin{lemma}\label{lem:isGaussian_iff_gaussian_charFun}
  %  \uses{def:IsGaussian, def:charFun, def:covMatrix}
  \leanok
  %  \lean{ProbabilityTheory.isGaussian_iff_gaussian_charFun, ProbabilityTheory.gaussian_charFun_congr}
A finite measure $\mu$ on $E$ is Gaussian if and only if there exists $m \in E$ and $C$ positive semidefinite such that for all $t \in E$, the characteristic function of $\mu$ at $t$ is
\begin{align*}
  \hat{\mu}(t) = \exp\left(i \langle t, m \rangle - \frac{1}{2} C(t, t)\right) \: ,
\end{align*}
If that's the case, then $m = m_\mu$ and $C = C'_\mu$.
\end{lemma}

Note that this lemma does not say that there exists a Gaussian measure for any such $m$ and $C$.
We will prove that later.

\begin{proof}\leanok
  %  \uses{lem:IsGaussian.charFun_eq, lem:charFun_map_eq_charFunDual_smul, thm:ext_of_charFun}
Lemma~\ref{lem:IsGaussian.charFun_eq} states that the characteristic function of a Gaussian measure has the wanted form.

Suppose now that there exists $m \in E$ and $C$ positive semidefinite such that for all $t \in E$, $\hat{\mu}(t) = \exp\left(i \langle t, m \rangle - \frac{1}{2} C(t, t)\right)$.

We need to show that for all $L \in E^*$, $L_*\mu$ is a Gaussian measure on $\mathbb{R}$.
Such an $L$ can be written as $\langle u, \cdot \rangle$ for some $u \in E$.
Let then $u \in E$. We compute the characteristic function of $\langle u, \cdot\rangle_*\mu$ at $x \in \mathbb{R}$ with Lemma~\ref{lem:charFun_map_eq_charFunDual_smul}:
\begin{align*}
  \widehat{\langle u, \cdot\rangle_*\mu}(x)
  &= \hat{\mu}(x \cdot u)
  \\
  &= \exp\left(i x \langle u, m \rangle - \frac{1}{2} x^2 C(u, u)\right)
  \: .
\end{align*}
This is the characteristic function of a Gaussian measure on $\mathbb{R}$ with mean $\langle u, m \rangle$ and variance $C(u, u)$.
By Theorem~\ref{thm:ext_of_charFun}, $\langle u, \cdot\rangle_*\mu$ is Gaussian, hence $\mu$ is Gaussian.

By Lemma~\ref{lem:IsGaussian.charFun_eq}, we deduce that for any $t \in E$ we have
$$\exp\left(i\langle t, m \rangle - \frac{1}{2} C(t, t)\right) = \exp\left(i\langle t, m_\mu \rangle - \frac{1}{2} C'_\mu(t, t)\right).$$
In particular, for any $t$ there exists $n_t \in \mathbb{Z}$ such that
$$i\langle t, m \rangle - \frac{1}{2} C(t, t) = i\langle t, m_\mu \rangle - \frac{1}{2} C'_\mu(t, t) + 2i\pi n_t.$$
We deduce that $n$ is a continuous map from $E$ to $\mathbb{Z}$, and thus must be constant because $E$ is connected. By looking at the value at $t = 0$, we deduce that for any $t$, $n_t = 0$. Looking at real and imaginary parts we obtain that for any $t$,
$$\langle t, m \rangle = \langle t, m_\mu \rangle \quad \text{and} \quad C(t, t) = C'_\mu(t, t).$$
We immediately deduce that $m = m_\mu$. Moreover, because $C$ and $C'_\mu$ are symmetric, they are characterized by their values on the diagonal. Indeed, for any $x, y$,
$$C(x, y) = \frac{1}{2} (C(x + y, x + y) - C(x, x) - C(y, y)).$$
We deduce that $C = C'_\mu$.
\end{proof}

\begin{lemma}\label{lem:IsGaussian.ext_iff}
  %  \uses{def:IsGaussian, def:covInnerBilin}
  \leanok
  %  \lean{ProbabilityTheory.IsGaussian.ext, ProbabilityTheory.IsGaussian.ext_iff}
Two Gaussian measures $\mu$ and $\nu$ on a separable Hilbert space are equal if and only if they have same mean and same covariance.
\end{lemma}

\begin{proof}\leanok
  %  \uses{thm:ext_of_charFun, lem:IsGaussian.charFun_eq}
The forward direction is immediate.

For the converse direction, it is enough to show that $\mu$ and $\nu$ have the same characteristic function by Theorem~\ref{thm:ext_of_charFun}. As they are both Gaussian, their characteristic functions only depend on their mean and covariance by Lemma~\ref{lem:IsGaussian.charFun_eq}. Thus they are equal.
\end{proof}


\begin{definition}[Standard Gaussian measure]\label{def:stdGaussian}
  %  \uses{def:gaussianReal}
  \leanok
  %  \lean{ProbabilityTheory.stdGaussian}
Let $(e_1, \ldots, e_d)$ be an orthonormal basis of $E$ and let $\mu$ be the standard Gaussian measure on $\mathbb{R}$.
The standard Gaussian measure on $E$ is the pushforward measure of the product measure $\mu \times \ldots \times \mu$ by the map $x \mapsto \sum_{i=1}^d x_i \cdot e_i$.
\end{definition}

The fact that this definition does not depend on the choice of basis will be a consequence of the fact that its characteristic function does not depend on the basis.


\begin{lemma}\label{lem:integral_eval_pi}
  \leanok
  %  \lean{ProbabilityTheory.integral_eval_pi}
For $\mu_1, \ldots, \mu_d$ probability measures on $\mathbb{R}$ and $f : \mathbb{R} \to \mathbb{R}$ integrable with respect to $\mu_i$, we have
\begin{align*}
  \int_x f(x_i) \, d(\mu_1 \times \ldots \times \mu_d)(x)
  = \int_x f(x) \, d\mu_i
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
As $f$ is integrable, we can use Fubini theorem to obtain that
$$\int f(x_i) \, d(\mu_1 \times \ldots \times \mu_d)(x) = \int f(x) \, d\mu_i(x) \times \prod_{j \ne i} \int 1 \, d\mu_j(x) = \int f(x) \, d\mu_i(x)$$
because the $\mu_j$s are probability measures.
\end{proof}


\begin{lemma}\label{lem:isCentered_stdGaussian}
  %  \uses{def:stdGaussian}
  \leanok
  %  \lean{ProbabilityTheory.isCentered_stdGaussian}
The standard Gaussian measure on $E$ is centered, i.e., $\mu[L] = 0$ for every $L \in E^*$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:integral_eval_pi}

\end{proof}


\begin{lemma}\label{lem:isProbabilityMeasure_stdGaussian}
  %  \uses{def:stdGaussian}
  \leanok
  %  \lean{ProbabilityTheory.isProbabilityMeasure_stdGaussian}
The standard Gaussian measure is a probability measure.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:charFun_stdGaussian}
  %  \uses{def:stdGaussian, def:charFun}
  \leanok
  %  \lean{ProbabilityTheory.charFun_stdGaussian}
The characteristic function of the standard Gaussian measure on $E$ is given by
\begin{align*}
  \hat{\mu}(t) = \exp\left(-\frac{1}{2} \Vert t \Vert^2 \right) \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:charFun_gaussianReal}
Denote by $\nu$ the standard Gaussian measure on $\mathbb{R}$. This is a straightforward computation:
\begin{align*}
  \hat{\mu}(t) = \int \exp\left(i\langle t, \sum_{j=1}^d x_j \cdot e_j \rangle\right) d(\nu \times \ldots \times \nu)(dx) &= \int \exp\left(\sum_{j=1}^d ix_j\langle t, e_j \rangle\right) d(\nu \times \ldots \times \nu)(dx) \\
  &= \int \prod_{j=1}^d \exp\left(ix_j\langle t, e_j \rangle\right) d(\nu \times \ldots \times \nu)(dx) \\
  &= \prod_{j=1}^d \int \exp\left(ix\langle t, e_j \rangle\right) d\nu(x) \\
  &= \prod_{j=1}^d \exp\left(-\frac{\langle t, e_j \rangle^2}{2}\right) \\
  &= \exp\left(-\frac{1}{2} \Vert t \Vert^2 \right).
\end{align*}
\end{proof}


\begin{lemma}\label{lem:isGaussian_stdGaussian}
  %  \uses{def:stdGaussian, def:IsGaussian}
  \leanok
  %  \lean{ProbabilityTheory.isGaussian_stdGaussian}
The standard Gaussian measure on $E$ is a Gaussian measure.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:isGaussian_iff_gaussian_charFun, lem:charFun_stdGaussian, lem:isProbabilityMeasure_stdGaussian}
Since the standard Gaussian is a probability measure (hence finite), we can apply Lemma~\ref{lem:isGaussian_iff_gaussian_charFun} that states that it suffices to show that the characteristic function has a particular form.
That form is given by Lemma~\ref{lem:charFun_stdGaussian}, taking $m=0$ and $C = \langle\cdot, \cdot\rangle$.
\end{proof}


\begin{lemma}\label{lem:integral_id_stdGaussian}
  %  \uses{def:stdGaussian}
  \leanok
  %  \lean{ProbabilityTheory.integral_id_stdGaussian}
The mean of the standard Gaussian measure is $0$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:integral_eval_pi}

\end{proof}


\begin{lemma}\label{lem:covMatrix_stdGaussian}
  %  \uses{def:stdGaussian, def:covMatrix}
  \leanok
  %  \lean{ProbabilityTheory.covMatrix_stdGaussian}
The covariance matrix of the standard Gaussian measure is the identity matrix.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:isGaussian_iff_gaussian_charFun, lem:charFun_stdGaussian}
From Lemma~\ref{lem:charFun_stdGaussian}, we know that for all $t \in \mathbb{R}$,
$$\hat{\mu}(t) = \exp\left(-\frac{\|t\|^2}{2}\right) = \exp\left(-\frac{\langle t, \mathrm{I}t\rangle}{2}\right).$$
As the identity is positive semidefinite, we deduce from Lemma~\ref{lem:isGaussian_iff_gaussian_charFun} that $\Sigma_\mu$ is the identity matrix.
\end{proof}


\begin{definition}[Multivariate Gaussian]\label{def:multivariateGaussian}
  %  \uses{def:stdGaussian}
  \leanok
  %  \lean{ProbabilityTheory.multivariateGaussian}
The multivariate Gaussian measure on $\mathbb{R}^d$ with mean $m \in \mathbb{R}^d$ and covariance matrix $\Sigma \in \mathbb{R}^{d \times d}$, with $\Sigma$ positive semidefinite, is the pushforward measure of the standard Gaussian measure on $\mathbb{R}^d$ by the map $x \mapsto m + \Sigma^{1/2} x$.
We denote this measure by $\mathcal{N}(m, \Sigma)$.
\end{definition}


\begin{lemma}\label{lem:integral_id_multivariateGaussian}
  %  \uses{def:multivariateGaussian}
  \leanok
  %  \lean{ProbabilityTheory.integral_id_multivariateGaussian}
The mean of the multivariate Gaussian measure $\mathcal{N}(m, \Sigma)$ is $m$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:integral_id_stdGaussian}

\end{proof}


\begin{lemma}\label{lem:covMatrix_multivariateGaussian}
  %  \uses{def:multivariateGaussian}
  \leanok
  %  \lean{ProbabilityTheory.covInnerBilin_multivariateGaussian}
The covariance matrix of the multivariate Gaussian measure $\mathcal{N}(m, \Sigma)$ is $\Sigma$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:covMatrix_stdGaussian}

\end{proof}


\begin{lemma}\label{lem:isGaussian_multivariateGaussian}
  %  \uses{def:multivariateGaussian, def:IsGaussian}
  \leanok
  %  \lean{ProbabilityTheory.isGaussian_multivariateGaussian}
A multivariate Gaussian measure is a Gaussian measure.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:isGaussian_stdGaussian, lem:isGaussian_add_const, lem:isGaussian_map}
The multivariate Gaussian measure is the pushforward of the standard Gaussian measure by an affine map, and is thus Gaussian by Lemma~\ref{lem:isGaussian_add_const} and Lemma~\ref{lem:isGaussian_map}.
\end{proof}


\begin{theorem}\label{thm:charFun_multivariateGaussian}
  %  \uses{def:multivariateGaussian, def:charFun}
  \leanok
  %  \lean{ProbabilityTheory.charFun_multivariateGaussian}
The characteristic function of a multivariate Gaussian measure $\mathcal{N}(m, \Sigma)$ is given by
\begin{align*}
  \hat{\mu}(t) = \exp\left(i \langle m, t \rangle - \frac{1}{2} \langle t, \Sigma t \rangle\right)
  \: .
\end{align*}
\end{theorem}

\begin{proof}\leanok
  %  \uses{lem:isGaussian_multivariateGaussian, lem:IsGaussian.charFun_eq, lem:integral_id_multivariateGaussian, lem:covMatrix_multivariateGaussian}
Since the multivariate Gaussian measure is a Gaussian measure, we can apply Lemma~\ref{lem:IsGaussian.charFun_eq} to it.
It suffices then to show that the mean and the covariance matrix of the multivariate Gaussian measure are equal to $m$ and $\Sigma$, respectively.
This is given by Lemma~\ref{lem:integral_id_multivariateGaussian} and Lemma~\ref{lem:covMatrix_multivariateGaussian}.
\end{proof}


\subsection{Gaussian processes}
\label{sec:gaussian_processes}

\begin{definition}[Gaussian process]\label{def:IsGaussianProcess}
  %  \uses{def:IsGaussian}
  \leanok
  %  \lean{ProbabilityTheory.IsGaussianProcess}
A process $X : T \to \Omega \to E$ is Gaussian if for every finite subset $t_1, \ldots, t_n \in T$, the random vector $(X_{t_1}, \ldots, X_{t_n})$ has a Gaussian distribution.
\end{definition}


\begin{lemma}\label{lem:isGaussianProcess_of_modification}
  %  \uses{def:IsGaussianProcess}
  \leanok
  %  \lean{ProbabilityTheory.IsGaussianProcess.modification}
Let $X, Y : T \to \Omega \to E$ be two stochastic processes that are modifications of each other (that is, for all $t \in T$, $X_t =_{a.e.} Y_t$).
If $X$ is a Gaussian process, then $Y$ is a Gaussian process as well.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:map_eq_of_modification}
Being a Gaussian process is defined in terms of the distribution of finite-dimensional random vectors.
By Lemma~\ref{lem:map_eq_of_modification}, the random vector $(Y_{t_1}, \ldots, Y_{t_n})$ has the same distribution as the random vector $(X_{t_1}, \ldots, X_{t_n})$ for all $t_1, \ldots, t_n \in T$.
\end{proof}

\color{black}

\section{The Kolmogorov-Chentsov Theorem}
\label{S:continuity}

We follow the proof of the Kolmogorov-Chentsov theorem from \cite{kratschmer2023kolmogorov}.
That proof notably uses the chaining technique developed by Talagrand \cite{talagrand2022upper}.

That theorem is about stochastic processes $X : T \to \Omega \to E$, where $\Omega$ is a measurable space with a probability measure $\mathbb{P}$, the index set $T$ is a metric space with distance $d_T$, and $E$ is also a metric space with distance $d_E$, on which we put the Borel $\sigma$-algebra.

The main result is Theorem~\ref{thm:countable_set_bound}.
Under an assumption on the covering number of $T$, for a process $X$ that satisfies the Kolmogorov condition $\mathbb{E}[d_E(X_s, X_t)^p] \le M d_T(s, t)^q$ (see Definition~\ref{def:IsKolmogorovProcess}),the theorem gives a finite bound on the expectation of the supremum of the ratio
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in T'} \frac{d_E(X_s, X_t)^p}{d_T(s, t)^{\beta p}} \right]
  \: ,
\end{align*}
for $T'$ a countable subset of $T$.
As a corollary, we obtain that there exists a modification of $X$ with Hölder continuous paths.

In Lean, we will use the typeclass \texttt{PseudoEMetricSpace} for both $T$ and $E$ as long as possible, and then specialize to \texttt{EMetricSpace} (or perhaps even \texttt{MetricSpace}) when we need the stronger properties of a metric space.
For example, to prove the existence of a modification of a stochastic process, we will eventually use the fact that $d_E(x, y) = 0$ implies $x = y$, which does not hold in a pseudo-metric space.
All distances will be expressed with \texttt{edist}, which takes values in \texttt{ENNReal}, and the integrals refer to Lebesgue integrals.

\subsection{Covers and covering numbers}

Let $(E, d_E)$ be a pseudo-metric space.

\begin{definition}[$\varepsilon$-cover]\label{def:IsCover}
  \leanok
  %  \lean{IsCover}
  A set $C \subseteq E$ is an $\varepsilon$-cover of a set $A \subseteq E$ if for every $x \in A$, there exists $y \in C$ such that $d_E(x, y) \le \varepsilon$.
\end{definition}


\begin{definition}[External covering number]\label{def:externalCoveringNumber}
  %  \uses{def:IsCover}
  \leanok
  %  \lean{externalCoveringNumber}
  The external covering number of a set $A \subseteq E$ for $\varepsilon \ge 0$ is the smallest cardinality of an $\varepsilon$-cover of $A$.
  Denote it by $N^{ext}_\varepsilon(A)$.
\end{definition}


\begin{definition}[Internal covering number]\label{def:internalCoveringNumber}
  %  \uses{def:IsCover}
  \leanok
  %  \lean{internalCoveringNumber}
  The internal covering number of a set $A \subseteq E$ for $\varepsilon \ge 0$ is the smallest cardinality of an $\varepsilon$-cover of $A$ which is a subset of $A$.
  Denote it by $N^{int}_\varepsilon(A)$.
\end{definition}


\begin{definition}[Separated set]\label{def:IsSeparated}
  \mathlibok
  %  \lean{Metric.IsSeparated}
A set $c \subseteq E$ is $\varepsilon$-separated if for all $x, y \in c$, $d_E(x, y) > \varepsilon$.
\end{definition}


\begin{definition}[Packing number]\label{def:packingNumber}
  %  \uses{def:IsSeparated}
  \leanok
  %  \lean{packingNumber}
The packing number of a set $A \subseteq E$ for $\varepsilon > 0$ is the largest cardinality of an $\varepsilon$-separated subset of $A$.
Denote it by $P_\varepsilon(A)$.
\end{definition}


\begin{lemma}\label{lem:externalCoveringNumber_le_internalCoveringNumber}
  %  \uses{def:externalCoveringNumber, def:internalCoveringNumber}
  \leanok
  %  \lean{externalCoveringNumber_le_internalCoveringNumber}
$N^{ext}_\varepsilon(A) \le N^{int}_\varepsilon(A)$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:internalCoveringNumber_le_packingNumber}
  %  \uses{def:internalCoveringNumber, def:packingNumber}
  \leanok
  %  \lean{internalCoveringNumber_le_packingNumber}
$N^{int}_\varepsilon(A) \le P_\varepsilon(A)$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:packingNumber_two_le_externalCoveringNumber}
  %  \uses{def:packingNumber, def:externalCoveringNumber}
  \leanok
  %  \lean{packingNumber_two_le_externalCoveringNumber}
$P_{2\varepsilon}(A) \le N^{ext}_\varepsilon(A)$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:internalCoveringNumber_eq_one_of_diam_le}
  %  \uses{def:internalCoveringNumber}
  \leanok
  %  \lean{internalCoveringNumber_eq_one_of_diam_le}
If $\mathrm{diam}(A) \le \varepsilon$ and $A$ is nonempty, then $N^{int}_\varepsilon(A) = 1$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:externalCoveringNumber_mono}
  %  \uses{def:externalCoveringNumber}
  \leanok
  %  \lean{externalCoveringNumber_mono_set}
For $B \subseteq A$, $N^{ext}_\varepsilon(B) \le N^{ext}_{\varepsilon}(A)$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:internalCoveringNumber_subset_le}
  %  \uses{def:internalCoveringNumber}
  \leanok
  %  \lean{internalCoveringNumber_subset_le}
For $B \subseteq A$, $N^{int}_\varepsilon(B) \le N^{int}_{\varepsilon/2}(A)$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:internalCoveringNumber_le_packingNumber, lem:packingNumber_two_le_externalCoveringNumber, lem:externalCoveringNumber_mono, lem:externalCoveringNumber_le_internalCoveringNumber}
\begin{align*}
  N^{int}_\varepsilon(B)
  &\le P_{\varepsilon}(B)
  \le N^{ext}_{\varepsilon/2}(B)
  \le N^{ext}_{\varepsilon/2}(A)
  \le N^{int}_{\varepsilon/2}(A)
  \: .
\end{align*}
\end{proof}


\subsection{Covering number and volume}

In this section $E$ is a finite dimensional inner product space, with dimension $d$.

\begin{lemma}\label{lem:volume_le_of_isCover}
  %  \uses{def:IsCover}
  \leanok
  %  \lean{volume_le_of_isCover}
Let $A \subseteq E$ and $C \subseteq E$ be a finite $\varepsilon$-cover of $A$. Denote by $V(A)$ the volume of $A$.
Then $V(A) \le \vert C \vert V(B_\varepsilon)$, in which $B_\varepsilon$ is the closed ball of radius $\varepsilon$ in $E$.
\end{lemma}

\begin{proof}\leanok
Since $C$ is a cover of $A$, $A$ is a subset of the union of the closed balls $B_\varepsilon(c)$ for $c \in C$. Then
\begin{align*}
  V(A) \le V(\bigcup_{c \in C} B_\varepsilon(c))
  &\le \sum_{c \in C} V(B_\varepsilon(c))
  = \vert C \vert V(B_\varepsilon)
  \: .
\end{align*}
\end{proof}

The volume of $B_\varepsilon$ is given by the following formula (see \href{https://leanprover-community.github.io/mathlib4_docs/Mathlib/MeasureTheory/Measure/Lebesgue/VolumeOfBalls.html#InnerProductSpace.volume_closedBall}{InnerProductSpace.volume\_closedBall}).
\begin{align*}
  V(B_\varepsilon) = \frac{\pi^{d/2}}{\Gamma(d/2 + 1)} \varepsilon^d
  \: .
\end{align*}


\begin{lemma}\label{lem:volume_le_externalCoveringNumber_mul}
  %  \uses{def:externalCoveringNumber}
  \leanok
  %  \lean{volume_le_externalCoveringNumber_mul}
If $0 < \varepsilon$ then $V(A) \le N^{ext}_\varepsilon(A) V(B_\varepsilon)$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:volume_le_of_isCover}
If $A$ has no $\varepsilon$-cover, then $N^{ext}_\varepsilon(A) = \infty$, and because $0 < \varepsilon$, we have that $0 < V(B_\varepsilon)$, so the right-hand side is infinite and the inequality follows.

Otherwise there exists an $\varepsilon$-cover which realizes $N^{ext}_\varepsilon(A)$. We can conclude by Lemma~\ref{lem:volume_le_of_isCover}.
\end{proof}


\begin{lemma}\label{lem:le_volume_of_isSeparated}
  %  \uses{def:IsSeparated}
  \leanok
  %  \lean{le_volume_of_isSeparated}
Let $A \subseteq E$ and let $S \subseteq A$ be an $\varepsilon$-separated set.
Then $\vert S \vert V(B_{\varepsilon/2}) \le V(A + B_{\varepsilon/2})$.
\end{lemma}

\begin{proof}\leanok
Since $S$ is $\varepsilon$-separated, the closed balls $B_{\varepsilon/2}(s)$ for $s \in S$ are pairwise disjoint.
Furthermore, these balls are contained in $A + B_{\varepsilon/2}$.
Thus, we have
\begin{align*}
  \vert S \vert V(B_{\varepsilon/2})
  &= \sum_{s \in S} V(B_{\varepsilon/2}(s))
  = V(\bigcup_{s \in S} B_{\varepsilon/2}(s))
  \le V(A + B_{\varepsilon/2})
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:packingNumber_mul_le_volume}
  %  \uses{def:packingNumber}
  \leanok
  %  \lean{packingNumber_mul_le_volume}
$P_\varepsilon(A) V(B_{\varepsilon/2}) \le V(A + B_{\varepsilon/2})$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:le_volume_of_isSeparated}
Use Lemma~\ref{lem:le_volume_of_isSeparated} with $S$ an $\varepsilon$-separated set of maximal cardinality.
\end{proof}


\begin{lemma}\label{lem:volume_div_le_internalCoveringNumber}
  %  \uses{def:internalCoveringNumber}
  \leanok
  %  \lean{volume_div_le_internalCoveringNumber}
If $0 < \varepsilon$ then $\frac{V(A)}{V(B_\varepsilon)} \le N^{int}_\varepsilon(A)$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:volume_le_externalCoveringNumber_mul, lem:externalCoveringNumber_le_internalCoveringNumber}
We have $\frac{V(A)}{V(B_\varepsilon)} \le N^{ext}_\varepsilon(A)$ by Lemma~\ref{lem:volume_le_externalCoveringNumber_mul} and $N^{ext}_\varepsilon(A) \le N^{int}_\varepsilon(A)$ by Lemma~\ref{lem:externalCoveringNumber_le_internalCoveringNumber}.
\end{proof}


\begin{lemma}\label{lem:internalCoveringNumber_le_volume_div}
  %  \uses{def:internalCoveringNumber}
  \leanok
  %  \lean{internalCoveringNumber_le_volume_div}
If $0 < \varepsilon < \infty$ then $N^{int}_\varepsilon(A) \le \frac{V(A + B_{\varepsilon/2})}{V(B_{\varepsilon/2})}$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:packingNumber_mul_le_volume, lem:internalCoveringNumber_le_packingNumber}
We have $N^{int}_\varepsilon(A) \le P_\varepsilon(A)$ by Lemma~\ref{lem:internalCoveringNumber_le_packingNumber} and $P_\varepsilon(A) \le \frac{V(A + B_{\varepsilon/2})}{V(B_{\varepsilon/2})}$ by Lemma~\ref{lem:packingNumber_mul_le_volume}.
\end{proof}


\begin{lemma}\label{lem:internalCoveringNumber_closedBall_ge}
  %  \uses{def:internalCoveringNumber}
  \leanok
  %  \lean{internalCoveringNumber_closedBall_ge}
$N_\varepsilon^{int}(B_1) \ge \frac{1}{\varepsilon^d}$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:volume_div_le_internalCoveringNumber}
By Lemma~\ref{lem:volume_div_le_internalCoveringNumber},
\begin{align*}
  N^{int}_\varepsilon(B_1)
  &\ge \frac{V(B_1)}{V(B_\varepsilon)}
  = \frac{1}{\varepsilon^d}
  \: .
\end{align*}

\end{proof}


\begin{lemma}\label{lem:internalCoveringNumber_closedBall_le}
  %  \uses{def:internalCoveringNumber}
  \leanok
  %  \lean{internalCoveringNumber_closedBall_le}
$N_\varepsilon^{int}(B_1) \le \left(\frac{2}{\varepsilon} + 1\right)^d$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:internalCoveringNumber_le_volume_div}
By Lemma~\ref{lem:internalCoveringNumber_le_volume_div},
\begin{align*}
  N^{int}_\varepsilon(B_1)
  &\le \frac{V(B_1 + B_{\varepsilon/2})}{V(B_{\varepsilon/2})}
  = \frac{V(B_{1 + \varepsilon/2})}{V(B_{\varepsilon/2})}
  = \frac{(1 + \varepsilon/2)^d}{(\varepsilon/2)^d}
  \: .
\end{align*}
\end{proof}


\subsection{Bounded internal covering number}

\begin{definition}[Bounded internal covering number]\label{def:HasBoundedInternalCoveringNumber}
  %  \uses{def:internalCoveringNumber}
  \leanok
  %  \lean{HasBoundedInternalCoveringNumber}
  Let $\mathrm{diam}(A)$ be the diameter of $A \subseteq E$, i.e. $\mathrm{diam}(A) = \sup_{x,y \in A} d_E(x, y)$.
  A set $A \subseteq E$ has bounded internal covering number with constant $c>0$ and exponent $t>0$ if for all $\varepsilon \in (0, \mathrm{diam}(A)]$, $N^{int}_\varepsilon(A) \le c \varepsilon^{-t}$.
\end{definition}


\begin{lemma}\label{lem:hasBoundedInternalCoveringNumber_unitInterval}
  %  \uses{def:HasBoundedInternalCoveringNumber}
  \leanok
  %  \lean{internalCoveringNumber_Icc_zero_one_le_one_div}
The unit interval $I = [0, 1] \subseteq \mathbb{R}$ has bounded internal covering number with constant $1$ and exponent $1$: for $\varepsilon \le 1$, $N^{int}_\varepsilon(I) \le 1/\varepsilon$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:hasBoundedInternalCoveringNumber_subset}
  %  \uses{def:HasBoundedInternalCoveringNumber}
  \leanok
  %  \lean{HasBoundedInternalCoveringNumber.subset}
If $A$ has bounded internal covering number with constant $c>0$ and exponent $d>0$, then for all $B \subseteq A$, $B$ has bounded internal covering number with constant $2^d c$ and exponent $d$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:internalCoveringNumber_subset_le}
\begin{align*}
  N^{int}_\varepsilon(B)
  &\le N^{int}_{\varepsilon/2}(A)
  \le c (\varepsilon/2)^{-d}
  = 2^d c \varepsilon^{-d}
  \: .
\end{align*}
\end{proof}


\subsection{Chaining}

\subsection{Chaining sequence}


\begin{definition}\label{def:nearestPt}
  \leanok
  %  \lean{nearestPt}
Let $S$ be a finite set of $E$ and $x \in E$.
We denote by $\pi(x, S)$ the point in $S$ which is closest to $x$, i.e. a point such that $d_E(x, S) = \min_{y \in S} d_E(x, y)$ (chosen arbitrarily among the minima if there are several).
\end{definition}


\begin{lemma}\label{lem:dist_nearestPt_le}
  %  \uses{def:nearestPt}
  \leanok
  %  \lean{edist_nearestPt_le}
Let $S$ be a finite set of $E$ and $x \in E$.
Then for all $y \in S$, $d_E(x, \pi(x, S)) \le d_E(x, y)$.
\end{lemma}

\begin{proof}\leanok
By definition.
\end{proof}


\begin{lemma}\label{lem:dist_nearestPt_of_isCover}
  %  \uses{def:nearestPt, def:IsCover}
  \leanok
  %  \lean{edist_nearestPt_of_isCover}
Let $C_\varepsilon$ be a finite $\varepsilon$-cover of $A \subseteq E$ (assuming such a finite cover exists).
Then for all $x \in A$, $d_E(x, \pi(x, C_\varepsilon)) \le \varepsilon$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{definition}[Chaining sequence]\label{def:chainingSequence}
  %  \uses{def:nearestPt, def:IsCover}
  \leanok
  %  \lean{chainingSequence}
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers, $C_n$ a finite $\varepsilon_n$-cover of $A \subseteq E$ with $C_n \subseteq A$ and $x \in C_k$ for some $k \in \mathbb{N}$.
We define the chaining sequence of $x$, denoted $(\bar{x}_i)_{i \le k}$, recursively as follows: $\bar{x}_k = x$ and for $i < k$, $\bar{x}_i = \pi(\bar{x}_{i+1}, C_i)$.
\end{definition}


\begin{lemma}\label{lem:chainingSequence_mem}
  %  \uses{def:chainingSequence}
  \leanok
  %  \lean{chainingSequence_mem}
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers, $C_n$ a finite $\varepsilon_n$-cover of $A \subseteq E$ with $C_n \subseteq A$ and $x \in C_k$ for some $k \in \mathbb{N}$.
Then for all $i \le k$, $\bar{x}_i\in C_i$.
\end{lemma}

\begin{proof}\leanok
By definition.
\end{proof}


\begin{lemma}\label{lem:dist_chainingSequence_add_one}
  %  \uses{def:chainingSequence}
  \leanok
  %  \lean{edist_chainingSequence_add_one}
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers, $C_n$ a finite $\varepsilon_n$-cover of $A \subseteq E$ with $C_n \subseteq A$ and $x \in C_k$ for some $k \in \mathbb{N}$.
Then for all $i < k$, $d_E(\bar{x}_i, \bar{x}_{i+1}) \le \varepsilon_i$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:dist_nearestPt_of_isCover, lem:chainingSequence_mem}
Apply Lemma~\ref{lem:dist_nearestPt_of_isCover} with $S = C_i$ and $x = \bar{x}_{i+1}$.
\end{proof}


\begin{lemma}\label{lem:dist_chainingSequence_le_sum}
  %  \uses{def:chainingSequence}
  \leanok
  %  \lean{edist_chainingSequence_le_sum}
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers, $C_n$ a finite $\varepsilon_n$-cover of $A \subseteq E$ with $C_n \subseteq A$ and $x \in C_k$ for some $k \in \mathbb{N}$.
Then for $m \le k$, $d_E(\bar{x}_m, x) \le \sum_{i=m}^{k-1} \varepsilon_i$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:dist_chainingSequence_add_one}
By the triangle inequality and Lemma~\ref{lem:dist_chainingSequence_add_one},
\begin{align*}
  d_E(\bar{x}_m, x)
  \le \sum_{i=m}^{k-1} d_E(\bar{x}_i, \bar{x}_{i+1})
  \le \sum_{i=m}^{k-1} \varepsilon_i
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:dist_chainingSequence_le}
  %  \uses{def:chainingSequence}
  \leanok
  %  \lean{edist_chainingSequence_le}
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers, $C_n$ a finite $\varepsilon_n$-cover of $A \subseteq E$ with $C_n \subseteq A$.
Let $m, k, \ell \in \mathbb{N}$ with $m \le k$ and $m \le \ell$ and let $x \in C_k$ and $y \in C_\ell$.
Then
\begin{align*}
  d_E(\bar{x}_m, \bar{y}_m)
  &\le d_E(x, y) + \sum_{i=m}^{k-1} \varepsilon_i + \sum_{j=m}^{\ell-1} \varepsilon_j
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:dist_chainingSequence_le_sum}
Triangle inequality and Lemma~\ref{lem:dist_chainingSequence_le_sum}.
\end{proof}


\begin{corollary}\label{cor:dist_chainingSequence_pow_two_le}
  %  \uses{def:chainingSequence}
  \leanok
  %  \lean{edist_chainingSequence_pow_two_le}
For $\varepsilon_n = \varepsilon_0 2^{-n}$, with the hypothesis of Lemma~\ref{lem:dist_chainingSequence_le}, we have
\begin{align*}
  d_E(\bar{x}_m, \bar{y}_m)
  &\le d_E(x, y) + \varepsilon_0 2^{-m+2}
  \: .
\end{align*}
\end{corollary}

\begin{proof}\leanok
  %  \uses{lem:dist_chainingSequence_le}

\end{proof}


\subsection{A subset of pairs}

We will be interested in bounding expressions of the form $\sup_{s,t\in J, d_T(s,t) \le c} d_E(f(s), f(t))$ for a finite set $J$ and some function $f : T \to E$.
This is a supremum over pairs in $J$ and there could be $\vert J \vert^2$ such pairs.
We will build a subset $K$ of $J^2$ which is much smaller, of size linear in $\vert J \vert$, such that its points are not too far apart and
\begin{align*}
  \sup_{s,t\in J, d_T(s,t) \le c} d_E(f(s), f(t))
  & \le 2 \sup_{(s,t) \in K} d_E(f(s), f(t))
  \: .
\end{align*}
The pairs $(s, t) \in K$ will still be close together, in the sense that $d_T(s, t) \le c n$ for some $n$ that is logarithmic in the size of $J$.

For $t \in V \subseteq T$ and $u\ge 0$, we denote by $B_V(t, u)$ the closed ball with center $t$ and radius $u$ in $V$.
That is, $B_V(t, u) = \{s \in V \mid d_T(s, t) \le u\}$.

We want to cover $J$ with balls that have radius logarithmic in the number of points of the ball.

\begin{definition}\label{def:logSizeRadius}
  \leanok
  %  \lean{logSizeRadius}
Let $V$ be a finite subset of a metric space and let $t \in V$ and $a > 1$, $c > 0$.
Let the \emph{log-size radius} of $t$ in $V$, denoted by $r_{V,t}$, be the smallest positive integer $r$ such that $\vert B_V(t, r c) \vert \le a^{r}$.
\end{definition}


\begin{lemma}\label{lem:card_logSizeRadius_ge}
  %  \uses{def:logSizeRadius}
  \leanok
  %  \lean{card_le_logSizeRadius_ge}
$a^{r_{V,t}-1} \le \vert B_V(t, (r_{V,t}-1)c) \vert$~.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:card_logSizeRadius_le}
  %  \uses{def:logSizeRadius}
  \leanok
  %  \lean{card_le_logSizeRadius_le}
$\vert B_V(t, r_{V,t}c) \vert \le a^{r_{V,t}}$~.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{definition}[Log-size ball sequence]\label{def:logSizeBallSequence}
  %  \uses{def:logSizeRadius}
  \leanok
  %  \lean{logSizeBallSeq}
Let $(T,d_T)$ be a metric space and let $J \subseteq T$ be finite, $a,c \in \mathbb R_+$ with $a \ge 1$ and $n \in \{1, 2, ...\}$ such that $|J| \le a^n$.
An log-size ball sequence for $(J, a, c, n)$ is a sequence of $(V_i, t_i, r_i)_{i \in \mathbb{N}}$ such that
\begin{itemize}
  \item $V_0 = J$, $t_0$ is an arbitrary point in $J$,
  \item for all $i$, $r_i$ is the log-size radius of $t_i$ in $V_i$,
  \item $V_{i+1} = V_i \setminus B_{V_i}(t_i, (r_i - 1)c)$, $t_{i+1}$ is arbitrarily chosen in $V_{i+1}$.
\end{itemize}
\end{definition}

A log-size ball sequence gives a partition of $J$ into sets which are contained in balls of radius $(r_i - 1)c$ around $t_i$, and satisfy cardinality constraints.


\begin{lemma}\label{lem:logSizeRadius_logSizeBallSequence_le}
  %  \uses{def:logSizeBallSequence}
  \leanok
  %  \lean{radius_logSizeBallSeq_le}
The radius of a log-size ball sequence $(V_i, t_i, r_i)_{i \in \mathbb{N}}$ for $(J, a, c, n)$ satisfies $r_i \le n$ for all $i \in \mathbb{N}$.
\end{lemma}

\begin{proof}
\leanok
Since $|J| \le a^n$, we have $\vert B_{V_i}(t_i, n c) \vert \le \vert J \vert \le a^{n}$.
\end{proof}


\begin{lemma}\label{lem:logSizeBallSequence_V_anti}
  %  \uses{def:logSizeBallSequence}
  \leanok
  %  \lean{finset_logSizeBallSeq_add_one_subset}
The sets $V_i$ of a log-size ball sequence $(V_i, t_i, r_i)_{i \in \mathbb{N}}$ are a decreasing sequence of sets. That is, $V_{i+1} \subseteq V_i$ for all $i \in \mathbb{N}$.
\end{lemma}

\begin{proof}
\leanok
$V_{i+1} = V_i \setminus B_{V_i}(t_i, (r_i - 1)c)$ hence $V_{i+1} \subseteq V_i$.
\end{proof}


\begin{lemma}\label{lem:logSizeBallSequence_eq_zero}
  %  \uses{def:logSizeBallSequence}
  \leanok
  %  \lean{card_finset_logSizeBallSeq_card_eq_zero}
For any log-size ball sequence $(V_i, t_i, r_i)_{i \in \mathbb{N}}$ for $(J, a, c, n)$, for all $k \ge \vert J \vert$, $V_k = \emptyset$.
\end{lemma}

\begin{proof}
  \leanok
  %  \uses{lem:logSizeBallSequence_V_anti}
$V_{i+1} = V_i \setminus B_{V_i}(t_i, (r_i - 1)c)$ and since $t_i \in B_{V_i}(t_i, (r_i - 1)c)$, we have $\vert V_{i+1} \vert < \vert V_i \vert$ and the cardinal eventually reaches $0$, in at most $\vert J \vert$ steps.
\end{proof}


\begin{lemma}\label{lem:logSizeBallSequence_disjoint_B}
  %  \uses{def:logSizeBallSequence}
  \leanok
  %  \lean{disjoint_smallBall_logSizeBallSeq}
For $i \ne j$, the balls $B_{V_i}(t, (r_i-1)c)$ and $B_{V_j}(t_j, (r_j-1)c)$ of a log-size ball sequence $(V_i, t_i, r_i)_{i \in \mathbb{N}}$ are disjoint.
\end{lemma}

\begin{proof}
  \leanok
  %  \uses{lem:logSizeBallSequence_V_anti}
Assume w.l.o.g. that $i < j$.
Then $B_{V_j}(t_j, (r_j-1)c) \subseteq V_j \subseteq V_{i+1}$.
It suffices to show that $B_{V_i}(t_i, (r_i-1)c)$ and $V_{i+1}$ are disjoint.
This follows from the definition of $V_{i+1} = V_i \setminus B_{V_i}(t_i, (r_i-1)c)$.
\end{proof}


\begin{definition}\label{def:pairSet}
  %  \uses{def:logSizeBallSequence}
  \leanok
  %  \lean{pairSet}
Let $(V_i, t_i, r_i)_{i \in \mathbb{N}}$ be a log-size ball sequence for $(J, a, c, n)$.
For $i \in \mathbb{N}$, let $K_i = \{t_i\} \times B_{V_i}(t_i, r_i c)$ be the set of pairs $(t_i, s)$ for $s$ in the ball $B_{V_i}(t_i, r_i c)$.
We define $K = \bigcup_{i=0}^{\vert J \vert-1} K_i$, set of all pairs from the log-size ball sequence.
\end{definition}


\begin{lemma}\label{lem:card_pairSet_le}
  %  \uses{def:pairSet}
  \leanok
  %  \lean{card_pairSet_le}
The cardinal of the pair set $K$ of a log-size ball sequence for $(J, a, c, n)$ satisfies $|K| \le a |J|$.
\end{lemma}

\begin{proof}
  \leanok
  %  \uses{lem:card_logSizeRadius_ge, lem:card_logSizeRadius_le, lem:logSizeBallSequence_disjoint_B}
Using Lemma~\ref{lem:card_logSizeRadius_le}, the cardinal of $K$ is bounded by
\begin{align*}
  \vert K \vert
  &\le \sum_{i=0}^{m-1} \vert K_i \vert
  \le \sum_{i=0}^{m-1} a^{r_i}
  \: .
\end{align*}
Since the sets $B_{V_i}(t_i, (r_i-1)c)$ are disjoint by Lemma~\ref{lem:logSizeBallSequence_disjoint_B}, we can use Lemma~\ref{lem:card_logSizeRadius_ge} to get
\begin{align*}
  \sum_{i=0}^{m-1} a^{r_i - 1}
  \le \sum_{i=0}^{m-1} \vert B_{V_i}(t_i, (r_i-1)c) \vert
  = \left\vert \bigcup_{i=0}^{m-1} B_{V_i}(t_i, (r_i-1)c) \right\vert
  \le \vert J \vert
  \: .
\end{align*}
We obtained the inequality $\vert K \vert \le a \vert J \vert$
\end{proof}


\begin{lemma}\label{lem:dist_le_of_mem_pairSet}
  %  \uses{def:pairSet}
  \leanok
  %  \lean{edist_le_of_mem_pairSet}
Let $(s, t)$ be a pair in the pair set $K$ of a log-size ball sequence for $(J, a, c, n)$.
Then $d_T(s, t) \le c n$.
\end{lemma}

\begin{proof}
  \leanok
  %  \uses{lem:logSizeRadius_logSizeBallSequence_le}
A pair $(t, s) \in K$ is of the form $(t_i, s)$ for $s \in B_V(t_i, r_i c)$ and satisfies
\begin{align*}
  d_T(t_i, s) \le c r_i \le c n \: .
\end{align*}
The last inequality is from Lemma~\ref{lem:logSizeRadius_logSizeBallSequence_le}.
\end{proof}


\begin{lemma}\label{lem:sup_dist_le_two_mul_sup_dist_pairSet}
  %  \uses{def:pairSet}
  \leanok
  %  \lean{iSup_edist_pairSet}
Let $K$ be the pair set of a log-size ball sequence $(V_i, t_i, r_i)_{i \in \mathbb{N}}$ for $(J, a, c, n)$.
Then for any function $f : T \to E$ with $(E,d_E)$ a metric space,
\begin{align*}
  \sup_{s,t\in J, d_T(s,t) \le c} d_E(f(s), f(t))
  & \le 2 \sup_{(s,t) \in K} d_E(f(s), f(t))
  \: .
\end{align*}
\end{lemma}

\begin{proof}
\leanok
Let $(s, t) \in J^2$ such that $d_T(s, t) \le c$.
Then there exists a largest $\ell \in \mathbb{N}$ such that $s, t \in V_\ell$.
Assume w.l.o.g. that $s \notin V_{\ell + 1}$. Then $s \in B_{V_\ell}(t_\ell, (r_\ell-1)c)$ (since $V_{\ell + 1} = V_\ell \setminus B_{V_\ell}(t_\ell, (r_\ell-1)c)$), which implies $d_T(s, t_\ell) \le (r_\ell - 1)c$.

Since $d_T(s, t) \le c$, $d_T(t, t_\ell) \le d_T(t, s) + d_T(s, t_\ell) \le r_\ell c$, hence $t \in B_{V_\ell}(t_\ell, r_\ell c)$ and we have that both $s$ and $t$ are in $B_{V_\ell}(t_\ell, r_\ell c)$.
Thus both $(t_\ell, s)$ and $(t_\ell, t)$ are in $K_\ell \subseteq K$.
Finally
\begin{align*}
  d_E(f(s), f(t))
  &\le d_E(f(s), f(t_\ell)) + d_E(f(t_\ell), f(t))
  \\
  &\le 2\sup_{(s',t') \in K} d_E(f(s'), f(t'))
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:pair_reduction}
  %  \uses{def:pairSet}
  \leanok
  %  \lean{pair_reduction}
Let $(T,d_T)$ be a metric space.
Let $J \subseteq T$ be finite, $a > 1$, $c>0$ and $n \in \{1, 2, ...\}$ such that $|J| \le a^n$.
Then, there is $K \subseteq J^2$ such that for any function $f : T \to E$ with $(E,d_E)$ a metric space,
\begin{align}
  |K|
  & \le a |J|
  \:, \label{eq:chain1} \\
  \forall (s,t) \in K,
  &\:  d_T(s,t) \le c n
  \:, \label{eq:chain2} \\
  \sup_{s,t\in J, d_T(s,t) \le c} d_E(f(s), f(t))
  & \le 2 \sup_{(s,t) \in K} d_E(f(s), f(t))
  \: . \label{eq:chain3}
\end{align}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:card_pairSet_le, lem:dist_le_of_mem_pairSet, lem:sup_dist_le_two_mul_sup_dist_pairSet}
Let $(V_i, t_i, r_i)_{i \in \mathbb{N}}$ be a log-size ball sequence for $(J, a, c, n)$. We show that its pair set satisfies the conditions of the lemma.

Equation~\eqref{eq:chain1} is given by Lemma~\ref{lem:card_pairSet_le}.
The second property~\eqref{eq:chain2} is Lemma~\ref{lem:dist_le_of_mem_pairSet}.
Equation~\eqref{eq:chain3} was proved in Lemma~\ref{lem:sup_dist_le_two_mul_sup_dist_pairSet}.
\end{proof}





\subsection{Chaining for stochastic processes under Kolmogorov conditions}

\subsection{Kolmogorov condition}

\begin{definition}[Kolmogorov condition]\label{def:IsKolmogorovProcess}
  \mathlibok
  %  \lean{ProbabilityTheory.IsAEKolmogorovProcess}
Let $X : T \to \Omega \to E$ be a stochastic process, where $(T, d_T)$ and $(E, d_E)$ are pseudo-metric spaces and $(\Omega, \mathbb{P})$ is a measure space.
Let $p, q > 0$.
We say that $X$ satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$ if for all $s, t \in T$, $(X_s, X_t)$ is $\mathbb{P}$-a.e. measurable for the Borel $\sigma$-algebra on $E^2$ and
\begin{align*}
  \mathbb{E}[d_E(X_s, X_t)^p] \le M d_T(s, t)^q
  \: .
\end{align*}
\end{definition}

Remark: the measurability condition on the pair would be implied by the measurability of $X_t$ for all $t \in T$ if we assumed that $E$ is separable (\texttt{SecondCountableTopology} in Lean), which implies that the Borel $\sigma$-algebra on the product is equal to the product of the Borel $\sigma$-algebras.
We follow \cite{kratschmer2023kolmogorov} and do not require separability.


\begin{lemma}\label{lem:IsKolmogorovProcess.edist_eq_zero}
  %  \uses{def:IsKolmogorovProcess}
  \mathlibok
  %  \lean{ProbabilityTheory.IsAEKolmogorovProcess.edist_eq_zero}
If $X : T \to \Omega \to E$ is a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$ and $s, t \in T$ are such that $d_T(s, t) = 0$, then $\mathbb{P}$-a.e. $d_E(X_s, X_t) = 0$.
\end{lemma}

\begin{proof}\leanok
It suffices to show that $d_E(X_s, X_t)^p = 0$ almost everywhere, which is in turn implied by $\mathbb{E}[d_E(X_s, X_t)^p] \le M d_t(s, t)^q = 0$.
\end{proof}


\begin{lemma}\label{lem:IsKolmogorovProcess.lintegral_sup_rpow_edist_eq_zero}
  %  \uses{def:IsKolmogorovProcess}
  \leanok
  %  \lean{ProbabilityTheory.IsAEKolmogorovProcess.lintegral_sup_rpow_edist_eq_zero}
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$.
Let $T'$ be a countable subset of $T$ such that for all $s, t \in T'$, $d_T(s, t) = 0$.
Then
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in T'} d_E(X_s, X_t)^p \right]
  &= 0
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:IsKolmogorovProcess.edist_eq_zero}
Since $T'$ is countable, we get from Lemma~\ref{lem:IsKolmogorovProcess.edist_eq_zero} that almost surely, for all $s, t \in T'$, $d_E(X_s, X_t)^p = 0$.
In particular the expectation of the supremum is $0$.
\end{proof}


\paragraph{Measurability}

\begin{lemma}\label{lem:IsKolmogorovProcess.aemeasurable}
  %  \uses{def:IsKolmogorovProcess}
  \mathlibok
  %  \lean{ProbabilityTheory.IsAEKolmogorovProcess.aemeasurable}
If $X : T \to \Omega \to E$ is a function that satisfies the Kolmogorov condition, then for all $t \in T$, $X_t$ is $\mathbb{P}$-a.e. measurable.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:aemeasurable_pair_of_aemeasurable}
  \leanok
  %  \lean{ProbabilityTheory.aemeasurable_pair_of_aemeasurable}
If $E$ is separable and $X : T \to \Omega \to E$ is a process such that $X_t$ is $\mathbb{P}$-a.e. measurable for all $t \in T$, then for all $s, t \in T$, the pair $(X_s, X_t)$ is $\mathbb{P}$-a.e. measurable for the Borel $\sigma$-algebra on $E^2$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:IsKolmogorovProcess.aemeasurable_edist}
  %  \uses{def:IsKolmogorovProcess}
  \mathlibok
  %  \lean{ProbabilityTheory.IsAEKolmogorovProcess.aemeasurable_edist}
If $X : T \to \Omega \to E$ is a process that satisfies the Kolmogorov condition, then for all $s,t \in T$ the function $\omega \mapsto d_E(X_s(\omega), X_t(\omega))$ is $\mathbb{P}$-a.e. measurable.
\end{lemma}

\begin{proof}\leanok

\end{proof}

\paragraph{Distance bounds}

\begin{lemma}\label{lem:integral_sup_rpow_dist_le_card_mul_rpow}
  %  \uses{def:IsKolmogorovProcess}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_le_card_mul_rpow}
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$.
Let $\varepsilon > 0$ and $C \subseteq T^2$ be a finite set such that for all $(s, t) \in C$, $d_T(s, t) \le \varepsilon$.
Then
\begin{align*}
  \mathbb{E}\left[\sup_{(s,t) \in C} d_E(X_s, X_t)^p \right]
  &\le \vert C \vert M \varepsilon^q
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{def:IsKolmogorovProcess}
\begin{align*}
  \mathbb{E}\left[\sup_{(s,t) \in C} d_E(X_s, X_t)^p \right]
  &\le \mathbb{E}\left[\sum_{(s,t) \in C} d_E(X_s, X_t)^p \right]
  \\
  &\le M \sum_{(s,t) \in C} d_T(s, t)^q
  \\
  &\le \vert C \vert M \varepsilon^q
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:integral_sup_rpow_dist_of_dist_le}
  %  \uses{def:IsKolmogorovProcess}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_le_card_mul_rpow_of_dist_le}
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$.
Let $J \subseteq T$ be finite, $a, c \in \mathbb R_+$ with $a \ge 1$ and $n \in \{1, 2, ...\}$ such that $|J| \le a^n$.
Then
\begin{align*}
  \mathbb{E} \left[ \sup_{s, t \in J; d_T(s, t) \le c} d_E(X_s, X_t)^p \right]
  &\le 2^p a |J| M (cn)^q
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:pair_reduction, lem:integral_sup_rpow_dist_le_card_mul_rpow}
By Lemma~\ref{lem:pair_reduction}, there exists $K \subseteq J^2$ such that
\begin{align*}
  |K|
  & \le a |J|
  \:, \\
  \forall (s,t) \in K,
  & \ d_T(s,t) \le c n
  \:, \\
  \sup_{s,t\in J, d_T(s,t) \le c} d_E(X_s, X_t)
  & \le 2 \sup_{(s,t) \in K} d_E(X_s, X_t)
  \: .
\end{align*}
Hence for such a set $K$,
\begin{align*}
  \mathbb{E} \left[ \sup_{s, t \in J; d_T(s, t) \le c} d_E(X_s, X_t)^p \right]
  &\le 2^p \mathbb{E} \left[ \sup_{(s, t) \in K} d_E(X_s, X_t)^p \right]
  \: .
\end{align*}
Then by Lemma~\ref{lem:integral_sup_rpow_dist_le_card_mul_rpow},
\begin{align*}
  \mathbb{E} \left[ \sup_{(s, t) \in K} d_E(X_s, X_t)^p \right]
  &\le |K| M (cn)^q
  \le a |J| M (cn)^q
  \: .
\end{align*}
\end{proof}


\subsection{Bound for a set of points that are close together}

For a finite index set $T$, we want to obtain a bound on
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in T; d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right] \: .
\end{align*}
Note the condition that the supremum is taken over pairs $(s, t)$ such that $d_T(s, t) \le \delta$.

We consider covers of $T$ at different scales. $C_n$ is a finite $\varepsilon_n$-cover of $T$ with $\varepsilon_n = \varepsilon_0 2^{-n}$.
$T$ is equal to $C_k$ for some $k$ large enough, so the supremum over $T$ is a supremum at that scale $k$.
We will change scale to some $m \le k$ that depends on the distance bound $\delta$ ($m$ is of order $\log_2 \delta$) and consider the supremum over $C_m$ (plus a term due to the scale change).
Then for the supremum over a set in $C_m$, we use the reduction in the number of pairs of Lemma~\ref{lem:pair_reduction}.

\begin{lemma}\label{lem:scale_change}
  %  \uses{def:chainingSequence}
  \leanok
  %  \lean{scale_change}
Let $X : T \to E$.
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers, $C_n$ a finite $\varepsilon_n$-cover of $J \subseteq T$ with $C_n \subseteq J$.
For $m \le k$,
\begin{align*}
  \sup_{s, t \in C_k; d_T(s, t) \le \delta} d_E(X_s, X_t)
  &\le \sup_{s, t \in C_k; d_T(s, t) \le \delta} d_E(X_{\bar{s}_m}, X_{\bar{t}_m})
    + 2 \sup_{s \in C_k} d_E(X_s, X_{\bar{s}_m})
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
By the triangle inequality,
\begin{align*}
  d_E(X_s, X_t)
  &\le d_E(X_s, X_{\bar{s}_m}) + d(X_{\bar{s}_m}, X_{\bar{t}_m}) + d_E(X_{\bar{t}_m}, X_t)
  \: .
\end{align*}
\end{proof}


\begin{corollary}\label{cor:scale_change_rpow}
  %  \uses{def:chainingSequence}
  \leanok
  %  \lean{scale_change_rpow}
Let $X : T \to E$.
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers, $C_n$ a finite $\varepsilon_n$-cover of $J \subseteq T$ with $C_n \subseteq J$.
For $m \le k$,
\begin{align*}
  \sup_{s, t \in C_k; d_T(s, t) \le \delta} d_E(X_s, X_t)^p
  &\le 2^p \sup_{s, t \in C_k; d_T(s, t) \le \delta} d_E(X_{\bar{s}_m}, X_{\bar{t}_m})^p
    + 4^p \sup_{s \in C_k} d_E(X_s, X_{\bar{s}_m})^p
  \: .
\end{align*}
\end{corollary}

\begin{proof}\leanok
  %  \uses{lem:scale_change}
This is Lemma~\ref{lem:scale_change}, together with the fact that for $a, b \ge 0$,
\begin{align*}
  (a + b)^p \le (2\max(a,b))^p = 2^p \max(a^p,b^p) \le 2^p (a^p + b^p)
  \: .
\end{align*}
\end{proof}



\subsubsection{First term}


\begin{lemma}\label{lem:integral_sup_rpow_dist_cover_of_dist_le}
  %  \uses{def:IsKolmogorovProcess}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_cover_of_dist_le}
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$.
Let $C$ be a finite $\varepsilon$-cover of $J \subseteq T$ with $C \subseteq J$, with minimal cardinal.
Then for $c \ge 0$,
\begin{align*}
  \mathbb{E} \left[ \sup_{s, t \in C; d_T(s, t) \le c} d_E(X_s, X_t)^p \right]
  &\le 2^{p+1} M \left(2 c \log_2 N^{int}_{\varepsilon}(J) \right)^q  N^{int}_{\varepsilon}(J)
  \: .
\end{align*}
Note the logarithm has base $2$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:integral_sup_rpow_dist_of_dist_le}
Let $\bar{r} = 1 + \log_2 N^{int}_{\varepsilon}(J)$. Then
\begin{align*}
  \vert C \vert
  = N^{int}_{\varepsilon}(J)
  \le 2^{\bar{r}}
  \: .
\end{align*}
By Lemma~\ref{lem:integral_sup_rpow_dist_of_dist_le} with $J = C$, $a = 2$, $c = c$, $n = \bar{r}$,
\begin{align*}
  \mathbb{E} \left[ \sup_{s, t \in C; d_T(s, t) \le c} d_E(X_s, X_t)^p \right]
  &\le 2^{p+1} |C| M (c \bar{r})^q
  = 2^{p+1} M (c \bar{r})^q N^{int}_{\varepsilon}(J)
  \: .
\end{align*}

Suppose $N^{int}_{\varepsilon}(J) \ge 2$ (if it equals one the result is trivial).
Then $\bar{r} \le 2 \log_2 N^{int}_{\varepsilon}(J)$.
\begin{align*}
  \mathbb{E} \left[ \sup_{s, t \in C; d_T(s, t) \le c} d_E(X_s, X_t)^p \right]
  &\le 2^{p+1} M \left(2 c \log_2 N^{int}_{\varepsilon}(J) \right)^q  N^{int}_{\varepsilon}(J)
  \: .
\end{align*}
\end{proof}

\begin{lemma}\label{lem:integral_sup_rpow_dist_cover_rescale}
  %  \uses{def:IsKolmogorovProcess, def:chainingSequence}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_cover_rescale}
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$.
For all $n \in \mathbb{N}$, let $C_n$ a finite $\varepsilon_n$-cover of $J \subseteq T$ with $C_n \subseteq J$ for $\varepsilon_n = \varepsilon_0 2^{-n}$, with minimal cardinal.
Suppose $\varepsilon_0 < \infty$, let $\delta \in (0, 4 \varepsilon_0]$ and let $m$ be a natural number such that $\varepsilon_0 2^{-m} \le \delta$ and $\delta \le \varepsilon_0 2^{-m+2}$.
Then for $k \ge m$,
\begin{align*}
  \mathbb{E} \left[ \sup_{s, t \in C_k; d_T(s, t) \le \delta} d_E(X_{\bar{s}_m}, X_{\bar{t}_m})^p \right]
  &\le 2^{p+1} M \left(16 \delta \log_2 N^{int}_{\delta/4}(J) \right)^q  N^{int}_{\delta/4}(J)
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:integral_sup_rpow_dist_cover_of_dist_le, cor:dist_chainingSequence_pow_two_le}
By definition of $m$, $\delta \le \varepsilon_0 2^{-m+2}$.
For $s, t \in C_k$ with $d_T(s, t) \le \delta$, $d_T(\bar{s}_m, \bar{t}_m) \le \delta + \varepsilon_0 2^{-m+2} \le \varepsilon_0 2^{-m+3}$ (Corollary~\ref{cor:dist_chainingSequence_pow_two_le}).
It thus suffices to get a bound on $\mathbb{E} \left[ \sup_{s, t \in C_m; d_T(s, t) \le \varepsilon_0 2^{-m+3}} d_E(X_s, X_t)^p \right]$.

We can apply Lemma~\ref{lem:integral_sup_rpow_dist_cover_of_dist_le} with $\varepsilon = \varepsilon_m$, $c = \varepsilon_0 2^{-m+3}$. We obtain
\begin{align*}
  \mathbb{E} \left[ \sup_{s, t \in C_m; d_T(s, t) \le \varepsilon_0 2^{-m+3}} d_E(X_s, X_t)^p \right]
  &\le 2^{p+1} M \left(16 \varepsilon_0 2^{-m} \log_2 N^{int}_{\varepsilon_m}(J) \right)^q  N^{int}_{\varepsilon_m}(J)
  \: .
\end{align*}
By definition of $m$, $\varepsilon_m = \varepsilon_0 2^{-m} \ge \delta/4$,
hence $N^{int}_{\varepsilon_m}(J) \le N^{int}_{\delta / 4}(J)$.

Finally, by definition of $m$ we have $\varepsilon_0 2^{-m} \le \delta$.
\end{proof}



\subsubsection{Second term}


\begin{lemma}\label{lem:integral_sup_rpow_dist_succ}
  %  \uses{def:IsKolmogorovProcess}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_succ}
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$.
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers and $C_n$ a finite $\varepsilon_n$-cover of $T$ with $C_n \subseteq T$.
Then for $j < k$,
\begin{align*}
  \mathbb{E}\left[\sup_{t \in C_k} d_E(X_{\bar{t}_j}, X_{\bar{t}_{j+1}})^p \right]
  &\le \vert C_{j+1} \vert M \varepsilon_j^q
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:dist_chainingSequence_add_one, lem:integral_sup_rpow_dist_le_card_mul_rpow}
\begin{align*}
  \mathbb{E}\left[\sup_{t \in C_k} d_E(X_{\bar{t}_j}, X_{\bar{t}_{j+1}})^p \right]
  &\le \mathbb{E}\left[\sup_{u \in C_{j+1}} d_E(X_{\bar{u}_j}, X_{u})^p \right]
  \: .
\end{align*}
We then apply Lemma~\ref{lem:integral_sup_rpow_dist_le_card_mul_rpow} to the set $C = \{(\bar{u}_j, u) \mid u \in C_{j+1}\}$, which satisfies the condition $d_T(\bar{u}_j, u) \le \varepsilon_j$ and has cardinal $\vert C_{j+1} \vert$.
\end{proof}



\paragraph{Case $p \ge 1$}


\begin{lemma}\label{lem:integral_sup_dist_le_sum_rpow}
  %  \uses{def:chainingSequence}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_le_sum_rpow}
Let $X : T \to \Omega \to E$ be a stochastic process.
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers and $C_n$ a finite $\varepsilon_n$-cover of $T$ with $C_n \subseteq T$.
For $p \ge 1$ and $m \le k$,
\begin{align*}
  \mathbb{E}\left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le \left(\sum_{i=m}^{k-1} \left( \mathbb{E}\left[\sup_{t \in C_k} d_E(X_{\bar{t}_i}, X_{\bar{t}_{i+1}})^p\right] \right)^{1/p}\right)^p
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
By the triangle inequality,
\begin{align*}
  \sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p
  &\le \sup_{t \in C_k} \left( \sum_{i=m}^{k-1} d_E(X_{\bar{t}_i}, X_{\bar{t}_{i+1}}) \right)^p
  \\
  &\le \left( \sum_{i=m}^{k-1} \sup_{t \in C_k} d_E(X_{\bar{t}_i}, X_{\bar{t}_{i+1}}) \right)^p
  \: .
\end{align*}
We thus have
\begin{align*}
  \left(\mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]\right)^{1/p}
  &\le \left(\mathbb{E} \left[\left( \sum_{i=m}^{k-1} \sup_{t \in C_k} d_E(X_{\bar{t}_i}, X_{\bar{t}_{i+1}}) \right)^p\right]\right)^{1/p}
  \: .
\end{align*}
And then, by Minkowski's inequality, since $p \ge 1$,
\begin{align*}
  \left(\mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]\right)^{1/p}
  &\le \sum_{i=m}^{k-1} \left( \mathbb{E}\left[\sup_{t \in C_k} d_E(X_{\bar{t}_i}, X_{\bar{t}_{i+1}})^p \right] \right)^{1/p}
  \: .
\end{align*}
Finally, we raise to the $p$-th power to obtain the result.
\end{proof}


\begin{lemma}\label{lem:integral_sup_rpow_dist_le_sum}
  %  \uses{def:IsKolmogorovProcess}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_le_sum}
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$.
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers and $C_n$ a finite $\varepsilon_n$-cover of $T$ with $C_n \subseteq T$.
Then for $p \ge 1$ and $m \le k$,
\begin{align*}
  \mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le M \left( \sum_{j=m}^{k-1} \vert C_{j+1} \vert^{1/p} \varepsilon_j^{q/p} \right)^p
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:integral_sup_rpow_dist_succ, lem:integral_sup_dist_le_sum_rpow}
Put together Lemma~\ref{lem:integral_sup_rpow_dist_succ} and Lemma~\ref{lem:integral_sup_dist_le_sum_rpow}.
\end{proof}


\begin{lemma}\label{lem:integral_sup_rpow_dist_le_of_minimal_cover}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_le_of_minimal_cover}
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$.
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers in $(0, \mathrm{diam}(T))$ and $C_n$ a finite $\varepsilon_n$-cover of $T$ with $C_n \subseteq T$, and with minimal cardinality.
Suppose that $T$ has bounded internal covering number with constant $c_1>0$ and exponent $d > 0$.
Then for $p \ge 1$ and $m \le k$,
\begin{align*}
  \mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le M c_1 \left( \sum_{j=m}^{k-1} \varepsilon_{j+1}^{-d/p} \varepsilon_j^{q/p} \right)^p
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:integral_sup_rpow_dist_le_sum, def:HasBoundedInternalCoveringNumber}
By Lemma~\ref{lem:integral_sup_rpow_dist_le_sum}, we have
\begin{align*}
  \mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le M \left( \sum_{j=m}^{k-1} \vert C_{j+1} \vert^{1/p} \varepsilon_j^{q/p} \right)^p
  \: .
\end{align*}
Then by the minimality of the cardinality of $C_n$ and the bounded internal covering number hypothesis, we have
\begin{align*}
  \vert C_{j+1} \vert
  &\le N^{int}_{\varepsilon_{j+1}}(T)
  \le c_1 \varepsilon_{j+1}^{-d}
  \: .
\end{align*}
\end{proof}


\begin{corollary}\label{cor:integral_sup_rpow_dist_le_of_minimal_cover_two}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_le_of_minimal_cover_two}
Under the assumptions of Lemma~\ref{lem:integral_sup_rpow_dist_le_of_minimal_cover}, for $\varepsilon_n = \varepsilon_0 2^{-n}$, then for $m \le k$,
\begin{align*}
  \mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le 2^d M c_1 (\varepsilon_0 2^{-m + 1})^{q - d} \frac{1}{\left( 2^{(q -d)/p} - 1\right)^p}
  \: .
\end{align*}
\end{corollary}

\begin{proof}\leanok
  %  \uses{lem:integral_sup_rpow_dist_le_of_minimal_cover}
Applying first Lemma~\ref{lem:integral_sup_rpow_dist_le_of_minimal_cover}, we get
\begin{align*}
  \mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le 2^d M c_1 \varepsilon_0^{q - d} \left( \sum_{j=m}^{k-1} 2^{- j(q - d)/p} \right)^p
  \\
  &= 2^d M c_1 (\varepsilon_0 2^{-m})^{q - d} \left( \sum_{j=0}^{k-m-1} 2^{- j(q - d)/p} \right)^p
  \\
  &\le 2^d M c_1 (\varepsilon_0 2^{-m})^{q - d} \left( \sum_{j=0}^{\infty} 2^{- j(q - d)/p} \right)^p
  \\
  &= 2^d M c_1 (\varepsilon_0 2^{-m})^{q - d} \frac{1}{(1 - 2^{-(q-d)/p})^p}
  \\
  &= 2^d M c_1 (\varepsilon_0 2^{-m+1})^{q - d} \frac{1}{(2^{(q-d)/p} - 1)^p}
  \: .
\end{align*}
\end{proof}



\paragraph{Case $p \le 1$}


\begin{lemma}\label{lem:integral_sup_dist_le_sum_rpow_of_le_one}
  %  \uses{def:chainingSequence}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_le_sum_rpow_of_le_one}
Let $X : T \to \Omega \to E$ be a stochastic process.
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers and $C_n$ a finite $\varepsilon_n$-cover of $T$ with $C_n \subseteq T$.
For $0 < p \le 1$ and $m \le k$,
\begin{align*}
  \mathbb{E}\left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le \sum_{i=m}^{k-1} \mathbb{E}\left[\sup_{t \in C_k} d_E(X_{\bar{t}_i}, X_{\bar{t}_{i+1}})^p\right]
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
For $0 < p \le 1$, the power function is sub-additive, i.e. for $a, b \ge 0$,
\begin{align*}
  (a + b)^p \le a^p + b^p
  \: .
\end{align*}
We can thus apply the triangle inequality to obtain
\begin{align*}
  \sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p
  &\le \sup_{t \in C_k} \left(\sum_{i=m}^{k-1} d_E(X_{\bar{t}_i}, X_{\bar{t}_{i+1}})\right)^p
  \\
  &\le \sup_{t \in C_k} \sum_{i=m}^{k-1} d_E(X_{\bar{t}_i}, X_{\bar{t}_{i+1}})^p
  \\
  &\le \sum_{i=m}^{k-1} \sup_{t \in C_k} d_E(X_{\bar{t}_i}, X_{\bar{t}_{i+1}})^p
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:integral_sup_rpow_dist_le_sum_of_le_one}
  %  \uses{def:chainingSequence}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_le_sum_of_le_one}
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$.
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers and $C_n$ a finite $\varepsilon_n$-cover of $T$ with $C_n \subseteq T$.
For $0 < p \le 1$ and $m \le k$,
\begin{align*}
  \mathbb{E}\left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le M \sum_{i=m}^{k-1} \vert C_{j+1} \vert \varepsilon_j^{q}
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:integral_sup_rpow_dist_succ, lem:integral_sup_dist_le_sum_rpow_of_le_one}
Put together Lemma~\ref{lem:integral_sup_rpow_dist_succ} and Lemma~\ref{lem:integral_sup_dist_le_sum_rpow_of_le_one}.
\end{proof}


\begin{lemma}\label{lem:integral_sup_rpow_dist_le_of_minimal_cover_of_le_one}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_le_of_minimal_cover_of_le_one}
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$.
Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be a sequence of positive numbers in $(0, \mathrm{diam}(T)]$ and $C_n$ a finite $\varepsilon_n$-cover of $T$ with $C_n \subseteq T$, and with minimal cardinality.
Suppose that $T$ has bounded internal covering number with constant $c_1>0$ and exponent $d > 0$.
Then for $p \le 1$ and $m \le k$,
\begin{align*}
  \mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le M c_1 \sum_{j=m}^{k-1} \varepsilon_{j+1}^{-d} \varepsilon_j^{q}
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:integral_sup_rpow_dist_le_sum_of_le_one, def:HasBoundedInternalCoveringNumber}
By Lemma~\ref{lem:integral_sup_rpow_dist_le_sum_of_le_one}, we have
\begin{align*}
  \mathbb{E}\left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le M \sum_{i=m}^{k-1} \vert C_{j+1} \vert \varepsilon_j^{q}
  \: .
\end{align*}
Then by the minimality of the cardinality of $C_n$ and the bounded internal covering number hypothesis, we have
\begin{align*}
  \vert C_{j+1} \vert
  &= N^{int}_{\varepsilon_{j+1}}(T)
  \le c_1 \varepsilon_{j+1}^{-d}
  \: .
\end{align*}
\end{proof}


\begin{corollary}\label{cor:integral_sup_rpow_dist_le_of_minimal_cover_two_of_le_one}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_rpow_edist_le_of_minimal_cover_two_of_le_one}
Under the assumptions of Lemma~\ref{lem:integral_sup_rpow_dist_le_of_minimal_cover_of_le_one}, for $\varepsilon_n = \varepsilon_0 2^{-n}$, then for $m \le k$,
\begin{align*}
  \mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le 2^d M c_1 (\varepsilon_0 2^{-m + 1})^{q - d} \frac{1}{\left( 2^{(q -d)} - 1\right)}
  \: .
\end{align*}
\end{corollary}

\begin{proof}\leanok
  %  \uses{lem:integral_sup_rpow_dist_le_of_minimal_cover_of_le_one}
Applying first Lemma~\ref{lem:integral_sup_rpow_dist_le_of_minimal_cover_of_le_one}, we get
\begin{align*}
  \mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le 2^d M c_1 (\varepsilon_0 2^{-m})^{q-d}\sum_{j=0}^{k-m-1} 2^{- j (q - d)}
  \\
  &\le 2^d M c_1 (\varepsilon_0 2^{-m})^{q-d}\sum_{j=0}^{+\infty} 2^{- j (q - d)}
  \\
  &= 2^d M c_1 (\varepsilon_0 2^{-m})^{q-d} \frac{1}{1 - 2^{-(q - d)}}
  \\
  &= 2^d M c_1 (\varepsilon_0 2^{-m+1})^{q-d} \frac{1}{2^{(q - d)} - 1}
  \: .
\end{align*}
\end{proof}


\paragraph{Any $p>0$}


\begin{definition}\label{def:Cp}
  \leanok
  %  \lean{ProbabilityTheory.Cp}
\begin{align*}
  C_p = \max\left\{\frac{1}{\left( 2^{(q -d)/p} - 1\right)^p}, \frac{1}{\left( 2^{(q -d)} - 1\right)} \right\}
  \: .
\end{align*}
\end{definition}


\begin{lemma}\label{lem:second_term_bound}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber, def:Cp}
  \leanok
  %  \lean{ProbabilityTheory.second_term_bound}
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$.
Let $C_n$ a finite $(\varepsilon_0 2^{-n})$-cover of $T$ for $\varepsilon_0 \le \mathrm{diam}(T)$ with $C_n \subseteq T$, and with minimal cardinality.
Suppose that $T$ has bounded internal covering number with constant $c_1>0$ and exponent $d > 0$.
Then for $m \le k$,
\begin{align*}
  \mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le 2^d M c_1 (\varepsilon_0 2^{-m + 1})^{q - d} C_p
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{cor:integral_sup_rpow_dist_le_of_minimal_cover_two_of_le_one,cor:integral_sup_rpow_dist_le_of_minimal_cover_two}
This is the max of the two bounds obtained $p \ge 1$ and $p \le 1$.
\end{proof}



\subsubsection{Putting it all together}


\begin{lemma}\label{lem:lintegral_sup_cover_eq_of_lt_iInf_dist}
  %  \uses{def:IsKolmogorovProcess, def:IsCover}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_sup_cover_eq_of_lt_iInf_dist}
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$ and let $J$ be a finite subset of $T$.
Let $C$ be an $\varepsilon$-cover of $J$ with $C \subseteq J$.
If $\varepsilon < \inf_{s, t \in J; d_T(s, t)>0} d_T(s, t)$ then
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in C; d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right]
  &= \mathbb{E}\left[ \sup_{s, t \in J; d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right]
\end{align*}
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:IsKolmogorovProcess.edist_eq_zero}
First, remark that $C$ is actually a $0$-cover of $J$.
For $s, t \in J$, let $s', t' \in C$ be such that $d_T(s, s') = 0$ and $d_T(t, t') = 0$.
Then by the triangle inequality,
\begin{align*}
  d_E(X_s, X_t)
  &\le d_E(X_s, X_{s'}) + d_E(X_{s'}, X_{t'}) + d_E(X_t, X_{t'})
\end{align*}
and by Lemma~\ref{lem:IsKolmogorovProcess.edist_eq_zero}, we have $d_E(X_s, X_{s'}) = 0$ and $d_E(X_t, X_{t'}) = 0$ almost surely, hence $d_E(X_s, X_t) \le d_E(X_{s'}, X_{t'})$.
Since $J$ is finite, almost surely we have that inequality for all pairs $(s, t) \in J$ and their corresponding $(s', t') \in C$.
Note that $d_T(s', t') = d_T(s, t)$, hence $d_T(s, t) \le \delta$ is equivalent to $d_T(s', t') \le \delta$.
We obtain
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in J; d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right]
  &\le \mathbb{E}\left[ \sup_{s, t \in J; d_T(s, t) \le \delta} d_E(X_{s'}, X_{t'})^p \right]
  \\
  &= \mathbb{E}\left[ \sup_{s, t \in J; d_T(s', t') \le \delta} d_E(X_{s'}, X_{t'})^p \right]
  \\
  &\le \mathbb{E}\left[ \sup_{s, t \in C; d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right]
  \: .
\end{align*}
The reverse inequality holds because $C$ is a subset of $J$.
\end{proof}


\begin{theorem}\label{thm:finite_set_bound_of_dist_le_of_diam_le}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber, def:Cp}
  \leanok
  %  \lean{ProbabilityTheory.finite_set_bound_of_edist_le_of_diam_le}
Suppose that $T$ is a finite set with bounded internal covering number with constant $c_1>0$ and exponent $d > 0$.
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$, with $q > d$ and $p > 0$.
For all $\delta \ge 4\mathrm{diam}(T)$,
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in T; d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right]
  \le 4^p 2^q M c_1 \delta^{q - d} C_p
  \: .
\end{align*}
\end{theorem}

\begin{proof}\leanok
  %  \uses{lem:second_term_bound, cor:scale_change_rpow, lem:lintegral_sup_cover_eq_of_lt_iInf_dist}
Let $\varepsilon_0 = \mathrm{diam}(T)$.
For all $n \in \mathbb{N}$, let $C_n$ a finite $\varepsilon_n$-cover of $T$ with $C_n \subseteq T$ for $\varepsilon_n = \varepsilon_0 2^{-n}$, with minimal cardinal.

Let $k$ be a natural number such that $\varepsilon_0 2^{-k} < \inf_{s, t \in T; d_T(s,t)>0} d_T(s, t)$, which exists since $T$ is finite.
By Lemma~\ref{lem:lintegral_sup_cover_eq_of_lt_iInf_dist}, the supremum over $T$ can be replaced by a supremum over $C_k$.

By Corollary~\ref{cor:scale_change_rpow},
\begin{align*}
  &\mathbb{E}\left[ \sup_{s, t \in C_k; d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right]
  \\
  &\le 2^p \mathbb{E}\left[ \sup_{s, t \in C_k; d_T(s, t) \le \delta} d_E(X_{\bar{s}_0}, X_{\bar{t}_0})^p \right]
    + 4^p \mathbb{E}\left[ \sup_{s \in C_k} d_E(X_s, X_{\bar{s}_0})^p \right]
  \: .
\end{align*}

Since $\varepsilon_0 = \mathrm{diam}(T)$, $C_0$ is a singleton and $d_E(X_{\bar{s}_0}, X_{\bar{t}_0}) = 0$ for all $s, t$.
We thus have
\begin{align*}
  \mathbb{E} \left[ \sup_{s, t \in C_k; d_T(s, t) \le \delta} d_E(X_{\bar{s}_0}, X_{\bar{t}_0})^p \right]
  &= 0
  \: .
\end{align*}

By Lemma~\ref{lem:second_term_bound},
\begin{align*}
  \mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_0})^p \right]
  &\le 2^q M c_1 \varepsilon_0^{q - d} C_p
  \le 2^q M c_1 \delta^{q - d} C_p
  \: .
\end{align*}
\end{proof}


\begin{theorem}\label{thm:finite_set_bound_of_dist_le_of_le_diam}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber, def:Cp}
  \leanok
  %  \lean{ProbabilityTheory.finite_set_bound_of_edist_le_of_le_diam}
Suppose that $T$ is a finite set with bounded internal covering number with constant $c_1>0$ and exponent $d > 0$.
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$, with $q > d$ and $p > 0$.
For all $\delta \in (0, 4\mathrm{diam}(T)]$,
\begin{align*}
  &\mathbb{E}\left[ \sup_{s, t \in T; d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right]
  \\
  &\le 2^{2p+4q+1} M \delta^{q-d} \left(\delta^d \left(\log_2 N^{int}_{\delta/4}(T) \right)^q  N^{int}_{\delta/4}(T)
    + c_1 C_p\right)
  \: .
\end{align*}
\end{theorem}

\begin{proof}\leanok
  %  \uses{lem:second_term_bound, lem:integral_sup_rpow_dist_cover_rescale, cor:scale_change_rpow, lem:lintegral_sup_cover_eq_of_lt_iInf_dist, lem:IsKolmogorovProcess.lintegral_sup_rpow_edist_eq_zero}
Let $\varepsilon_0 = \mathrm{diam}(T)$.
For all $n \in \mathbb{N}$, let $C_n$ a finite $\varepsilon_n$-cover of $T$ with $C_n \subseteq T$ for $\varepsilon_n = \varepsilon_0 2^{-n}$, with minimal cardinal.

Let $k$ be a natural number such that $\varepsilon_0 2^{-k} < \inf_{s, t \in T; d_T(s,t)>0} d_T(s, t)$, which exists since $T$ is finite.
If $\delta \le \varepsilon_0 2^{-k}$, then $\{(s, t) \in C_k; d_T(s, t) \le \delta\} = \{(s, t) \mid s,t \in C_k, d_T(s,t) = 0\}$ and the inequality holds trivially (by Lemma~\ref{lem:IsKolmogorovProcess.lintegral_sup_rpow_edist_eq_zero}).
We can thus assume $\delta > \varepsilon_0 2^{-k}$.

By Lemma~\ref{lem:lintegral_sup_cover_eq_of_lt_iInf_dist}, the supremum over $T$ can be replaced by a supremum over $C_k$.

By Corollary~\ref{cor:scale_change_rpow}, for any $m \le k$,
\begin{align*}
  &\mathbb{E}\left[ \sup_{s, t \in C_k; d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right]
  \\
  &\le 2^p \mathbb{E}\left[ \sup_{s, t \in C_k; d_T(s, t) \le \delta} d_E(X_{\bar{s}_m}, X_{\bar{t}_m})^p \right]
    + 4^p \mathbb{E}\left[ \sup_{s \in C_k} d_E(X_s, X_{\bar{s}_m})^p \right]
  \: .
\end{align*}

\emph{First term}

We have $\delta \le 4\varepsilon_0$ by assumption.
Let $n_2 = \lfloor \log_2(4\varepsilon_0/\delta) \rfloor$ and $m = \min\{n_2, k\}$.
If $m = n_2$ then $\varepsilon_0 2^{-m} = \varepsilon_0 2^{-n_2} < \delta/2$.
Otherwise, $m = k$ and $\varepsilon_0 2^{-m} = \varepsilon_0 2^{-k} < \delta$ as argued at the start of the proof.
We thus get $\varepsilon_0 2^{-m} \le \delta$.
We can also verify that $\delta \le \varepsilon_0 2^{-n_2+2} \le \varepsilon_0 2^{-m+2}$.
By Lemma~\ref{lem:integral_sup_rpow_dist_cover_rescale},
\begin{align*}
  \mathbb{E} \left[ \sup_{s, t \in C_k; d_T(s, t) \le \delta} d_E(X_{\bar{s}_m}, X_{\bar{t}_m})^p \right]
  &\le 2^{p+1} M \left(16 \delta \log_2 N^{int}_{\delta/4}(T) \right)^q  N^{int}_{\delta/4}(T)
  \: .
\end{align*}

\emph{Second term}

By Lemma~\ref{lem:second_term_bound} and then the inequality $\varepsilon_0 2^{-m} \le \delta$,
\begin{align*}
  \mathbb{E} \left[\sup_{t \in C_k} d_E(X_t, X_{\bar{t}_m})^p \right]
  &\le 2^d M c_1 (\varepsilon_0 2^{-m+1})^{q - d} C_p
  \\
  &\le 2^q M c_1 \delta^{q - d} C_p
  \: .
\end{align*}

Putting the two terms together, we obtain
\begin{align*}
  &\mathbb{E}\left[ \sup_{s, t \in C_k; d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right]
  \\
  &\le 4^p M \left(4\left(16 \delta \log_2 N^{int}_{\delta/4}(T) \right)^q  N^{int}_{\delta/4}(T)
    + 2^q c_1 \delta^{q - d} C_p\right)
  \\
  &\le 2^{2p+4q+1} M \delta^{q-d} \left(\delta^d \left(\log_2 N^{int}_{\delta/4}(T) \right)^q  N^{int}_{\delta/4}(T)
    + c_1 C_p\right)
  \: .
\end{align*}
\end{proof}


\begin{corollary}\label{cor:finite_set_bound_of_dist_le_of_le_diam_bis}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber}
  \leanok
  %  \lean{ProbabilityTheory.finite_set_bound_of_edist_le_of_le_diam'}
With the same assumptions and notations as in Theorem~\ref{thm:finite_set_bound_of_dist_le_of_le_diam}, for all $\delta \in (0, 4\mathrm{diam}(T)]$,
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in T; d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right]
  &\le 2^{2p+4q+1} M c_1 \delta^{q-d} \left(4^d \left(\log_2 \left(c_1 \delta^{-d} 4^d \right) \right)^q
    + C_p\right)
  \: .
\end{align*}
\end{corollary}

\begin{proof}\leanok
  %  \uses{thm:finite_set_bound_of_dist_le_of_le_diam}
We apply Theorem~\ref{thm:finite_set_bound_of_dist_le_of_le_diam} and then remark that for $\delta \le 4\mathrm{diam}(T)$, we can use the bounded internal covering number hypothesis to bound $N^{int}_{\delta/4}(T)$~:
\begin{align*}
  N^{int}_{\delta/4}(T) \le c_1 \left(\frac{\delta}{4}\right)^{-d} \: .
\end{align*}
\end{proof}


\begin{corollary}\label{cor:finite_set_bound_of_dist_le}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber}
  \leanok
  %  \lean{ProbabilityTheory.finite_set_bound_of_edist_le}
Suppose that $T$ is a finite set with bounded internal covering number with constant $c_1>0$ and exponent $d > 0$.
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$, with $q > d$ and $p > 0$.
For all $\delta > 0$,
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in T; d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right]
  &\le 2^{2p+4q+1} M c_1 \delta^{q-d} \left(4^d \left(\max\left\{0, \log_2 \left(c_1 \delta^{-d} 4^d\right) \right\} \right)^q
    + C_p\right)
  \: .
\end{align*}
\end{corollary}


\begin{proof}\leanok
  %  \uses{cor:finite_set_bound_of_dist_le_of_le_diam_bis, thm:finite_set_bound_of_dist_le_of_diam_le}
We combine Corollary~\ref{cor:finite_set_bound_of_dist_le_of_le_diam_bis} and Theorem~\ref{thm:finite_set_bound_of_dist_le_of_diam_le}.
\end{proof}




\subsection{Kolmogorov-Chentsov Theorem}


\subsection{Sets with bounded internal covering number}

TODO: change the proofs here to avoid $s \ne t$ and use instead properties of processes satisfying the Kolmogorov condition for exponents $(p,q)$.

\begin{lemma}\label{lem:integral_div_dist_le_sum_integral_dist_le}
  \leanok
  %  \lean{ProbabilityTheory.lintegral_div_edist_le_sum_integral_edist_le}
Let $J \subseteq T$ be a finite set and suppose that $T$ has finite diameter.
For $k \in \mathbb{N}$, let $\eta_k = 2^{-k}(\mathrm{diam}(T) + 1)$.
For $X : T \to \Omega \to E$ a stochastic process and $\beta \in(0, (q - d)/p)$,
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in J;\: s \ne t} \frac{d_E(X_s, X_t)^p}{d_T(s, t)^{\beta p}} \right]
  &\le \sum_{k=0}^\infty 2^{k \beta p} \mathbb{E}\left[ \sup_{s, t \in J;\: s \ne t, \: d_T(s, t) \le 2 \eta_k} d_E(X_s, X_t)^p \right]
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
We introduce for each $k \in \mathbb{N}$ the set of pairs $(s, t)$ such that $\eta_k < d_T(s, t) \le 2 \eta_k$.
Note that $\eta_k \ge 2^{-k}$.
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in J;\: s \ne t} \frac{d_E(X_s, X_t)^p}{d_T(s, t)^{\beta p}} \right]
  &\le \sum_{k=0}^\infty \mathbb{E}\left[ \sup_{s, t \in J;\: s \ne t, \: \eta_k < d_T(s, t) \le 2 \eta_k} \frac{d_E(X_s, X_t)^p}{d_T(s, t)^{\beta p}} \right]
  \\
  &\le \sum_{k=0}^\infty \eta_k^{-\beta p} \mathbb{E}\left[ \sup_{s, t \in J;\: s \ne t, \: d_T(s, t) \le 2 \eta_k} d_E(X_s, X_t)^p \right]
  \\
  &\le \sum_{k=0}^\infty 2^{k \beta p} \mathbb{E}\left[ \sup_{s, t \in J;\: s \ne t, \: d_T(s, t) \le 2 \eta_k} d_E(X_s, X_t)^p \right]
  \: .
\end{align*}
\end{proof}


\begin{definition}\label{def:L}
  %  \uses{def:Cp}
  \leanok
  %  \lean{ProbabilityTheory.constL}
We introduce the constant
\begin{align*}
  L(T, c_1, d, p, q, \beta)
  &= 2^{2p+5q+1} c_1 (\mathrm{diam}(T)+1)^{q-d}
  \\&\quad \times \sum_{k=0}^\infty 2^{k (\beta p - (q-d))}\left(4^d \left(\max\left\{0, \log_2(c_1) + (k + 2)d \right\}\right)^q
    + C_p\right)
  \: .
\end{align*}
\end{definition}


\begin{lemma}\label{lem:L_lt_top}
  %  \uses{def:L}
  \leanok
  %  \lean{ProbabilityTheory.constL_lt_top}
For $\mathrm{diam}(T) < \infty$, $p> 0$, $q > d > 0$ and $\beta \in (0, (q-d)/p)$, the constant $L(T, c_1, d, p, q, \beta)$ is finite.
\end{lemma}

\begin{proof}
\leanok
Let $a_k = 2^{2p+5q+1} M c_1 (\mathrm{diam}(T)+1)^{q-d} 2^{k (\beta p - (q-d))} \left(4^d \left(\max\left\{0, \log_2(c_1) + (k + 2)d \right\}\right)^q
    + C_p\right)$.
Then $L(T, c_1, d, p, q, \beta) = \sum_{k=0}^\infty a_k$.
To show that the sum is finite, we can use the ratio test.
\begin{align*}
  \frac{\vert a_{k+1} \vert}{\vert a_k \vert}
  &= 2^{\beta p - (q - d)}
    \frac{\left(4^d \left(\max\left\{0, \log_2(c_1) + (k + 3)d \right\}\right)^q + C_p\right)}
    {\left(4^d \left(\max\left\{0, \log_2(c_1) + (k + 2)d \right\}\right)^q + C_p\right)}
\end{align*}
The limit at infinity of that ratio is $2^{\beta p - (q - d)} < 1$, hence the series $\sum_{k=0}^\infty a_k$ converges.
\end{proof}


\begin{lemma}\label{lem:finite_set_bound}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber, def:L}
  \leanok
  %  \lean{ProbabilityTheory.finite_kolmogorov_chentsov}
Suppose that $J \subseteq T$ is a finite set and that $T$ has bounded internal covering number with constant $c_1>0$ and exponent $d > 0$.
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$, with $q > d$ and $p > 0$.
Let $\beta \in(0, (q - d)/p)$.
Then
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in J;\: s \ne t} \frac{d_E(X_s, X_t)^p}{d_T(s, t)^{\beta p}} \right]
  \le M L(T, c_1, d, p, q, \beta)
  \: .
\end{align*}
\end{lemma}

\begin{proof}
  \leanok
  %  \uses{cor:finite_set_bound_of_dist_le, lem:integral_div_dist_le_sum_integral_dist_le, lem:hasBoundedInternalCoveringNumber_subset}

Since $J \subseteq T$, $J$ has bounded internal covering number with constant $2^d c_1$ and exponent $d$ (Lemma~\ref{lem:hasBoundedInternalCoveringNumber_subset}).

Let $\eta_k = 2^{-k}(\mathrm{diam}(T) + 1)$ for $k \in \mathbb{N}$.
By Lemma~\ref{lem:integral_div_dist_le_sum_integral_dist_le}, we have
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in J;\: s \ne t} \frac{d_E(X_s, X_t)^p}{d_T(s, t)^{\beta p}} \right]
  &\le \sum_{k=0}^\infty 2^{k \beta p} \mathbb{E}\left[ \sup_{s, t \in J;\: s \ne t, \: d_T(s, t) \le 2 \eta_k} d_E(X_s, X_t)^p \right]
  \: .
\end{align*}
We apply Corollary~\ref{cor:finite_set_bound_of_dist_le} to bound each expectation in the sum.
\begin{align*}
  &\mathbb{E}\left[ \sup_{s, t \in J;\: s \ne t, \: d_T(s, t) \le 2 \eta_k} d_E(X_s, X_t)^p \right]
  \\
  &\le 2^{2p+4q+1} M 2^d c_1 (2 \eta_k)^{q-d} \left(4^d \left(\max\left\{0, \log_2 \left(2^d c_1 (2 \eta_k)^{-d} 4^d \right) \right\} \right)^q
    + C_p\right)
  \\
  &\le 2^{2p+5q+1} M c_1 (\mathrm{diam}(T)+1)^{q-d} 2^{-k(q-d)} \left(4^d \left(\max\left\{0, \log_2 \left(c_1 2^{(k + 2)d} \right) \right\} \right)^q
    + C_p\right)
  \\
  &= 2^{2p+5q+1} M c_1 (\mathrm{diam}(T)+1)^{q-d} 2^{-k(q-d)} \left(4^d \left(\max\left\{0, \log_2(c_1) + (k + 2)d \right\} \right)^q
    + C_p\right)
  \: .
\end{align*}
The sum is then less than $M$ times $L(T, c_1, d, p, q, \beta)$.
\end{proof}


\begin{theorem}\label{thm:countable_set_bound}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber}
  \leanok
  %  \lean{ProbabilityTheory.countable_kolmogorov_chentsov}
Suppose that $T$ has bounded internal covering number with constant $c_1>0$ and exponent $d > 0$.
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition for exponents $(p,q)$ with constant $M$, with $q > d$ and $p > 0$.
Let $\beta \in(0, (q - d)/p)$.
Then for every countable subset $T' \subseteq T$ with positive diameter,
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in T';\: s \ne t} \frac{d_E(X_s, X_t)^p}{d_T(s, t)^{\beta p}} \right]
  \le M L(T, c_1, d, p, q, \beta)
  \: .
\end{align*}
\end{theorem}

\begin{proof}\leanok
  %  \uses{lem:finite_set_bound}
Build a monotone sequence of finite sets $T_n \subseteq T'$, use Lemma~\ref{lem:finite_set_bound} to obtain a bound for each $T_n$ that does not depend on $T_n$, and then use monotone convergence.
\end{proof}


\begin{corollary}\label{cor:countable_set_bound_of_le}
Under the same assumptions as in Theorem~\ref{thm:countable_set_bound}, for every countable subset $T' \subseteq T$ with positive diameter, for $L(T, c_1, d, p, q, \beta) < \infty$ the same constant,
\begin{align*}
  \mathbb{E}\left[ \sup_{s, t \in T';\: d_T(s, t) \le \delta} d_E(X_s, X_t)^p \right]
  \le M L(T, c_1, d, p, q, \beta) \delta^{\beta p}
  \: .
\end{align*}
\end{corollary}

\begin{proof}
  %  \uses{thm:countable_set_bound}
Immediately follows from Theorem~\ref{thm:countable_set_bound}.
\end{proof}


\begin{lemma}\label{lem:holder_modification_single}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber}
  \leanok
  %  \lean{ProbabilityTheory.exists_modification_holder_aux}
Under the assumptions of Theorem~\ref{thm:countable_set_bound}, for $E$ a complete space and $\beta \in (0, (q - d)/p)$, there exists a modification $Y$ of $X$ (i.e., a process $Y$ with $\mathbb{P}(Y_t \ne X_t) = 0$ for all $t$) such that the paths of $Y$ are Hölder continuous of order $\beta$.
\end{lemma}

\begin{proof}\leanok
  %  \uses{thm:countable_set_bound, lem:L_lt_top}
Let $T'$ be a countable dense subset of $T$.
Let $A$ be the event
\begin{align*}
  \left\{\sup_{s, t \in T';\: s \ne t} \frac{d_E(X_s, X_t)^p}{d_T(s, t)^{\beta p}} < \infty \right\}
  \: .
\end{align*}
As a consequence of Theorem~\ref{thm:countable_set_bound}, we have $\mathbb{P}(A) = 1$.

On the event $A$, $(X_t)_{t \in T'}$ has Hölder continuous paths of order $\beta$.
Let $x_0 \in E$ be arbitrary and let $Y: T \to \Omega \to E$ be the process defined by
\begin{align*}
  Y_t(\omega)
  &= \begin{cases}
    \lim_{s \to t, s \in T'} X_s(\omega) & \text{if } \omega \in A \\
    x_0 & \text{otherwise}
  \end{cases}
  \: .
\end{align*}
Then $Y$ has Hölder continuous paths of order $\beta$ almost surely.

We can show that $(Y_s, Y_t)$ is $\mathbb{P}$-a.e. measurable for all $s, t \in T$.

It remains to show that $Y$ is a modification of $X$.
Let then $t \in T$ and let $(t_n)_{n \in \mathbb{N}}$ be a sequence in $T'$ that converges to $t$.
We want to show that $\mathbb{P}(Y_t \ne X_t) = 0$.
It suffices to show that $\mathbb{P}(d_E(Y_t, X_t) > 0) = 0$, which itself would follow from $\mathbb{P}(d_E(Y_t, X_t) > \varepsilon) = 0$ for all $\varepsilon > 0$.

\begin{align*}
  \mathbb{P}(d_E(Y_t, X_t) > \varepsilon)
  &\le \mathbb{P}(d_E(Y_t, X_{t_n}) + d_E(X_{t_n}, X_t) > \varepsilon)
  \\
  &\le \mathbb{P}(d_E(Y_t, X_{t_n}) > \varepsilon/2) + \mathbb{P}(d_E(X_{t_n}, X_t) > \varepsilon/2)
  \: .
\end{align*}

TODO
\end{proof}


\begin{theorem}\label{thm:holder_modification}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedInternalCoveringNumber}
  \leanok
  %  \lean{ProbabilityTheory.exists_modification_holder}
Under the assumptions of Theorem~\ref{thm:countable_set_bound}, for $E$ a complete space, there exists a modification $Y$ of $X$ (i.e., a process $Y$ with $\mathbb{P}(Y_t \ne X_t) = 0$ for all $t$) such that the paths of $Y$ are Hölder continuous of all orders $\gamma \in (0, (q - d)/p)$.
\end{theorem}

\begin{proof}\leanok
  %  \uses{lem:holder_modification_single, lem:indistinguishable_of_modification_of_continuous}
Let $(\beta_n)$ be an increasing sequence of numbers in $(0, (q - d)/p)$ such that $\beta_n \to (q - d)/p$.
For each $n$, let $Y^n$ be the modification of $X$ given by Lemma~\ref{lem:holder_modification_single} for $\beta = \beta_n$.
Then by Lemma~\ref{lem:indistinguishable_of_modification_of_continuous}, the processes $Y^0$ and $Y^n$ are indistinguishable for all $n$.
That is, there exists an event $A_n$ such that $\mathbb{P}(A_n) = 1$ and such that for all $\omega \in A_n$, $Y^0_t(\omega) = Y^n_t(\omega)$ for all $t \in T$.

Let $A = \bigcap_{n \in \mathbb{N}} A_n$ and let $x_0 \in E$ be arbitrary.
Then $\mathbb{P}(A) = 1$ and the process $Y(\omega) = Y^0(\omega)$ for $\omega \in A$ and $Y(\omega) = x_0$ for $\omega \notin A$ has paths that are Hölder continuous of all orders $\gamma \in (0, (q - d)/p)$.
\end{proof}



\subsection{Localized Kolmogorov-Chentsov theorem}

\begin{definition}[Cover with bounded covering numbers]\label{def:HasBoundedCoveringNumberCover}
  %  \uses{def:HasBoundedInternalCoveringNumber}
  \leanok
  %  \lean{IsCoverWithBoundedCoveringNumber}
A set $T$ is said to have a cover with bounded covering numbers if there exists a monotone sequence of totally bounded subsets $(T_n)_{n \in \mathbb{N}}$ of $T$ such that for all $n$, $T_n$ has bounded internal covering number with constant $c_n$ and exponent $d_n > 0$, and such that $T \subseteq \bigcup_{n \in \mathbb{N}} T_n$.
\end{definition}


\begin{lemma}\label{lem:hasBoundedCoveringNumberCover_nnreal}
  %  \uses{def:HasBoundedCoveringNumberCover}
  \leanok
  %  \lean{isCoverWithBoundedCoveringNumber_Ico_nnreal}
$\mathbb{R}_+$ has a cover with bounded covering numbers for the sets $T_n = [0,n)$, constants $c_n = n$ and exponents $d_n = 1$.
\end{lemma}

Note: in the Lean code, we proved this result for weaker constants: $c_n = 3n$.

\begin{proof}\leanok
  %  \uses{lem:hasBoundedInternalCoveringNumber_unitInterval}

\end{proof}

We say that a function is \emph{locally} Hölder continuous of order $\gamma$ if for any point $x$ in its domain there is a neighborhood of $x$ on which the function is Hölder continuous of order $\gamma$.

\begin{theorem}\label{thm:localized_holder_modification}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedCoveringNumberCover}
  \leanok
  %  \lean{ProbabilityTheory.exists_modification_holder'}
Let $T$ be a metric space with a cover $(T_n)$ with bounded covering numbers with constants $c_n$ and the same exponent $d$.
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition with exponents $(p, q)$ with $q > d$.
Then $X$ has a modification $Y$ such that almost surely the paths of $Y$ are locally Hölder continuous of all orders $\gamma \in (0, (q - d)/p)$.
\end{theorem}

\begin{proof}\leanok
  %  \uses{thm:holder_modification, lem:indistinguishable_of_modification_of_continuous}
For each $n$, by Theorem~\ref{thm:holder_modification} there is a modification $Y_n$ of $X$ seen as a process on $T_n$ such that the paths of $Y_n$ are Hölder continuous of all orders $\gamma \in (0, (q - d)/p)$.
By Lemma~\ref{lem:indistinguishable_of_modification_of_continuous}, $Y_n$ and $Y_{n+1}$ are indistinguishable on $T_n$.
That is, almost surely $Y_n = Y_{n+1}$ on $T_n$.
Since there are countably many such almost sure equalities, we get that almost surely there is equality for all $n$.
Let $A$ be the event that this happens, let $x_0 \in E$ be arbitrary and define a process $Y : T \to \Omega \to E$ by
\begin{align*}
  Y(t, \omega)
  &= \begin{cases}
    Y_n(t, \omega) & \text{if } \omega \in A \: , \: t \in T_n \setminus T_{n-1} \: ,
    \\
    x_0 & \text{if } \omega \notin A \: .
  \end{cases}
\end{align*}
Then $Y$ is a modification of $X$ and has paths that are locally Hölder continuous of all orders $\gamma \in (0, (q - d)/p)$ almost surely.
\end{proof}


\begin{theorem}\label{thm:localized_holder_modification_sup}
  %  \uses{def:IsKolmogorovProcess, def:HasBoundedCoveringNumberCover}
  \leanok
  %  \lean{ProbabilityTheory.exists_modification_holder_iSup}
Let $T$ be a metric space with a cover $(T_n)$ with bounded covering numbers with constants $c_n$ and the same exponent $d$.
Let $(p_n, q_n)_{n \in \mathbb{N}}$ be a sequence of pairs of positive numbers such that $q_n > d$ for all $n \in \mathbb{N}$.
Let $X : T \to \Omega \to E$ be a process that satisfies the Kolmogorov condition with exponents $(p_n, q_n)$ for all $n \in \mathbb{N}$.
Then $X$ has a modification $Y$ such that almost surely the paths of $Y$ are locally Hölder continuous of all orders $\gamma \in (0, \sup_n (q_n - d)/p_n)$.
\end{theorem}

\begin{proof}\leanok
  %  \uses{thm:localized_holder_modification}
For each $n$, by Theorem~\ref{thm:localized_holder_modification} there is a modification $Y_n$ of $X$ such that the paths of $Y_n$ are locally Hölder continuous of all orders $\gamma \in (0, (q_n - d)/p_n)$.
By Lemma~\ref{lem:indistinguishable_of_modification_of_continuous}, any two processes $Y_n, Y_m$ are indistinguishable.
That is, almost surely $Y_n = Y_m$.
Since there are countably many such almost sure equalities, we get that almost surely there is equality for all $n, m$.
Let then $Y$ be the process equal to $Y_0$ on the event that the equalities hold, and equal to an arbitrary point $x_0 \in E$ otherwise.
Then first, $Y$ is a modification of $X$.
Then for any $\gamma < \sup_n (q_n - d)/p_n$ there is $n$ such that $\gamma < (q_n - d)/p_n$ and thus since $Y = Y_n$ the paths of $Y$ are locally Hölder continuous of order $\gamma$ almost surely.
\end{proof}

\color{blue}

\section{Construction of Brownian motion}
\label{S:BM}
\subsection{Projective family of Gaussian measures}

We build a projective family of Gaussian measures indexed by $\mathbb{R}_+$.
In order to do so, we need to define specific Gaussian measures on finite index sets $\{t_1, \ldots, t_n\}$.
We want to build a multivariate Gaussian measure on $\mathbb{R}^n$ with mean $0$ and covariance matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.

% \paragraph{First method: Gaussian increments}

% In this method, we build the Gaussian measure by adding independent Gaussian increments.

% \begin{definition}(Gaussian increment)\label{def:gaussianIncrement}
% For $v \ge 0$, the map from $\mathbb{R}$ to the probability measures on $\mathbb{R}$ defined by $x \mapsto \mathcal{N}(x, v)$ is a Markov kernel.
% We call that kernel the \emph{Gaussian increment} with variance $v$ and denote it by $\kappa^G_v$.
% \end{definition}

% TODO: perhaps the equality $\mathcal{N}(x, v) = \delta_x \ast \mathcal{N}(0, v)$ is useful to show that it is a kernel?

% \begin{definition}\label{def:gaussianFromIncrements}
%   %  \uses{def:gaussianIncrement}
% Let $0 \le t_1 \le \ldots \le t_n$ be non-negative reals.
% Let $\mu_0$ be the real Gaussian distribution $\mathcal{N}(0, t_1)$.
% For $i \in \{1, \ldots, n-1\}$, let $\kappa_i$ be the Markov kernel from $\mathbb{R}$ to $\mathbb{R}$ defined by $\kappa_i(x) = \mathcal{N}(x, t_{i+1} - t_i)$ (the Gaussian increment $\kappa^G_{t_{i+1} - t_i}$).
% Let $P_{t_1, \ldots, t_n}$ be the measure on $\mathbb{R}^n$ defined by $\mu_0 \otimes \kappa_1 \otimes \ldots \otimes \kappa_{n-1}$.
% \end{definition}

% TODO: explain the notation $\otimes$ in the lemma above: $\kappa_{n-1}$ takes the value at $n-1$ only to produce the distribution at $n$.

% \begin{lemma}\label{lem:isGaussian_gaussianFromIncrements}
%   %  \uses{def:gaussianFromIncrements, def:IsGaussian}
% $P_{t_1, \ldots, t_n}$ is a Gaussian measure on $\mathbb{R}^n$ with mean $0$ and covariance matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.
% \end{lemma}

% \begin{proof}

% \end{proof}


% \paragraph{Second method: covariance matrix}

We prove that the matrix $C_{ij} = \min(t_i, t_j)$ is positive semidefinite, which means that there exists a Gaussian distribution with mean 0 and covariance matrix $C$.

\begin{definition}[Gram matrix]\label{def:gramMatrix}
  \leanok
  %  \lean{Matrix.gram}
Let $v_1, \ldots, v_n$ be vectors in an inner product space $E$.
The Gram matrix of $v_1, \ldots, v_n$ is the matrix in $\mathbb{R}^{n \times n}$ with entries $G_{ij} = \langle v_i, v_j \rangle$ for $1 \leq i,j \leq n$.
\end{definition}


\begin{lemma}\label{lem:posSemidef_gramMatrix}
  %  \uses{def:gramMatrix}
  \leanok
  %  \lean{Matrix.gram_posSemidef}
A gram matrix is positive semidefinite.
\end{lemma}

\begin{proof}\leanok
Symmetry is obvious from the definition.
Let $x \in E$. Then
\begin{align*}
  \langle x, G x \rangle
  &= \sum_{i,j} x_i x_j \langle v_i, v_j \rangle
  \\
  &= \langle \sum_i x_i v_i, \sum_j x_j v_j \rangle
  \\
  &= \left\Vert \sum_i x_i v_i \right\Vert^2
  \\
  &\ge 0
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:C_eq_gramMatrix}
  %  \uses{def:gramMatrix}
  \leanok
Let $I = \{t_1, \ldots, t_n\}$ be a finite subset of $\mathbb{R}_+$.
For $i \le n$, let $v_i = \mathbb{I}_{[0, t_i]}$ be the indicator function of the interval $[0, t_i]$, as an element of $L^2(\mathbb{R})$.
Then the Gram matrix of $v_1, \ldots, v_n$ is equal to the matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.
\end{lemma}

\begin{proof}\leanok
By definition of the inner product in $L^2(\mathbb{R})$,
\begin{align*}
  \langle v_i, v_j \rangle
  &= \int_{\mathbb{R}} \mathbb{I}_{[0, t_i]}(x) \mathbb{I}_{[0, t_j]}(x) \: dx
  = \min(t_i, t_j)
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:posSemidef_brownianCov}
  \leanok
  %  \lean{ProbabilityTheory.posSemidef_brownianCovMatrix}
For $I = \{t_1, \ldots, t_n\}$ a finite subset of $\mathbb{R}_+$, let $C \in \mathbb{R}^{n \times n}$ be the matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.
Then $C$ is positive semidefinite.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:C_eq_gramMatrix, lem:posSemidef_gramMatrix}
$C$ is a Gram matrix by Lemma~\ref{lem:C_eq_gramMatrix}.
By Lemma~\ref{lem:posSemidef_gramMatrix}, it is positive semidefinite.
\end{proof}


\paragraph{Definition of the projective family and extension}

\begin{definition}[Projective family of the Brownian motion]\label{def:gaussianProjectiveFamily}
  %  \uses{def:multivariateGaussian, lem:posSemidef_brownianCov}
  \leanok
  %  \lean{ProbabilityTheory.gaussianProjectiveFamily}
For $I = \{t_1, \ldots, t_n\}$ a finite subset of $\mathbb{R}_+$, let $P^B_I$ be the multivariate Gaussian measure on $\mathbb{R}^n$ with mean $0$ and covariance matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.
We call the family of measures $P^B_I$ the \emph{projective family of the Brownian motion}.
\end{definition}


\begin{lemma}\label{lem:isProjectiveMeasureFamily_gaussianProjectiveFamily}
  %  \uses{def:gaussianProjectiveFamily, def:IsProjectiveMeasureFamily}
  \leanok
  %  \lean{ProbabilityTheory.isProjectiveMeasureFamily_gaussianProjectiveFamily}
The projective family of the Brownian motion is a projective family of measures.
\end{lemma}

\begin{proof}\leanok
  %  \uses{lem:isGaussian_multivariateGaussian, lem:covMatrix_map,lem:integral_id_multivariateGaussian, lem:covMatrix_multivariateGaussian, lem:IsGaussian.ext_iff}
Let $J \subseteq I$ be finite subsets of $\mathbb{R}_+$.
We need to show that the restriction from $\mathbb{R}^I$ to $\mathbb{R}^J$ (denote it by $\pi_{IJ}$) maps $P^B_I$ to $P^B_J$.

The restriction is a continuous linear map from $\mathbb{R}^I$ to $\mathbb{R}^J$.
The map of a Gaussian measure by a continuous linear map is Gaussian (Lemma~\ref{lem:isGaussian_map}).
It thus suffices to show that the mean and covariance matrix of the map are equal to the ones of $P^B_J$ by Lemma~\ref{lem:IsGaussian.ext_iff}.

The mean of the map is $0$, since the mean of $P^B_I$ is $0$ and the map is linear.

Let us turn to the covariance matrix. For any $i \in J$, the map $x : \mathbb{R}^I \mapsto \pi_{IJ}(x) i$ is equal to $x : \mathbb{R}^I \mapsto x i$. Let $i, j \in J$. The covariance of $x : \mathbb{R}^J \mapsto x i$ and $x : \mathbb{R}^J \mapsto x j$ with respect to ${\pi_{IJ}}_*P^B_J$ is equal to the covariance of $x : \mathbb{R}^I \mapsto \pi_{IJ}(x) i$ and $x : \mathbb{R}^I \mapsto \pi_{IJ}(x) j$ with respect to $P^B_I$, which is equal to the covariance of $x : \mathbb{R}^I \mapsto x i$ and $x : \mathbb{R}^I \mapsto x i$ with respect to $P^B_I$, which is equal to $t_i \land t_j$. But this is also the covariance of $x : \mathbb{R}^J \mapsto x i$ and $x : \mathbb{R}^J \mapsto x j$ with respect to $P^B_J$, so we are done.
\end{proof}


\begin{definition}\label{def:gaussianLimit}
  %  \uses{thm:kolmogorovExtension, lem:isProjectiveMeasureFamily_gaussianProjectiveFamily}
  \leanok
  %  \lean{ProbabilityTheory.gaussianLimit}
We denote by $P_B$ the projective limit of the projective family of the Brownian motion given by Theorem~\ref{thm:kolmogorovExtension}.
This is a probability measure on $\mathbb{R}^{\mathbb{R}_+}$.
\end{definition}

\color{black}



\section*{Acknowledgments}

Other contributors, who made one or two PRs: Jonas Bayer, Lorenzo Loccioli, Alessio Rondelli, Jérémy Scanvic

Blueprint: Patrick Massot (citation?)

Project template \cite{Monticone_LeanProject_2025} and technical support: Pietro Monticone

Lean/Mathlib community, Mathlib reviewers (Sébastien Gouëzel in particular?)

%%
%% Bibliography
%%

\printbibliography

\end{document}








%Some useful commands:
%\mathlib{}
%\leaninline{}
%\leanlink{url}{this is a link to Lean code}

%Example of Lean code:
%\begin{lstlisting}
%/-- A process `X : T → Ω → E` has independent increments if for any `n ≥ 2` and `t₁ ≤ ... ≤ tₙ`,
%the random variables `X t₂ - X t₁, X t₃ - X t₂, ...` are independent. -/
%def HasIndepIncrements {Ω T E : Type*} {mΩ : MeasurableSpace Ω} [Sub E]
%    [Preorder T] [MeasurableSpace E] (X : T → Ω → E) (P : Measure Ω) : Prop :=
%  ∀ n, ∀ t : Fin (n + 2) → T, Monotone t →
%    iIndepFun (fun i : Fin (n + 1) ↦ X (t i.succ) - X (t i.castSucc)) P
%\end{lstlisting}

%Inline code: \lstinline|sorry|

\begin{abstract}
Concise abstract.
\end{abstract}

% please fill in
\msc{}

% please fill in
\keywords{Formalization, Mathlib, Lean, Brownian motion}

\addbibresource{biblio.bib}

\renewcommand{\lean}[1]{\lstinline[language=lean]{#1}}

\begin{document}


\section{Introduction}

\subsection{Mathematical background}
\sloppy Brownian motion is arguably one of the most important stochastic processes (e.g.\ \cite{karatzas1991brownian, morters2010brownian}), and is used as a modeling tool across all sciences (physics: e.g.\ \cite{einstein1906theorie, bian2016111}; biology: e.g.\ \cite{erban2014molecular}; finance: e.g.\ \cite{davis2006louis}). Mathematically, Brownian Motion led to Wiener measure \cite{wiener1923differential}, which is the first instance of a probability measure on a function space, but also to developments such as Stochastic (Partial) Differential equations (see e.g.\ \cite{hairer2009introduction}).

\section{Acknowledgements}
Other contributors, who made one or two PRs: Jonas Bayer, Lorenzo Loccioli, Alessio Rondelli, Jérémy Scanvic

Blueprint: Patrick Massot (citation?)

Project template \cite{Monticone_LeanProject_2025} and technical support: Pietro Monticone

Lean/Mathlib community, Mathlib reviewers (Sébastien Gouëzel in particular?)

\printbibliography

\end{document}
