\chapter{Doob-Meyer Theorem}
\label{chap:doob_meyer}


This chapter starts with a short review of the properties of the Doob decomposition of an adapted process indexed on a discrete set, and then follows \cite{Beiglböck_Schachermayer_Veliyev_2012} which gives an elementary and short proof of the Doob-Meyer theorem.


\section{Doob decomposition in discrete time}


\begin{definition}\label{def:predictablePart}
  \uses{def:filtration}
  \mathlibok
  \lean{MeasureTheory.predictablePart}
Let $X : \mathbb{N} \to \Omega \to E$ be a process indexed by $\mathbb{N}$, for $E$ a Banach space.
Let $(\mathcal{F}_n)_{n\in\mathbb{N}}$ be a filtration on $\Omega$.
The predictable part of $X$ is the process $A : \mathbb{N} \to \Omega \to E$ defined for $n \ge 0$ by
$$A_n = \sum_{k=0}^{n-1} \mathbb{E}[X_{k+1}-X_k \mid \mathcal{F}_k].$$
\end{definition}

In what follows, we fix a process $X : \mathbb{N} \to \Omega \to E$ and a filtration $(\mathcal{F}_n)_{n \in \mathbb{N}}$, and denote by $A$ the predictable part of $X$.

\begin{lemma}\label{lem:predictablePart_zero}
  \uses{def:predictablePart}
  \mathlibok
  \lean{MeasureTheory.predictablePart_zero}
We have $A_0 = 0$.
\end{lemma}

\begin{proof}\leanok

\end{proof}

\begin{lemma}\label{lem:predictablePart_add_one}
  \uses{def:predictablePart}
  \leanok
  \lean{MeasureTheory.predictablePart_add_one}
For any integer $n \ge 0$, $A_{n+1} = A_n + \mathbb{E}[X_{n+1} - X_n \mid \mathcal{F}_n]$.
\end{lemma}

\begin{proof}\leanok
  \uses{def:predictablePart}
Let $n \in \mathbb{N}$. Then
\begin{align*}
  A_{n+1}
  & = \sum_{k=0}^n \mathbb{E}[X_{k+1}-X_k \mid \mathcal{F}_k] \\
  & = \sum_{k=0}^{n-1} \mathbb{E}[X_{k+1}-X_k \mid \mathcal{F}_k] + \mathbb{E}[X_{n+1}-X_n \mid \mathcal{F}_n] \\
  & = A_n + \mathbb{E}[X_{n+1} - X_n \mid \mathcal{F}_n],
\end{align*}
which concludes the proof.
\end{proof}

\begin{lemma}\label{lem:adapted_predictablePart}
  \uses{def:predictablePart}
  \mathlibok
  \lean{MeasureTheory.stronglyAdapted_predictablePart}
The predictable part $A$ is adapted to the filtration $(\mathcal{F}_{n+1})_{n \in \mathbb{N}}$.
\end{lemma}

\begin{proof}\mathlibok

\end{proof}

\begin{definition}\label{def:martingalePart}
  \uses{def:predictablePart}
  \mathlibok
  \lean{MeasureTheory.martingalePart}
Let $X : \mathbb{N} \to \Omega \to E$ be a process indexed by $\mathbb{N}$, for $E$ a Banach space.
Let $(\mathcal{F}_n)_{n\in\mathbb{N}}$ be a filtration on $\Omega$ and let $A$ be the predictable part of $X$ for that filtration.
The martingale part of $X$ is the process $M : \mathbb{N} \to \Omega \to E$ defined by $M_n = X_n - A_n$.
\end{definition}


\begin{lemma}\label{lem:predictable_predictablePart}
  \uses{def:predictablePart, def:predictable}
  \leanok
  \lean{MeasureTheory.isPredictable_predictablePart}
The predictable part of a process is predictable.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:predictable_nat_iff, lem:predictablePart_zero, lem:adapted_predictablePart}
By Lemma~\ref{lem:predictable_nat_iff}, the process $A$ is predictable if $A_0$ is $\mathcal{F}_0$-measurable and for all integer $n$, $A_{n+1}$ is $\mathcal{F}_n$-measurable. As $A_0 = 0$ from Lemma~\ref{lem:predictablePart_zero}, it is $\mathcal{F}_0$-measurable. Lemma~\ref{lem:adapted_predictablePart} allows to conclude the proof.
\end{proof}


\begin{lemma}\label{lem:martingale_martingalePart}
  \uses{def:martingalePart, def:Martingale}
  \mathlibok
  \lean{MeasureTheory.martingale_martingalePart}
Suppose that the filtration is sigma-finite.
Then the martingale part of an adapted process $X$ such that $X_n$ is integrable for all $n$ is a martingale.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:nondecreasing_predictablePart_of_submartingale}
  \uses{def:predictablePart, def:Submartingale}
  \leanok
  \lean{MeasureTheory.Submartingale.monotone_predictablePart}
The predictable part of a real-valued submartingale is an almost surely nondecreasing process.
\end{lemma}

\begin{proof}\leanok
  \uses{def:Submartingale, lem:condExp_sub_nonneg, lem:predictablePart_add_one}
Let $X$ be a submartingale and let $A$ be its predictable part. Then for all $n \geq 0$, from Lemma~\ref{lem:condExp_sub_nonneg} we have that almost surely
\begin{align*}
  A_{n+1} &= A_n + \mathbb{E}\left[ X_{n+1} - X_n | \mathcal{F}_n \right]
  \ge A_n
  \: .
\end{align*}
The first equality comes from Lemma~\ref{lem:predictablePart_add_one}. As $\mathbb{N}$ is countable, we deduce that almost surely, for all $n \in \mathbb{N}$, $A_{n+1} \ge A_n$.
Thus, $(A_n)_{n \in \mathbb{N}}$ is almost surely nondecreasing.
\end{proof}




\section{Komlòs Lemma}


%technically a more general version with Cesaro sums exists, but it is not needed for this case
%(see "J. Komlòs, A generalization of a problem of Steinhaus, Acta Math. Acad. Sci. Hungar. 18 (1967) 217–229").

\begin{lemma}\label{lem:komlos_convex}
  \leanok
  \lean{komlos_convex}
Let $(f_n)_{n\in\mathbb{N}}$ be a sequence in a vector space $E$ and $\phi : E \to \mathbb{R}_+$ be a function such that $\phi(f_n)$ is a bounded sequence.
For $\delta > 0$, let $S_\delta = \{(f, g) \mid \phi(f)/2 + \phi(g)/2 - \phi((f+g)/2) \ge \delta\}$.
Then there exist $g_n\in convex(f_n,f_{n+1},\cdots)$ such that for all $\delta > 0$, for $N$ large enough and $n, m \ge N$, $(g_n, g_m) \notin S_\delta$.
\end{lemma}

\begin{proof}
  \leanok
Let $B$ be the bound of $(\phi(f_n))_{n\in\mathbb{N}}$.
Then for all $n\in\mathbb{N}$ and $g\in convex(f_n,f_{n+1},\cdots)$ we have $\phi(g)\le B$ by convexity of $\phi$.
Let $r_n = \inf(\phi(g) \mid g\in convex(f_n, f_{n+1},\ldots))$.
By construction $(r_n)_{n\in\mathbb{N}}$ is nondecreasing.
Let $A = \sup_{n \ge 1} r_n$, which is finite (as $A \le B$) and for each $n$ we may pick some $g_n\in convex(f_n, f_{n+1},\ldots)$ such that $\phi(g_n) \le A+1/n$ by $\inf$ and $\sup$ definitions.

Let $\varepsilon \in (0, \delta/4)$.
By properties of $\sup$ there exists $\bar{n}$ such that $r_{\bar{n}} \ge A-\varepsilon$ and such that $\frac{1}{\bar{n}} \le \varepsilon$.
Let $m \ge k \ge \bar{n}$.
We have $(g_k+g_m)/2 \in convex(f_k,f_{k+1},\ldots)$ and it follows since $(r_n)_{n\in\mathbb{N}}$ is nondecreasing that $\phi((g_k+g_m)/2) \ge A - \varepsilon$.
Hence due to the ordering of $m,k,\bar{n}$,
\begin{align*}
  \phi(g_k)/2 + \phi(g_m)/2 - \phi((g_k+g_m)/2)
  &\le 2(A + \frac{1}{\bar{n}}) - 2(A - \varepsilon)
  \\
  &\le 4 \varepsilon
  \\
  &< \delta
  \: .
\end{align*}
Thus, for $n, m \ge \bar{n}$, $(g_n, g_m) \notin S_\delta$.
\end{proof}


\begin{lemma}\label{lem:komlos_norm}
  \leanok
  \lean{komlos_norm}
Let $H$ be a Hilbert space and $(f_n)_{n\in\mathbb{N}}$ a bounded sequence in $H$. Then there exist functions $g_n\in convex(f_n,f_{n+1},\cdots)$ such that $(g_n)_{n\in\mathbb{N}}$ converges in $H$.
\end{lemma}

\begin{proof}
  \leanok
  \uses{lem:komlos_convex}
Consider $\phi : H \to \mathbb{R}_+$ defined by $\phi(f) = \|f\|_2^2$, which is convex.
Then Lemma \ref{lem:komlos_convex} applied to $(f_n)_{n\in\mathbb{N}}$ and $\phi$ gives us functions $g_n\in convex(f_n,f_{n+1},\cdots)$ such that for every $\delta>0$ there exists $N$ such that for $n,m\geq N$, $(g_n,g_m)\notin S_\delta$.
Thus for every $\delta>0$ there exists $N$ such that for $n,m\geq N$,
\begin{align*}
  \|g_n\|_2^2/2 + \|g_m\|_2^2/2 - \|(g_n+g_m)/2\|_2^2
  &< \delta
  \: .
\end{align*}
But the left-hand side is equal to $\|g_n - g_m\|_2^2/4$ by the parallelogram identity, hence $(g_n)_{n\in\mathbb{N}}$ is a Cauchy sequence in $H$ and thus converges in $H$ by completeness.
\end{proof}


\begin{lemma}\label{lem:convex_of_converg_seq_is_converg}
  Let $(x_n)_{n \in \mathbb{N}}$ be a sequence in a real vector space converging to $x$.
  Let $\mathcal{C}((x_n))$ be the set of sequences $(y_n)_{n \in \mathbb{N}}$ such that for all $n$, $y_n \in convex(x_n, x_{n+1}, \ldots)$.
  Then we have
  \begin{enumerate}
    \item uniform convergence over $\mathcal{C}((x_n))$: for all $\varepsilon > 0$, there exists $\bar{n}$ such that for all $n \ge \bar{n}$, for all $(y_n)_{n \in \mathbb{N}} \in \mathcal{C}((x_n))$, $\Vert y_n - x \Vert \le \varepsilon$;
    \item pointwise convergence: for all $(y_n)_{n \in \mathbb{N}} \in \mathcal{C}((x_n))$, $(y_n)_{n \in \mathbb{N}}$ converges to $x$.
  \end{enumerate}
\end{lemma}

\begin{proof}
The second point is a direct consequence of the first one, so we only prove the first one.
Let $\varepsilon>0$.
By convergence of $x_n$, there exists $\bar{n}$ such that for all $n \ge \bar{n}$, $\Vert x_n-x \Vert \le \varepsilon$.
Let $a_{n, m}$ be convex weights such that $y_n = \sum_{m = n}^{N_n} a_{n, m} x_m$.
By triangular inequality it follows that for $n \ge \bar{n}$,
\begin{align*}
  \Vert y_n - x \Vert
  = \left\Vert \sum_{m = n}^{N_n} a_{n, m} x_m - x \right\Vert
  = \left\Vert \sum_{m = n}^{N_n} a_{n, m} (x_m - x) \right\Vert
  \le \sum_{m = n}^{N_n} a_{n, m} \Vert x_m - x \Vert
  \le \varepsilon
  \: .
\end{align*}
\end{proof}

By convex weights on $\mathbb{N}$, we mean a sequence of non-negative real numbers $(a_n)_{n \in \mathbb{N}}$ with finitely many nonzero entries such that $\sum_{n \in \mathbb{N}} a_n = 1$.
If $(a_m)_{m \in \mathbb{N}}$ are convex weights and $(b^n_m)_{n,m \in \mathbb{N}}$ is such that for all $n$, the $(b^n_m)$ are convex weights, then we denote by $(a_\cdot) * (b^\cdot_\cdot)$ the convex weights defined by $((a_\cdot) * (b^\cdot_\cdot))_m = \sum_{k} a_k b^k_m$.


\begin{lemma}\label{lem:komlos_convex_weights}
Let $E$ be a Hilbert space and for $i \in \mathbb{N}$, let $(x_n^{(i)})_{n \in \mathbb{N}}$ be a bounded sequence in $E$.
Then there exists a sequence of convex weights $(\prescript{k}{}{\lambda}^n_\cdot)_{k, n \in \mathbb{N}}$ with $\prescript{k}{}{\lambda}^n_m = 0$ for $m < n$ such that for all $k \in \mathbb{N}$, $\left(\sum_{m \ge n} \left((\prescript{k}{}{\lambda}^n_\cdot) * \ldots * (\prescript{1}{}{\lambda}^\cdot_\cdot)\right)_m x_m^{(k)}\right)_{n \in \mathbb{N}}$ converges.
\end{lemma}

\begin{proof}
  \uses{lem:komlos_norm}
Firstly by lemma \ref{lem:komlos_norm} applied to $(x_n^{(1)})_{n\in\mathbb{N}}$ in the Hilbert space $E$, there exist $g_n^1 \in convex(x_n^{(1)}, x_{n+1}^{(1)}, \ldots)$ (call its weights $\prescript{1}{}{\lambda}^n_n,\cdots,\prescript{1}{}{\lambda}^n_{N^1_n}$) such that $g_n^1$ converges to some $g^1$.

Secondly define $\tilde{g}_n^2$, convex combination of $x_n^{(2)}, x_{n+1}^{(2)}, \ldots$ with weights $\prescript{1}{}{\lambda}^n_n,\cdots,\prescript{1}{}{\lambda}^n_{N^1_n}$.
Applying lemma~\ref{lem:komlos_norm} to $(\tilde{g}_n^2)_{n\in\mathbb{N}}$ gives us $g_n^2 \in convex(\tilde{g}_n^2, \tilde{g}_{n+1}^2, \ldots)$ (call its weights $\prescript{2}{}{\lambda}^n_n,\cdots,\prescript{2}{}{\lambda}^n_{N^2_n}$) such that $g_n^2$ converges to some $g^2$.
$g_n^2$ is a convex combination of $x_n^{(2)}, x_{n+1}^{(2)}, \ldots$ with weights $(\prescript{2}{}{\lambda}^n_\cdot) * (\prescript{1}{}{\lambda}^\cdot_\cdot)$.

We continue iterating this process inductively.
At iteration $k$ we have weights $(\prescript{k}{}{\lambda}^n_\cdot * \ldots * \prescript{1}{}{\lambda}^\cdot_\cdot)$.
We define $\tilde{g}_n^{k+1}$ as the convex combination of $x_n^{(k+1)}, x_{n+1}^{(k+1)}, \ldots$ with those weights.
We apply Lemma~\ref{lem:komlos_norm} to $(\tilde{g}_n^{k+1})_{n\in\mathbb{N}}$ to get $g_n^{k+1} \in convex(\tilde{g}_n^{k+1}, \tilde{g}_{n+1}^{k+1}, \ldots)$ such that $g_n^{k+1}$ converges to some $g^{k+1}$. We denote its weights by $\prescript{k+1}{}{\lambda}^n_n,\cdots,\prescript{k+1}{}{\lambda}^n_{N^{k+1}_n}$.

We have thus defined, for all $k, n \in \mathbb{N}$, convex weights $(\prescript{k}{}{\lambda}^n_m)$ (that are zero for $m < n$) such that $\sum_{m \ge n}((\prescript{k}{}{\lambda}^n_\cdot * \ldots * \prescript{1}{}{\lambda}^\cdot_\cdot))_m x_m^{(k)}$ converges to $g^k$.
\end{proof}


\begin{lemma}\label{lem:komlos_convex_weights_tendsto}
  \uses{lem:komlos_convex_weights}
Let $E$ be a Hilbert space and for $i \in \mathbb{N}$, let $(x_n^{(i)})_{n \in \mathbb{N}}$ be a bounded sequence in $E$.
Let $(\prescript{k}{}{\lambda}^n_\cdot)_{k, n \in \mathbb{N}}$ be convex weights satisfying the conclusion of Lemma~\ref{lem:komlos_convex_weights}, and let $(g^i)_{i\in \mathbb{N}}$ be the sequence of limits of the sums.

Then for every $k \ge i$, the sequence $\left(\sum_{m \ge n} \left((\prescript{k}{}{\lambda}^n_\cdot) * \ldots * (\prescript{1}{}{\lambda}^\cdot_\cdot)\right)_m x_m^{(i)}\right)_{n \in \mathbb{N}}$ converges to $g^i$, uniformly in $k$.
\end{lemma}

\begin{proof}
  \uses{lem:convex_of_converg_seq_is_converg}
Let $i \in \mathbb{N}$.
By Lemma~\ref{lem:convex_of_converg_seq_is_converg}, there is uniform convergence over all convex combinations of the sequence $\left(\sum_{m \ge n} \left((\prescript{i}{}{\lambda}^n_\cdot) * \ldots * (\prescript{1}{}{\lambda}^\cdot_\cdot)\right)_m x_m^{(i)}\right)_{n \in \mathbb{N}}$ to $g^i$.
All sums $\sum_{m \ge n} \left((\prescript{k}{}{\lambda}^n_\cdot) * \ldots * (\prescript{1}{}{\lambda}^\cdot_\cdot)\right)_m x_m^{(i)}$ for $k \ge i$ are convex combinations of $\sum_{m \ge n} \left((\prescript{i}{}{\lambda}^n_\cdot) * \ldots * (\prescript{1}{}{\lambda}^\cdot_\cdot)\right)_m x_m^{(i)}$, hence they converge to $g^i$ uniformly in $k$.
\end{proof}


\begin{lemma}\label{lem:komlos_convex_weights_diagonal}
Let $E$ be a Hilbert space and for $i \in \mathbb{N}$, let $(x_n^{(i)})_{n \in \mathbb{N}}$ be a bounded sequence in $E$.
Then there exists a sequence of convex weights $(\eta^n_\cdot)_{n \in \mathbb{N}}$ with $\eta^n_m = 0$ for $m < n$ such that for all $i \in \mathbb{N}$, the sequence $\left(\sum_{m \ge n} \eta^n_m x_m^{(i)}\right)_{n \in \mathbb{N}}$ converges.
\end{lemma}

\begin{proof}
  \uses{lem:komlos_convex_weights, lem:komlos_convex_weights_tendsto}
Let $(\prescript{k}{}{\lambda}^n_\cdot)_{k, n \in \mathbb{N}}$ be convex weights satisfying the conclusion of Lemma~\ref{lem:komlos_convex_weights}, and let $(g^i)_{i\in \mathbb{N}}$ be the sequence of limits of the sums.
Let $\eta^n_m = (\prescript{n}{}{\lambda}^n_\cdot * \ldots * \prescript{1}{}{\lambda}^\cdot_\cdot)_m$.
We show that for all $i \in \mathbb{N}$, the sequence $\left(\sum_{m \ge n} \eta^n_m x_m^{(i)}\right)_{n \in \mathbb{N}}$ converges to $g^i$.

Let $i \in \mathbb{N}$.
By Lemma~\ref{lem:komlos_convex_weights_tendsto}, for all $\varepsilon > 0$, there exists $\bar{n}$ such that for all $n \ge \bar{n}$, for all $k \ge i$, $\left\Vert\sum_{m \ge n} \left((\prescript{k}{}{\lambda}^n_\cdot) * \ldots * (\prescript{1}{}{\lambda}^\cdot_\cdot)\right)_m x_m^{(i)} - g^i\right\Vert \le \varepsilon$.
Hence for $n \ge \max(\bar{n}, i)$,
\begin{align*}
  \left\Vert\sum_{m \ge n} \eta^n_m x_m^{(i)} - g^i\right\Vert
  &= \left\Vert\sum_{m \ge n} \left((\prescript{n}{}{\lambda}^n_\cdot) * \ldots * (\prescript{1}{}{\lambda}^\cdot_\cdot)\right)_m x_m^{(i)} - g^i\right\Vert
  \le \varepsilon
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:komlos_convex_aux}
Let $E$ be a Hilbert space and let $(f_n)_{n \in \mathbb{N}}$ be a sequence in $\Omega \to E$.
For $i \in \mathbb{N}$, set $f_n^{(i)} = f_n \mathbb{1}_{(\Vert f_n \Vert \le i)}$, such that $f_n^{(i)} \in L^2(E)$.
Then there exists a sequence of convex weights $\lambda_n^{n}, \ldots, \lambda_{N_n}^{n}$ such that the functions
$\left(\lambda_n^{n} f_n^{(i)} + \ldots + \lambda_{N_n}^{n} f_{N_n}^{(i)} \right)_{n\in\mathbb{N}}$
converge in $L^2(E)$ for every $i \in \mathbb{N}$.
\end{lemma}

\begin{proof}
  \uses{lem:komlos_convex_weights_diagonal}
Use Lemma~\ref{lem:komlos_convex_weights_diagonal} in the Hilbert space $L^2(E)$ with the sequence of sequences $(f_n^{(i)})$, which are bounded in $L^2(E)$ for each $i \in \mathbb{N}$.
\end{proof}


\begin{lemma}[Komlòs Lemma]\label{lem:komlos}
  Let $( f_n)_{n\in\mathbb{N}}$ be a uniformly integrable sequence of functions on a probability space $(\Omega , \mathcal{F} , P)$.
  Then there exist functions $g_n \in convex( f_n, f_{n+1}, \cdots)$ such that $(g_n)_{n\in\mathbb{N}}$ converges in  $L^1$.
\end{lemma}

\begin{proof}
  \uses{lem:komlos_convex_aux}
  For $i,n\in\mathbb{N}$ set $f_{n}^{(i)}:=f_n \mathbb{1}_{(|f_n|\leq i)}$ such that $f_{n}^{(i)}\in L^2$.
  Using \ref{lem:komlos_convex_aux} there exist for every $n$ convex weights $\lambda_n^{n}, \ldots, \lambda_{N_n}^{n}$ such that the functions
  $ \lambda_n^{n} f_n^{(i)} + \ldots+\lambda_{N_n}^{n} f_{N_n}^{(i)}$ converge in $L^2$ for every $i\in\mathbb{N}$.
  By uniform integrability, $\lim_{i\to \infty}\| f^{(i)}_n- f_n\|_1=0$, uniformly with respect to $n$.
  Hence, once again, uniformly with respect to $n$,
  $$ \textstyle\lim_{i\to\infty}\|  (\lambda_n^{n} f_n^{(i)} + \ldots+\lambda_{N_n}^{n} f_{N_n}^{(i)})-(\lambda_n^{n} f_n + \ldots+\lambda_{N_n}^{n} f_{N_n})\|_1= 0.$$
  Thus $(\lambda_n^{n} f_n + \ldots+\lambda_{N_n}^{n} f_{N_n})_{n\geq 1}$  is a Cauchy sequence in $L^1$.
\end{proof}


\paragraph{Komlòs lemma for nonnegative random variables}

\begin{lemma}[Komlòs lemma - nonnegative, a.e. convergence]\label{lem:komlos_ennreal}
  \leanok
  \lean{komlos_ennreal}
Let $(f_n)_{n\in\mathbb{N}}$ be a sequence of random variables with values in $[0, \infty]$.
Then there exist random variables $g_n \in convex( f_n, f_{n+1}, \cdots)$ such that $(g_n)_{n\in\mathbb{N}}$ converges almost surely to a random variable $g$.
\end{lemma}

\begin{proof}
  \uses{lem:komlos_convex}
Let $\phi : (\Omega \to [0, \infty]) \to [0, \infty]$ be defined by $\phi(X) = \mathbb{E}[e^{-X}]$.
Then $\phi$ is convex and $\phi(f_n) \le 1$ for all $n$.
By Lemma~\ref{lem:komlos_convex}, there exist $g_n \in convex( f_n, f_{n+1}, \cdots)$ such that for all $\delta>0$, for $N$ large enough and $n, m \ge N$,
\begin{align*}
  \mathbb{E}[e^{-g_n}]/2 + \mathbb{E}[e^{-g_m}]/2 - \mathbb{E}[e^{-(g_n + g_m)/2}] < \delta
  \: .
\end{align*}

For $\varepsilon > 0$, let $B_\varepsilon = \{(x, y) \in [0, \infty]^2 \mid \vert x - y \vert \ge \varepsilon \text{ and } \min\{x, y\} \le 1/\varepsilon \}$.
Then for all $x, y$,
\begin{align*}
  \left\vert e^{-x} - e^{-y} \right\vert
  &\le \varepsilon + 2 e^{-1/\varepsilon} + 2 \mathbb{1}_{B_\varepsilon}(x, y)
  \: .
\end{align*}
Hence for any pair of random variables $(X, Y)$ with values in $[0, \infty]$,
\begin{align*}
  \mathbb{E}\left[\left\vert e^{-X} - e^{-Y} \right\vert\right]
  &\le \varepsilon + 2 e^{-1/\varepsilon} + 2 P((X, Y) \in B_\varepsilon) \: .
\end{align*}
On the other hand, for $(x, y) \in B_\varepsilon$, there exists $\delta_\varepsilon > 0$ such that
\begin{align*}
  e^{-x}/2 + e^{-y}/2 - e^{-(x + y)/2} \ge \delta_\varepsilon
  \: .
\end{align*}
Thus,
\begin{align*}
  P((X, Y) \in B_\varepsilon)
  &\le \frac{1}{\delta_\varepsilon} \mathbb{E}\left[ e^{-X}/2 + e^{-Y}/2 - e^{-(X + Y)/2} \right]
  \: .
\end{align*}
For $n, m \ge N$ large enough so that we can apply the first inequality of this proof with $\delta = \varepsilon \delta_\varepsilon$, we deduce that
\begin{align*}
  \mathbb{E}\left[\left\vert e^{-g_n} - e^{-g_m} \right\vert\right]
  &\le \varepsilon + 2 e^{-1/\varepsilon} + \frac{2}{\delta_\varepsilon} \mathbb{E}\left[ e^{-g_n}/2 + e^{-g_m}/2 - e^{-(g_n + g_m)/2} \right] \\
  &\le \varepsilon + 2 e^{-1/\varepsilon} + 2 \varepsilon
  \: .
\end{align*}
As $\varepsilon$ is arbitrary, we deduce that $(e^{-g_n})_{n\in\mathbb{N}}$ is a Cauchy sequence in $L^1$ and thus converges in $L^1$ to some random variable $h$.
Therefore, it has a subsequence $(e^{-g_{n_k}})_{k\in\mathbb{N}}$ converging almost surely to $h$.
Finally, the subsequence of $g_n$ converges almost surely to $g = -\log(h)$.
\end{proof}


\section{Doob-Meyer decomposition}



For uniqueness of Doob-Meyer Decomposition we will need theorem \ref{thm:IsLocalMartingale.eq_zero_of_finiteVariation}.

We now start the construction for the existence part.


\begin{definition}[Dyadics]\label{def:dyadics}
For $T>0$, let $\mathcal{D}_n^T = \left\lbrace \frac{k}{2^n}T \mid k=0,\cdots 2^n\right\rbrace$ be the set of dyadics at scale $n$ and let $\mathcal{D}^T=\bigcup_{n\in\mathbb{N}}\mathcal{D}_n^T$ be the set of all dyadics of $[0,T]$.
\end{definition}

TODO: everywhere below, $S$ is a cadlag submartingale of class D on $[0,T]$?


\begin{definition}[A]\label{def:A}
  \uses{def:dyadics, def:predictablePart}
Define $A_0=0$ and for $t\in\mathcal{D}_n^T$ positive,
\begin{align*}
A^n_t
&=A^n_{t-T2^{-n}} + \mathbb{E}\left[ S_t-S_{t-T2^{-n}}|\mathcal{F}_{t-T2^{-n}}\right]
\: .
\end{align*}
\end{definition}


\begin{definition}[M]\label{def:M}
  \uses{def:A, def:martingalePart}
For $t\in\mathcal{D}_n^T$, define $M^n_t = S_t-A^n_t$~.
\end{definition}


\begin{lemma}\label{lem:Doob_Meyer_Finite_Predictable}
  \uses{def:A, def:predictable}
  $(A^n_t)_{t\in\mathcal{D}_n^T}$ is a predictable process.
\end{lemma}

\begin{proof}
  \uses{lem:predictable_nat_iff, lem:predictable_predictablePart}
  Trivial
\end{proof}


\begin{lemma}\label{lem:Doob_Meyer_Finite_Martingale}
  \uses{def:M, def:Martingale}
  $(M^n_t)_{t\in\mathcal{D}_n^T}$ is a martingale.
\end{lemma}

\begin{proof}
  \uses{lem:martingale_martingalePart}
  Trivial
\end{proof}


\begin{lemma}\label{lem:Predict_Part_Increasing}
  \uses{def:A}
  $(A^n_t)_{t\in\mathcal{D}_n^T}$ is an increasing process.
\end{lemma}

\begin{proof}
  \uses{def:Submartingale, lem:nondecreasing_predictablePart_of_submartingale}
$S$ is a submartingale:
\begin{align*}
  A^n_{t+T2^{-n}} - A^n_t
  &= \mathbb{E}\left[ S_{t+T2^{-n}}-S_t|\mathcal{F}_t\right] \ge 0
  \: .
\end{align*}
\end{proof}


\begin{definition}[Hitting time for $A$]\label{def:hittingAGT}
  \uses{def:A}
Let $c>0$. Define the hitting time on $\mathcal{D}^T_n$
\begin{align*}
  \tau_n(c)
  &= \inf\{t \in \mathcal{D}^T_n \mid A^n_{t + 2^{-n}T} > c\} \wedge T
  \: .
\end{align*}
\end{definition}


\begin{lemma}\label{lem:IsStoppingTime_hittingAGT}
  \uses{def:IsStoppingTime,def:hittingAGT}
  $\tau_n(c)$ is a stopping time.
\end{lemma}

\begin{proof}
Since $A^n_{t}$ is predictable, $A^n_{t + 2^{-n}T}$ is adapted.
The hitting time of an adapted process is a stopping time (we use the discrete time version of that result here, not the full Début theorem).
\end{proof}


\begin{lemma}\label{lem:A_hittingAGT_le}
  \uses{def:hittingAGT}
$A^n_{\tau_n(c)} \le c$ and if $\tau_n(c) < T$ then $A^n_{\tau_n(c)+T2^{-n}} > c$.
\end{lemma}

\begin{proof}

\end{proof}


\begin{lemma}\label{lem:A_hittingAGT_sub_ge}
  \uses{def:hittingAGT}
Let $a, b > 0$ with $a \le b$. If $\tau_n(b) < T$ then $A^n_{\tau_n(b)+T2^{-n}} - A^n_{\tau_n(a)} \ge b - a$.
\end{lemma}

\begin{proof}
  \uses{lem:A_hittingAGT_le}

\end{proof}


\begin{lemma}\label{lem:A_uniform_integrable}
  \uses{def:A}
  The sequence $(A^n_T)_{n\in\mathbb{N}}$ is uniformly integrable (bounded in $L^1$ norm).
\end{lemma}

\begin{proof}
  \uses{lem:Doob_Meyer_Finite_Predictable,lem:Predict_Part_Increasing,lem:Doob_Meyer_Finite_Martingale,lem:IsStoppingTime_hittingAGT,lem:A_hittingAGT_sub_ge}
  WLOG $S_T=0$ and $S_t\leq 0$ (else consider $S_t-\mathbb{E}\left[S_T\vert\mathcal{F}_{t}\right]$).

  We have that $0=S_T=M^n_T+A^n_T$. Thus
  \begin{equation}\label{equation_DM_e1}
  M^n_T=-A^n_T.
  \end{equation}
  Since $M^n$ is a martingale it follows by optional sampling that for any $(\mathcal{F}_t)_{t\in\mathcal{D}_n}$ stopping time $\tau$
  \begin{equation}\label{equation_DM_e2}
  S_\tau=M^n_\tau+A^n_\tau = \mathbb{E}[M^n_T\vert\mathcal{F}_\tau]+A^n_\tau\stackrel{\eqref{equation_DM_e1}}{=} -\mathbb{E}[A^n_T\vert\mathcal{F}_\tau]+A^n_\tau.
  \end{equation}
  Let $c>0$. By Lemma~\ref{lem:IsStoppingTime_hittingAGT}, $\tau_n(c)$ (Definition~\ref{def:hittingAGT}) is a stopping time.
  % Define the last time when $A^n$ has always been inside $[0,c]$, by the Début Theorem \ref{thm:hitting_is_stopping_time} and the fact that $A^n$ is predictable the following is a stopping time
  % $$
  % \tau_n(c)=\inf\left(\frac{j-1}{2^n}T\vert\, A^n_{jT2^{-n}}>c\right)\wedge T.
  % $$
  By construction $A^n_{\tau_n(c)}\leq c$. It follows that
  \begin{equation}\label{equation_DM_e3}
  S_{\tau_n(c)}\stackrel{\eqref{equation_DM_e2}}{=}-\mathbb{E}[A^n_T\vert\mathcal{F}_{\tau_n(c)}]+A^n_{\tau_n(c)}\leq -\mathbb{E}[A^n_T\vert\mathcal{F}_{\tau_n(c)}]+c.
  \end{equation}
  Since $(A^n_T>c)=(\tau_n(c)<T)$ we have
  \begin{align}\nonumber
  \int_{(A^n_T>c)}A^n_TdP&=\int_{(\tau_n(c)<T)}A^n_TdP\stackrel{\mathrm{Tower}}{=}\int_{(\tau_n(c)<T)}\mathbb{E}[A^n_T\vert\mathcal{F}_{\tau_n(c)}]dP\\
  &\stackrel{\eqref{equation_DM_e3}}{\leq} cP(\tau_n(c)<T)-\int_{\tau_n(c)<T}S_{\tau_n(c)}dP.\label{equation_DM_e4}
  \end{align}
  Now we notice that $(\tau_n(c)<T)\subseteq (\tau_n(c/2)<T)$, thus
  \begin{align}\nonumber
  \int_{\tau_n(c/2)<T}-S_{\tau_n(c/2)}dP
  &\stackrel{\eqref{equation_DM_e2}}{=}\int_{(\tau_n(c/2))<T}\mathbb{E}[A^n_T\vert\mathcal{F}_{\tau_n(c/2)}]-A^n_{\tau_n(c/2)}dP \nonumber
  \\
  &\stackrel{\mathrm{Tower}}{=}\int_{(\tau_n(c/2)<T)}A^n_t-A^n_{\tau_n(c/2)}dP\nonumber
  \\
  &\geq \int_{(\tau_n(c)<T)}A^n_t-A^n_{\tau_n(c/2)}dP\nonumber
  \\
  \intertext{(over the event $(\tau_n(c)<T)$ $A^n_T\geq c$ and $A^n_{\tau_n(c/2)}\leq c/2$, thus $A^n_T-A^n_{\tau_n(c/2)}\geq c/2$)}
  &\geq \frac{c}{2}P(\tau_n(c)<T).\label{equation_DM_e5}
  \end{align}
  It follows
  $$
  \int_{(A^n_T>c)}A^n_TdP\stackrel{\eqref{equation_DM_e4}}{\leq}cP(\tau_n(c)<T)-\int_{\tau_n(c)<T}S_{\tau_n(c)}dP\stackrel{\eqref{equation_DM_e5}}{\leq}-2\int_{\tau_n(c/2)<T}S_{\tau_n(c/2)}dP-\int_{\tau_n(c)<T}S_{\tau_n(c)}dP.
  $$
  We may notice that
  $$
  P(\tau_n(c)<T)=P(A^n_T>c)\stackrel{Markov}{\leq}\frac{\mathbb{E}[A^n_T]}{c}=-\frac{\mathbb{E}[M^n_T]}{c}\stackrel{mg}{=}-\frac{\mathbb{E}[S_0]}{c}
  $$
  which goes to $0$ uniformly in $n$ as $c$ goes to infinity.
  This implies that $\int_{(A^n_T>c)}A^n_TdP$ is uniformly bounded in $n$ due to the fact that $S$ is of class $D$. And so also the $L^1$ norm is uniformly bounded.
\end{proof}


\begin{lemma}\label{lem:M_uniform_integrable}
  The sequence $(M^n_T)_{n\in\mathbb{N}}$ is uniformly integrable (bounded in $L^1$ norm).
\end{lemma}

\begin{proof}
  \uses{lem:A_uniform_integrable, def:classD}
  $M^n_T=S_T-A^n_T$, also $S$ is of class $D$ and $A^n_T$ is uniformly integrable.
\end{proof}


\begin{lemma}\label{lem:incr_fun_lim_right_cont_limsup_ineq}
  If $f_n, f : [0, 1] \rightarrow \mathbb{R}$ are increasing functions such that $f$ is right continuous and
  $\lim_n f_n(t) = f (t)$ for $t \in\mathcal{D}^T$, then  $\limsup_n  f_n(t) \leq f (t)$ for all $t \in [0, T]$.
\end{lemma}

\begin{proof}
  Let $t\in[0,T]$ and $s\in\mathcal{D}^T$ such that $t<s$. We have
  $$
  \limsup_n f_n(t)\leq \limsup_n f_n(s)=f(s).
  $$
  Since the above is true uniformly in $s$ in particular since $f$ is right-continuous
  $$
  \limsup_n f_n(t)\leq\lim_{\stackrel{s\rightarrow t^+}{s\in\mathcal{D}^T}}f(s)=f(t).
  $$
\end{proof}


\begin{lemma}\label{lem:incr_fun_lim_right_cont_lim_eq}
  If $f_n, f : [0, 1] \rightarrow \mathbb{R}$ are increasing functions such that $f$ is right continuous and
  $\lim_n f_n(t) = f (t)$ for $t \in\mathcal{D^T}$, if $f$ is continuous in $t\in[0,T]$ then $\lim_n  f_n(t) = f (t)$.
\end{lemma}

\begin{proof}
  \uses{lem:incr_fun_lim_right_cont_limsup_ineq}
  By lemma \ref{lem:incr_fun_lim_right_cont_limsup_ineq} it is enough to show that $\liminf_n f_n(t)\geq f(t)$.
  Let $s\in\mathcal{D}^T$ such that $t>s$. We have
  $$
  \liminf_n f_n(t)\geq \liminf_n f_n(s)=f(s).
  $$
  Since the above is true uniformly in $s$ in particular since $f$ is continuous in $t$
  $$
  \liminf_n f_n(t)\geq\lim_{\stackrel{s\rightarrow t^-}{s\in\mathcal{D}^T}}f(s)=f(t).
$$
\end{proof}

Define $M^n_t$ on $[0,T]$ using $M^n_t=\mathbb{E}[M^n_T\vert\mathcal{F}_t]$.

\begin{lemma}\label{lem:M_n_cadlag_mg}
  $M^n_t$ admits a modification which is a cadlag martingale.
\end{lemma}

\begin{proof}
  \uses{thm:Martingale.exists_cadlag_modification}
  By theorem \ref{thm:Martingale.exists_cadlag_modification}
\end{proof}

From this point onwards $M^n_t$ will be redefined as the modification from lemma \ref{lem:M_n_cadlag_mg}.

\begin{lemma}\label{lem:M_cal_converges_L1}
  There are convex weights $\lambda^n_n,\cdots,\lambda^n_{N_n}$ such that
  $\mathcal{M}^n_T\stackrel{L^1}{\rightarrow}M$, where $\mathcal{M}^n:=\lambda^n_nM^n+\cdots +\lambda^n_{N_n}M^{N_n}.$
\end{lemma}
\begin{proof}
  \uses{lem:M_uniform_integrable,lem:komlos}
  By lemma \ref{lem:M_uniform_integrable} $(M^n_T)_{n\in\mathbb{N}}$ is uniformly bounded in $L^1$, thus by lemma \ref{lem:komlos} there are convex weights $\lambda^n_n,\cdots,\lambda^n_{N_n}$ such that
  $\mathcal{M}^n_T\stackrel{L^1}{\rightarrow}M$, where $\mathcal{M}^n:=\lambda^n_nM^n+\cdots +\lambda^n_{N_n}M^{N_n}.$
\end{proof}

\begin{lemma}\label{lem:M_cal_cadlag}
  $\mathcal{M}^n$ is cadlag.
\end{lemma}
\begin{proof}
  \uses{lem:M_n_cadlag_mg,lem:M_cal_converges_L1}
  By construction and \ref{lem:M_n_cadlag_mg}
\end{proof}

Let \begin{equation}\label{equation_DM_e6} M_t = \mathbb{E}[M\vert\mathcal{F}_t].\end{equation}

\begin{lemma}\label{lem:M_cadlag_mg}
  $M_t$ admits a martingale cadlag modification.
\end{lemma}
\begin{proof}
  \uses{lem:M_cal_converges_L1, thm:Martingale.exists_cadlag_modification}
  By construction $M_t$ is a martingale and thus by theorem \ref{thm:Martingale.exists_cadlag_modification} admits a cadlag martingale modification
  ($M_t$ is a version of $\mathbb{E}[M\vert\mathcal{F}_t]$ and thus passing to modification does not pose any problem).
\end{proof}

From this point onwards $M^n_t$ will be redefined as the modification from lemma \ref{lem:M_cadlag_mg}.
Define
\begin{itemize}
  \item Extend now $A^n$ as a left continuous process $A^n_s:=\sum_{t\in\mathcal{D}^T_n}A^n_t\mathbb{1}_{]t-2^{-n},t]}(s)$
  \item $\mathcal{A}^n=\lambda^n_nA^n+\cdots +\lambda^n_{N_n}A^{N_n}$
  \item $A_t=S_t-M_t$
\end{itemize}

\begin{lemma}\label{lem:M1_komlos}
  For every $t\in[0,T]$ we have $\mathcal{M}^n_t\stackrel{L^1}{\rightarrow}M_t$.
\end{lemma}
\begin{proof}
  \uses{lem:M_cal_converges_L1}
  We may notice that by Jensen's inequality, the tower lemma and lemma \ref{lem:M_cal_converges_L1}
  \begin{gather}\nonumber
    \mathbb{E}[|\mathcal{M}^n_t-M_t|]=\mathbb{E}[|\mathbb{E}[\mathcal{M}^n_T-M\vert\mathcal{F}_t]|]\leq \mathbb{E}[|\mathcal{M}^n_T-M|]\rightarrow0,\\
    \Rightarrow\mathcal{M}^n_t\stackrel{L^1}{\rightarrow} M_t,\quad \forall t\in[0,T].\label{equation_DM_e7}
  \end{gather}
\end{proof}

\begin{lemma}\label{lem:A_cal_conv_A_on_D_T}
  There exists a set $E\subseteq\Omega$, $P(E)=0$ and a subsequence $k_n$ such that $\lim_n\mathcal{A}^{k_n}_t(\omega)=A_t(\omega)$ for every $t\in\mathcal{D}^T,\omega\in\Omega\setminus E$.
\end{lemma}
\begin{proof}
  \uses{lem:M1_komlos}
  By Lemma \ref{lem:M1_komlos}
  $$
  \mathcal{A}^n_t=S_t-\mathcal{M}^n_t\stackrel{L^1}{\rightarrow}S_t-M_t=A_t,\quad\forall t\in\mathcal{D}^T.
  $$
  $\mathcal{D}^T$ is countable we can arrange the elements as $(t_n)_{n\in\mathbb{N}}$.
  Given $t_0\in\mathcal{D}^T$ there exists a subsequence $k^{0}_n$ for which $\mathcal{A}^{k^{0}_n}_{t_0}$ converges to $A_{t_0}$ over the set $\Omega\setminus E_{0}$ where $P(E_{0})=0$.
  Suppose we have a sequence $k^m_n$ for which $\mathcal{A}^{k^j_n}_{t_j}$ converges to $A_{t_j}$ over the set $\Omega\setminus E_{m}$ where $P(E_{m})=0$ for each $j=0,\cdots,m$.
  From this subsequence we may extract a new subsequence $k^{m+1}_n$ for which $\mathcal{A}^{k^{m+1}_n}_{t_{m+1}}$ converges to $A_{t_{m+1}}$ over the set $\Omega\setminus E_{m+1}$ where $P(E_{m+1})=0$.
  By construction over this subsequence the convergence for $t_0,\cdots,t_m$ still applies.
  With a diagonal argument we obtain the final result with $E=\bigcup_n E_n$.
\end{proof}

\begin{lemma}\label{lem:A_increasing}
  $(A_t)_{t\in[0,T]}$ is an increasing process.
\end{lemma}
\begin{proof}
  \uses{lem:A_cal_conv_A_on_D_T, lem:Predict_Part_Increasing, lem:M_cadlag_mg}
  Since $\mathcal{A}^n_t$ is increasing on $\mathcal{D}^T$ by lemma \ref{lem:A_cal_conv_A_on_D_T} also $A$ is almost surely increasing on $\mathcal{D}^T$.
  Since $S,M$ are cadlag also $A$ is cadlag (thus right-continuous). It follows that $A$ must be increasing on $[0,T]$.
\end{proof}

\begin{lemma}\label{lem:lim_Exp_A_n_tau_is_Exp_A_tau}
  Let $\tau$ be an $(\mathcal{F}_t)_{t\in[0,T]}$ stopping time. We have $\lim_n\mathbb{E}[A^n_\tau]=\mathbb{E}[A_\tau]$.
\end{lemma}
\begin{proof}
  \uses{lem:M_cadlag_mg, lem:M_n_cadlag_mg, def:classD}
  Let $\sigma_n:=\inf\left(t\in\mathcal{D}^T_n\vert t>\tau\right)$. By construction of $A^n$ we have $A^n_\tau=A^n_{\sigma_n}$.
  Also $\sigma_n\searrow\tau$. Since $S$ is of class $D$ and cadlag we have
  \begin{align*}
    \mathbb{E}[A^n_\tau]&=\mathbb{E}[A^n_{\sigma_n}]=\mathbb{E}[S_{\sigma_n}]-\mathbb{E}[M^n_{\sigma_n}]=\mathbb{E}[S_{\sigma_n}]-\mathbb{E}[M^n_0]=\\
    &=\mathbb{E}[S_{\sigma_n}]-\mathbb{E}[S_0]\rightarrow \mathbb{E}[S_\tau]-\mathbb{E}[M_0]=\mathbb{E}[S_\tau]-\mathbb{E}[M_\tau]=\mathbb{E}[A_\tau].
  \end{align*}
\end{proof}

\begin{lemma}\label{lem:limsup_A_n_tau_is_A_tau_ae}
  Let $\tau$ be an $(\mathcal{F}_t)_{t\in[0,T]}$ stopping time. We have $\limsup_n \mathcal{A}_\tau^n = A_\tau$.
\end{lemma}
\begin{proof}
  \uses{lem:lim_Exp_A_n_tau_is_Exp_A_tau, lem:incr_fun_lim_right_cont_limsup_ineq, lem:Predict_Part_Increasing, lem:A_cal_conv_A_on_D_T}
  Firstly we notice that $\liminf_n \mathbb{E}[A_\tau^n]  \leq \limsup_n  \mathbb{E}  [\mathcal{A}_\tau^n  ]  \leq \mathbb{E}[\limsup_n  \mathcal{A}_\tau^n  ]  \leq \mathbb{E}[ A_\tau ]$,
  where the first inequality is justified by the definition of limsup and liminf and the fact that
  $$
  \sup_{k\geq n}\mathbb{E}[\mathcal{A}^k_\tau]\geq \sum_{m=k}^{N_k}\lambda^k_m\mathbb{E}[A^m_\tau]\geq \sum_{m=k}^{N_k}\lambda^k_m\inf_{j\geq n}\mathbb{E}[A^j_\tau]=\inf_{k\geq n}\mathbb{E}[A^k_\tau]
  $$
  the third inequality by \ref{lem:incr_fun_lim_right_cont_limsup_ineq}.
  Let's prove the second inequality: observe that
  $$
  \mathcal{A}^n_\tau= A_1+\mathcal{A}^n_\tau-A_1\leq A_1+(\mathcal{A}^n_\tau-A_1)_+,
  $$
  thus it follows that $\mathcal{A}^n_\tau - (\mathcal{A}^n_\tau-A_1)_+\leq A_1$; since $A_1$ is an integrable guardian the inverse Fatou Lemma may be applied to show together with limsup properties that
  \begin{align*}
    \limsup_n\mathbb{E}[\mathcal{A}^n_\tau]+0 &= \limsup_n\mathbb{E}[\mathcal{A}^n_\tau]+\liminf_n-\mathbb{E}[(\mathcal{A}^n_\tau-A_1)_+] \leq \limsup_n\mathbb{E}[\mathcal{A}^n_\tau-(\mathcal{A}^n_\tau-A_1)_+]\leq\\
    &\leq \mathbb{E}[\limsup_n\mathcal{A}^n_\tau-(\mathcal{A}^n_\tau-A_1)_+]\leq \mathbb{E}[\limsup_n\mathcal{A}^n_\tau]-\mathbb{E}[\liminf_n(\mathcal{A}^n_\tau-A_1)_+]\leq\mathbb{E}[\limsup_n\mathcal{A}^n_\tau],
    \end{align*}
  where the first equality is justified by the fact that $\mathcal{A}^n_\tau\leq\mathcal{A}^n_1\rightarrow A_1$ almost surely.
  Due to lemma \ref{lem:lim_Exp_A_n_tau_is_Exp_A_tau} and \ref{lem:incr_fun_lim_right_cont_limsup_ineq} the first sequence of inequalities is a sequence of equalities, thus
  we know that $A_\tau- \limsup_n \mathcal{A}_\tau^n $ is an a.s. nonnegative function with null expected value, and thus it must be almost everywhere null.
\end{proof}

\begin{theorem}\label{thm:Doob_Meyer}
  Let $S = (S_t )_{0\leq t\leq T}$ be a cadlag submartingale of class $D$.
  Then, $S$ can be written in a unique way in the form  $S = M + A$ where $M$ is a cadlag martingale and $A$ is a predictable increasing process starting at $0$.
\end{theorem}
\begin{proof}
  \uses{lem:A_increasing, lem:M_cadlag_mg, lem:limsup_A_n_tau_is_A_tau_ae, lem:incr_fun_lim_right_cont_lim_eq}
  By construction $M$ is a cadlag martingale and $A_0=0$ and by lemma \ref{lem:A_increasing} $A$ is increasing. It suffices to show that $A$ is predictable.
  $A^n,\mathcal{A}^n$ are left continuous and adapted, and thus they are predictable (measurable wrt the predictable sigma algebra (the one generated by left-cont adapted processes)).
  It is enough to show that $\omega-a.e.$, $\forall t\in[0,T]$, $\limsup_n\mathcal{A}^n_t(\omega)=A_t(\omega)$.

  By lemma \ref{lem:incr_fun_lim_right_cont_lim_eq} that is true for any continuity point of $A$. Since $A$ is increasing it can only have a finite amount of jumps larger than $1/k$ for any $k\in\mathbb{N}$.
  Consider now $\tau_{q,k}$ the family of stopping times equal to the $q$-th time that the process $A_t$ has a jump higher than $1/k$. This is a countable family.
  Given a time $t$ and a trajectory $\omega$ there are only two possibilities: either $A$ is continuous or not at time $t$ along $\omega$.
  If $A$ is continuous at time $t$ we have $\limsup_n\mathcal{A}^n_t(\omega)=A_t(\omega)$, if it jumps there exists $q(\omega),k(\omega)$ such that $t=\tau_{q(\omega),k(\omega)}(\omega)$.
  Due to lemma \ref{lem:limsup_A_n_tau_is_A_tau_ae} we know that $\limsup_n A^n_{\tau_{q,k}} = A_{\tau_{q,k}}$ for each $q,k$ almost surely. Thus, since it is an intersection of a countable amount of almost sure
  events $\forall\omega\in\Omega'$ with $P(\Omega')=1$, for each $q,k$ $\limsup_n A^n_{\tau_{q,k}}(\omega) = A_{\tau_{q,k}}(\omega)$ ($\omega$ does not depend upon $q,k$).
  Consequently, $\forall\omega\in\Omega'$ we have $\limsup_n\mathcal{A}^n_t(\omega)=\limsup_n\mathcal{A}^n_{\tau_{q(\omega),k(\omega)}}(\omega)=A_{\tau_{q(\omega),k(\omega)}}(\omega)=A_t(\omega)$
\end{proof}



\section{Local version of the Doob-Meyer decomposition}


\begin{theorem}[Doob-Meyer decomposition]\label{thm:local_doobMeyer}
  \uses{def:IsLocalSubmartingale, def:predictable, def:IsLocalMartingale}
  \leanok
  \lean{ProbabilityTheory.IsLocalSubmartingale.doob_meyer}
An adapted process $X$ is a cadlag local submartingale iff $X = M + A$ where $M$ is a cadlag local martingale and $A$ is a predictable, cadlag, locally integrable and increasing process starting at $0$.
The processes $M$ and $A$ are uniquely determined by $X$ a.s.
\end{theorem}

\begin{proof}
  \uses{lem:IsLocalSubmartingale.locally_classD, thm:Doob_Meyer}

\end{proof}
