\chapter{Doob-Meyer Theorem}
\label{chap:doob_meyer}


This chapter starts with a short review of the properties of the Doob decomposition of an adapted process indexed on a discrete set, and then follows \cite{Beiglböck_Schachermayer_Veliyev_2012} which gives an elementary and short proof of the Doob-Meyer theorem.


\section{Doob decomposition in discrete time}


\begin{definition}\label{def:predictablePart}
  \uses{def:filtration}
  \mathlibok
  \lean{MeasureTheory.predictablePart}
Let $X : \mathbb{N} \to \Omega \to E$ be a process indexed by $\mathbb{N}$, for $E$ a Banach space.
Let $(\mathcal{F}_n)_{n\in\mathbb{N}}$ be a filtration on $\Omega$.
The predictable part of $X$ is the process $A : \mathbb{N} \to \Omega \to E$ defined by $A_0 = 0$ and for $n \ge 0$,
\begin{align*}
  A_{n+1}
  &= A_n + \mathbb{E}\left[ X_{n+1} - X_n | \mathcal{F}_n \right]
  \: .
\end{align*}
\end{definition}


\begin{definition}\label{def:martingalePart}
  \uses{def:predictablePart}
  \mathlibok
  \lean{MeasureTheory.martingalePart}
Let $X : \mathbb{N} \to \Omega \to E$ be a process indexed by $\mathbb{N}$, for $E$ a Banach space.
Let $(\mathcal{F}_n)_{n\in\mathbb{N}}$ be a filtration on $\Omega$ and let $A$ be the predictable part of $X$ for that filtration.
The martingale part of $X$ is the process $M : \mathbb{N} \to \Omega \to E$ defined by $M_n = X_n - A_n$.
\end{definition}


\begin{lemma}\label{lem:predictable_predictablePart}
  \uses{def:predictablePart, def:predictable, def:adapted}
The predictable part of an adapted process is a predictable process.
\end{lemma}

\begin{proof}
This is almost \texttt{MeasureTheory.adapted\_predictablePart} but we need to formulate it with the new predictable definition.
\end{proof}


\begin{lemma}\label{lem:martingale_martingalePart}
  \uses{def:martingalePart, def:Martingale}
  \mathlibok
  \lean{MeasureTheory.martingale_martingalePart}
Suppose that the filtration is sigma-finite.
Then the martingale part of an adapted process $X$ such that $X_n$ is integrable for all $n$ is a martingale.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:nondecreasing_predictablePart_of_submartingale}
  \uses{def:predictablePart, def:Submartingale}
The predictable part of a submartingale is a nondecreasing process.
\end{lemma}

\begin{proof}
  \uses{def:Submartingale}
Let $X$ be a submartingale and let $A$ be its predictable part. Then for all $n \geq 0$,
\begin{align*}
  A_{n+1} &= A_n + \mathbb{E}\left[ X_{n+1} - X_n | \mathcal{F}_n \right] \\
  &\ge A_n
  \: .
\end{align*}
The last inequality follows from the submartingale property of $X$.
Thus, $(A_n)_{n \in \mathbb{N}}$ is nondecreasing.
\end{proof}


\section{Cadlag modifications of (local) martingales}



\begin{definition}[Dyadics]\label{def:dyadics}
For $T>0$, let $\mathcal{D}_n^T = \left\lbrace \frac{k}{2^n}T \mid k=0,\cdots 2^n\right\rbrace$ be the set of dyadics at scale $n$ and let $\mathcal{D}^T=\bigcup_{n\in\mathbb{N}}\mathcal{D}_n^T$ be the set of all dyadics of $[0,T]$.
\end{definition}


\begin{lemma}\label{lem:martingale_exists_dyadic_limit_left}
  \uses{def:dyadics, def:Martingale}
  Let $X=(X_t)_{t\in\mathcal{D}}$ be a martingale indexed by the dyadics. Then almost surely, for every $t\geq 0$ the limit
  $$
  \lim_{\stackrel{s\rightarrow t^-}{s\in\mathcal{D}}}X_s(\omega)
  $$
  exists and is finite.
\end{lemma}

\begin{proof}
  See 8.2.1 of Pascucci.
\end{proof}


\begin{lemma}\label{lem:martingale_exists_dyadic_limit_right}
  \uses{def:dyadics, def:Martingale}
  Let $X=(X_t)_{t\in\mathcal{D}}$ be a martingale indexed by the dyadics. Then almost surely, for every $t\geq 0$ the limit
  $$
  \lim_{\stackrel{s\rightarrow t^+}{s\in\mathcal{D}}}X_s(\omega)
  $$
  exists and is finite.
\end{lemma}

\begin{proof}
  See 8.2.1 of Pascucci.
\end{proof}


\begin{lemma}\label{lem:mg_is_cadlag}
  \uses{def:usualConditions, def:Martingale}
  Let the filtered probability space satisfy the usual conditions.
  Then every martingale $X$ admits a modification that is still a martingale with cadlag trajectories.
\end{lemma}

\begin{proof}
  \uses{lem:martingale_exists_dyadic_limit_right,lem:martingale_exists_dyadic_limit_left}
  See 8.2.3 of Pascucci.
\end{proof}


\begin{lemma}\label{lem:exists_cadlag_mod_of_nonneg_submg}
  \uses{def:usualConditions, def:Submartingale}
  Let the filtered probability space satisfy the usual conditions.
  Then every nonnegative submartingale $X$ admits a modification that is still a nonnegative submartingale with cadlag trajectories.
\end{lemma}

\begin{proof}
  \uses{lem:martingale_exists_dyadic_limit_right,lem:martingale_exists_dyadic_limit_left}
  See 8.2.3 of Pascucci.
\end{proof}


\begin{lemma}\label{lem:exists_cadlag_mod_of_local_mg}
  \uses{def:usualConditions}
  Let the filtered probability space satisfy the usual conditions.
  Then every local martingale $X$ admits a modification that is still a local martingale with cadlag trajectories.
\end{lemma}

\begin{proof}
  \uses{lem:mg_is_cadlag}
\end{proof}



\section{Komlòs Lemma}



Firstly we will need Komlos' Lemma.
%technically a more general version with Cesaro sums exists, but it is not needed for this case
%(see "J. Komlòs, A generalization of a problem of Steinhaus, Acta Math. Acad. Sci. Hungar. 18 (1967) 217–229").


\begin{lemma}\label{lem:komlos_aux}
  Let $H$ be a Hilbert space and $(f_n)_{n\in\mathbb{N}}$ a bounded sequence in $H$. Then there exist functions $g_n\in convex(f_n,f_{n+1},\cdots)$ such that $(g_n)_{n\in\mathbb{N}}$ converges in $H$.
\end{lemma}

\begin{proof}
  Let $r_n = \inf(\|g\|_2:g\in convex(f_n, f_{n+1},\ldots))$.
  Let $A=\sup_{n\geq1} r_n$. $A$ is finite by boundedness of $(f_n)_{n\in\mathbb{N}}$ and
  for each $n$ we  may pick some $g_n\in convex(f_n, f_{n+1},\ldots)$ such that $ \|g_n\|_2\leq A+1/n$ by $\inf$ and $\sup$ definitions.
  Let $\epsilon>0$.
  By construction $(r_n)_{n\in\mathbb{N}}$ is increasing. By properties of $\sup$ there exists $\bar{n}$ such that $r_{\bar{n}}\geq A-\epsilon$ and such that $\frac{1}{\bar{n}}\leq\epsilon$.
  Let $m\geq k\geq \bar{n}$. $(g_k+g_m)/2 \in convex(f_k,f_{k+1},\ldots)$. It follows since $(r_n)_{n\in\mathbb{N}}$ is increasing that
  $\|(g_k+g_m)/2\|_2\geq A-\epsilon$.
  Hence due to the ordering of $m,k,\bar{n}$
  $$ \|g_k-g_m\|_2^2=2 \|g_k\|_2^2+2\|g_m\|_2^2- \|g_k+g_m\|_2^2
  \leq 4(A+\frac{1}{\bar{n}})^2-4(A-\epsilon)^2\leq 16A\epsilon.$$ By completeness, $(g_n)_{n\geq1}$  converges in $\|.\|_2$.
\end{proof}


\begin{lemma}\label{lem:convex_of_converg_seq_is_converg}
  Let $X$ be a normed vector space (over $\mathbb{R}$).
  %For topological spaces we need that a convex combinations of elements of neighborhoods are still in the neighborhood (just like balls). Also for metric space we want that $d(ax,ay)\leq a d(x,y)$ (in lean dist_pair_smul).
  Let $(x_n)_{n\in\mathbb{N}}$ be a sequence in $X$ converging to $x$ w.r.t. the topology of $X$.
  Let $(N_n)_{n\in\mathbb{N}}$ be a sequence in $\mathbb{N}$ such that $n\leq N_n$ for every $n\in\mathbb{N}$ (maybe here we could have $N_n$ increasing WLOG).
  Let $(a_{n,m})_{n\in\mathbb{N},m\in\left\lbrace n,\cdots,N_n\right\rbrace}$ be a triangular array in $\mathbb{R}$ such that $0\leq a_{n,m}\leq 1$ and $\sum_{m=n}^{N_n}a_{n,m}=1$.
  Then $(\sum_{m=n}^{N_n}a_{n,m}x_m)_{n\in\mathbb{N}}$ converges to $x$ uniformly w.r.t. the triangular array.
\end{lemma}

\begin{proof}
  Let $\epsilon>0$.
  By convergence of $x_n$ we have $\exists \bar{n}$ such that $\forall n\geq\bar{n}$ $|x_n-x|\leq \epsilon$.
  By triangular inequality it follows that
  $$
  |\sum_{m=n}^{N_n}a_{n,m}x_m - x|\leq \sum_{m=n}^{N_n}a_{n,m}|x_m-x|\leq\epsilon.
  $$
\end{proof}


\begin{lemma}\label{lem:komlos_convex_aux}
  For $i,n\in\mathbb{N}$ set $f_{n}^{(i)}:=f_n \mathbb{1}_{(|f_n|\leq i)}$ such that $f_{n}^{(i)}\in L^2$.
  There exists the sequence of convex weights $\lambda_n^{n}, \ldots, \lambda_{N_n}^{n}$ such that the functions
  $ (\lambda_n^{n} f_n^{(i)} + \ldots+\lambda_{N_n}^{n} f_{N_n}^{(i)})_{n\in\mathbb{N}}$
  converge in $L^2$ for every $i\in\mathbb{N}$ uniformly.
\end{lemma}

\begin{proof}
  \uses{lem:komlos_aux, lem:convex_of_converg_seq_is_converg}
  Firstly by lemma \ref{lem:komlos_aux} over $(f_n^{(1)})_{n\in\mathbb{N}}$ there exist convex weights $\prescript{1}{}{\lambda}^n_n,\cdots,\prescript{1}{}{\lambda}^n_{N^1_n}$ such that
  $g^1_n=\sum_{m=n}^{N^1_n}\prescript{1}{}{\lambda}^n_mf_m^{(1)}$ converges to some $g^1$.
  Secondly apply the lemma to $(\tilde{g}^2_n=\sum_{m=n}^{N^1_n}\prescript{1}{}{\lambda}^n_mf^{(2)}_m)_{n\in\mathbb{N}}$, there exists convex weights $\tilde{\lambda}^n_n,\cdots,\tilde{\lambda}^n_{\tilde{N}_n}$ such that
  $g^2_n=\sum_{m=n}^{\tilde{N}_n}\tilde{\lambda}^n_m\tilde{g}_m^{(2)}=\sum_{m=n}^{N^2_n}\prescript{2}{}{\lambda}^n_mf_m^{(2)}$ converges to some $g^2$.
  Notice that $\sum_{m=n}^{N^2_n}\prescript{2}{}{\lambda}^n_mf_m^{(1)}=\sum_{m=n}^{\tilde{N}_n}\tilde{\lambda}^n_m\tilde{g}_m^{(1)}$ and thus this sequence by lemma \ref{lem:convex_of_converg_seq_is_converg} converges still to $g^1$.
  By iteration we may define $\prescript{i}{}{\lambda}^n_n,\cdots,\prescript{i}{}{\lambda}^n_{N^i_n}$ convex weights such that if used on $(f^j_n)_{n\in\mathbb{N}}$ they make the sequence convergent if $1\leq j\leq i$.
  At this point consider $\lambda^n_m=\prescript{n}{}{\lambda}^n_m$.
  Since $\forall m\geq i$ $\sum_{j=n}^{N^m_n}\prescript{m}{}{\lambda}^n_j f^{(i)}_j\rightarrow g^i$ and even better
  $\forall\epsilon>0$ $\exists\bar{n}$, $\forall n\geq\bar{n}$, $\forall m\geq i$ $|\sum_{j=n}^{N^m_n}\prescript{m}{}{\lambda}^n_j f^{(i)}_j - g^i|\leq\epsilon$
  (this works by lemma \ref{lem:convex_of_converg_seq_is_converg} uniformity of convergence w.r.t. triangular array) this concludes.
\end{proof}


\begin{lemma}[Komlòs Lemma]\label{lem:komlos}
  Let $( f_n)_{n\in\mathbb{N}}$ be a uniformly integrable sequence of functions on a probability space $(\Omega , \mathcal{F} , P)$.
  Then there exist functions $g_n \in convex( f_n, f_{n+1}, \cdots)$ such that $(g_n)_{n\in\mathbb{N}}$ converges in  $L^1 (\Omega )$.
\end{lemma}

\begin{proof}
  \uses{lem:komlos_convex_aux}
  For $i,n\in\mathbb{N}$ set $f_{n}^{(i)}:=f_n \mathbb{1}_{(|f_n|\leq i)}$ such that $f_{n}^{(i)}\in L^2$.
  Using \ref{lem:komlos_convex_aux} there exist for every $n$ convex weights $\lambda_n^{n}, \ldots, \lambda_{N_n}^{n}$ such that the functions
  $ \lambda_n^{n} f_n^{(i)} + \ldots+\lambda_{N_n}^{n} f_{N_n}^{(i)}$ converge in $L^2$ for every $i\in\mathbb{N}$.
  By uniform integrability, $\lim_{i\to \infty}\| f^{(i)}_n- f_n\|_1=0$, uniformly with respect to $n$.
  Hence, once again, uniformly with respect to $n$,
  $$ \textstyle\lim_{i\to\infty}\|  (\lambda_n^{n} f_n^{(i)} + \ldots+\lambda_{N_n}^{n} f_{N_n}^{(i)})-(\lambda_n^{n} f_n + \ldots+\lambda_{N_n}^{n} f_{N_n})\|_1= 0.$$
  Thus $(\lambda_n^{n} f_n + \ldots+\lambda_{N_n}^{n} f_{N_n})_{n\geq 1}$  is a Cauchy sequence in $L^1$.
\end{proof}



\section{Doob-Meyer decomposition}



For uniqueness of Doob-Meyer Decomposition we will need theorem \ref{thm:IsLocalMartingale.eq_zero_of_finiteVariation}.

We now start the construction for the existence part.
Let $T>0$ and recall that $\mathcal{D}_n^T=\left\lbrace \frac{k}{2^n}T \mid k=0,\cdots 2^n\right\rbrace$.

TODO: everywhere below, $S$ is a cadlag submartingale of class D on $[0,T]$?


\begin{definition}\label{def:Doob_Meyer_class}
  \uses{def:IsStoppingTime}
$D$ is the class of all adapted processes $(S_t)_{0\leq t\leq T}$ such that the set $\{S_\tau \mid \tau \text{ is a stopping time}\}$ is uniformly integrable.
\end{definition}


\begin{definition}[A]\label{def:A}
  \uses{def:dyadics, def:predictablePart}
Define $A_0=0$ and for $t\in\mathcal{D}_n^T$ positive,
\begin{align*}
A^n_t
&=A^n_{t-T2^{-n}} + \mathbb{E}\left[ S_t-S_{t-T2^{-n}}|\mathcal{F}_{t-T2^{-n}}\right]
\: .
\end{align*}
\end{definition}


\begin{definition}[M]\label{def:M}
  \uses{def:A, def:martingalePart}
For $t\in\mathcal{D}_n^T$, define $M^n_t = S_t-A^n_t$~.
\end{definition}


\begin{lemma}\label{lem:Doob_Meyer_Finite_Predictable}
  \uses{def:A, def:predictable}
  $(A^n_t)_{t\in\mathcal{D}_n^T}$ is a predictable process.
\end{lemma}

\begin{proof}
  \uses{lem:predictable_nat_iff, lem:predictable_predictablePart}
  Trivial
\end{proof}


\begin{lemma}\label{lem:Doob_Meyer_Finite_Martingale}
  \uses{def:M, def:Martingale}
  $(M^n_t)_{t\in\mathcal{D}_n^T}$ is a martingale.
\end{lemma}

\begin{proof}
  \uses{lem:martingale_martingalePart}
  Trivial
\end{proof}


\begin{lemma}\label{lem:Predict_Part_Increasing}
  \uses{def:A}
  $(A^n_t)_{t\in\mathcal{D}_n^T}$ is an increasing process.
\end{lemma}

\begin{proof}
  \uses{def:Submartingale, lem:nondecreasing_predictablePart_of_submartingale}
$S$ is a submartingale:
\begin{align*}
  A^n_{t+T2^{-n}} - A^n_t
  &= \mathbb{E}\left[ S_{t+T2^{-n}}-S_t|\mathcal{F}_t\right] \ge 0
  \: .
\end{align*}
\end{proof}


\begin{definition}[Hitting time for $A$]\label{def:hittingAGT}
  \uses{def:A}
Let $c>0$. Define the hitting time on $\mathcal{D}^T_n$
\begin{align*}
  \tau_n(c)
  &= \inf\{t \in \mathcal{D}^T_n \mid A^n_{t + 2^{-n}T} > c\} \wedge T
  \: .
\end{align*}
\end{definition}


\begin{lemma}\label{lem:IsStoppingTime_hittingAGT}
  \uses{def:IsStoppingTime,def:hittingAGT}
  $\tau_n(c)$ is a stopping time.
\end{lemma}

\begin{proof}
Since $A^n_{t}$ is predictable, $A^n_{t + 2^{-n}T}$ is adapted.
The hitting time of an adapted process is a stopping time (we use the discrete time version of that result here, not the full Début theorem).
\end{proof}


\begin{lemma}\label{lem:A_hittingAGT_le}
  \uses{def:hittingAGT}
$A^n_{\tau_n(c)} \le c$ and if $\tau_n(c) < T$ then $A^n_{\tau_n(c)+T2^{-n}} > c$.
\end{lemma}

\begin{proof}

\end{proof}


\begin{lemma}\label{lem:A_hittingAGT_sub_ge}
  \uses{def:hittingAGT}
Let $a, b > 0$ with $a \le b$. If $\tau_n(b) < T$ then $A^n_{\tau_n(b)+T2^{-n}} - A^n_{\tau_n(a)} \ge b - a$.
\end{lemma}

\begin{proof}
  \uses{lem:A_hittingAGT_le}

\end{proof}


\begin{lemma}\label{lem:A_uniform_integrable}
  \uses{def:A}
  The sequence $(A^n_T)_{n\in\mathbb{N}}$ is uniformly integrable (bounded in $L^1$ norm).
\end{lemma}

\begin{proof}
  \uses{lem:Doob_Meyer_Finite_Predictable,lem:Predict_Part_Increasing,lem:Doob_Meyer_Finite_Martingale,lem:IsStoppingTime_hittingAGT,lem:A_hittingAGT_sub_ge}
  WLOG $S_T=0$ and $S_t\leq 0$ (else consider $S_t-\mathbb{E}\left[S_T\vert\mathcal{F}_{t}\right]$).

  We have that $0=S_T=M^n_T+A^n_T$. Thus
  \begin{equation}\label{equation_DM_e1}
  M^n_T=-A^n_T.
  \end{equation}
  Since $M^n$ is a martingale it follows by optional sampling that for any $(\mathcal{F}_t)_{t\in\mathcal{D}_n}$ stopping time $\tau$
  \begin{equation}\label{equation_DM_e2}
  S_\tau=M^n_\tau+A^n_\tau = \mathbb{E}[M^n_T\vert\mathcal{F}_\tau]+A^n_\tau\stackrel{\eqref{equation_DM_e1}}{=} -\mathbb{E}[A^n_T\vert\mathcal{F}_\tau]+A^n_\tau.
  \end{equation}
  Let $c>0$. By Lemma~\ref{lem:IsStoppingTime_hittingAGT}, $\tau_n(c)$ (Definition~\ref{def:hittingAGT}) is a stopping time.
  % Define the last time when $A^n$ has always been inside $[0,c]$, by the Début Theorem \ref{thm:hitting_is_stopping_time} and the fact that $A^n$ is predictable the following is a stopping time
  % $$
  % \tau_n(c)=\inf\left(\frac{j-1}{2^n}T\vert\, A^n_{jT2^{-n}}>c\right)\wedge T.
  % $$
  By construction $A^n_{\tau_n(c)}\leq c$. It follows that
  \begin{equation}\label{equation_DM_e3}
  S_{\tau_n(c)}\stackrel{\eqref{equation_DM_e2}}{=}-\mathbb{E}[A^n_T\vert\mathcal{F}_{\tau_n(c)}]+A^n_{\tau_n(c)}\leq -\mathbb{E}[A^n_T\vert\mathcal{F}_{\tau_n(c)}]+c.
  \end{equation}
  Since $(A^n_T>c)=(\tau_n(c)<T)$ we have
  \begin{align}\nonumber
  \int_{(A^n_T>c)}A^n_TdP&=\int_{(\tau_n(c)<T)}A^n_TdP\stackrel{\mathrm{Tower}}{=}\int_{(\tau_n(c)<T)}\mathbb{E}[A^n_T\vert\mathcal{F}_{\tau_n(c)}]dP\\
  &\stackrel{\eqref{equation_DM_e3}}{\leq} cP(\tau_n(c)<T)-\int_{\tau_n(c)<T}S_{\tau_n(c)}dP.\label{equation_DM_e4}
  \end{align}
  Now we notice that $(\tau_n(c)<T)\subseteq (\tau_n(c/2)<T)$, thus
  \begin{align}\nonumber
  \int_{\tau_n(c/2)<T}-S_{\tau_n(c/2)}dP
  &\stackrel{\eqref{equation_DM_e2}}{=}\int_{(\tau_n(c/2))<T}\mathbb{E}[A^n_T\vert\mathcal{F}_{\tau_n(c/2)}]-A^n_{\tau_n(c/2)}dP \nonumber
  \\
  &\stackrel{\mathrm{Tower}}{=}\int_{(\tau_n(c/2)<T)}A^n_t-A^n_{\tau_n(c/2)}dP\nonumber
  \\
  &\geq \int_{(\tau_n(c)<T)}A^n_t-A^n_{\tau_n(c/2)}dP\nonumber
  \\
  \intertext{(over the event $(\tau_n(c)<T)$ $A^n_T\geq c$ and $A^n_{\tau_n(c/2)}\leq c/2$, thus $A^n_T-A^n_{\tau_n(c/2)}\geq c/2$)}
  &\geq \frac{c}{2}P(\tau_n(c)<T).\label{equation_DM_e5}
  \end{align}
  It follows
  $$
  \int_{(A^n_T>c)}A^n_TdP\stackrel{\eqref{equation_DM_e4}}{\leq}cP(\tau_n(c)<T)-\int_{\tau_n(c)<T}S_{\tau_n(c)}dP\stackrel{\eqref{equation_DM_e5}}{\leq}-2\int_{\tau_n(c/2)<T}S_{\tau_n(c/2)}dP-\int_{\tau_n(c)<T}S_{\tau_n(c)}dP.
  $$
  We may notice that
  $$
  P(\tau_n(c)<T)=P(A^n_T>c)\stackrel{Markov}{\leq}\frac{\mathbb{E}[A^n_T]}{c}=-\frac{\mathbb{E}[M^n_T]}{c}\stackrel{mg}{=}-\frac{\mathbb{E}[S_0]}{c}
  $$
  which goes to $0$ uniformly in $n$ as $c$ goes to infinity.
  This implies that $\int_{(A^n_T>c)}A^n_TdP$ is uniformly bounded in $n$ due to the fact that $S$ is of class $D$. And so also the $L^1$ norm is uniformly bounded.
\end{proof}


\begin{lemma}\label{lem:M_uniform_integrable}
  The sequence $(M^n_T)_{n\in\mathbb{N}}$ is uniformly integrable (bounded in $L^1$ norm).
\end{lemma}

\begin{proof}
  \uses{lem:A_uniform_integrable}
  $M^n_T=S_T-A^n_T$, also $S$ is of class $D$ and $A^n_T$ is uniformly integrable.
\end{proof}


\begin{lemma}\label{lem:incr_fun_lim_right_cont_limsup_ineq}
  If $f_n, f : [0, 1] \rightarrow \mathbb{R}$ are increasing functions such that $f$ is right continuous and
  $\lim_n f_n(t) = f (t)$ for $t \in\mathcal{D}^T$, then  $\limsup_n  f_n(t) \leq f (t)$ for all $t \in [0, T]$.
\end{lemma}

\begin{proof}
  Let $t\in[0,T]$ and $s\in\mathcal{D}^T$ such that $t<s$. We have
  $$
  \limsup_n f_n(t)\leq \limsup_n f_n(s)=f(s).
  $$
  Since the above is true uniformly in $s$ in particular since $f$ is right-continuous
  $$
  \limsup_n f_n(t)\leq\lim_{\stackrel{s\rightarrow t^+}{s\in\mathcal{D}^T}}f(s)=f(t).
  $$
\end{proof}


\begin{lemma}\label{lem:incr_fun_lim_right_cont_lim_eq}
  If $f_n, f : [0, 1] \rightarrow \mathbb{R}$ are increasing functions such that $f$ is right continuous and
  $\lim_n f_n(t) = f (t)$ for $t \in\mathcal{D^T}$, if $f$ is continuous in $t\in[0,T]$ then $\lim_n  f_n(t) = f (t)$.
\end{lemma}

\begin{proof}
  \uses{lem:incr_fun_lim_right_cont_limsup_ineq}
  By lemma \ref{lem:incr_fun_lim_right_cont_limsup_ineq} it is enough to show that $\liminf_n f_n(t)\geq f(t)$.
  Let $s\in\mathcal{D}^T$ such that $t>s$. We have
  $$
  \liminf_n f_n(t)\geq \liminf_n f_n(s)=f(s).
  $$
  Since the above is true uniformly in $s$ in particular since $f$ is continuous in $t$
  $$
  \liminf_n f_n(t)\geq\lim_{\stackrel{s\rightarrow t^-}{s\in\mathcal{D}^T}}f(s)=f(t).
$$
\end{proof}

Define $M^n_t$ on $[0,T]$ using $M^n_t=\mathbb{E}[M^n_T\vert\mathcal{F}_t]$.

\begin{lemma}\label{lem:M_n_cadlag_mg}
  $M^n_t$ admits a modification which is a cadlag martingale.
\end{lemma}

\begin{proof}
  \uses{lem:mg_is_cadlag}
  By theorem \ref{lem:mg_is_cadlag}
\end{proof}

From this point onwards $M^n_t$ will be redefined as the modification from lemma \ref{lem:M_n_cadlag_mg}.

\begin{lemma}\label{lem:M_cal_converges_L1}
  There are convex weights $\lambda^n_n,\cdots,\lambda^n_{N_n}$ such that
  $\mathcal{M}^n_T\stackrel{L^1}{\rightarrow}M$, where $\mathcal{M}^n:=\lambda^n_nM^n+\cdots +\lambda^n_{N_n}M^{N_n}.$
\end{lemma}
\begin{proof}
  \uses{lem:M_uniform_integrable,lem:komlos}
  By lemma \ref{lem:M_uniform_integrable} $(M^n_T)_{n\in\mathbb{N}}$ is uniformly bounded in $L^1$, thus by lemma \ref{lem:komlos} there are convex weights $\lambda^n_n,\cdots,\lambda^n_{N_n}$ such that
  $\mathcal{M}^n_T\stackrel{L^1}{\rightarrow}M$, where $\mathcal{M}^n:=\lambda^n_nM^n+\cdots +\lambda^n_{N_n}M^{N_n}.$
\end{proof}

\begin{lemma}\label{lem:M_cal_cadlag}
  $\mathcal{M}^n$ is cadlag.
\end{lemma}
\begin{proof}
  \uses{lem:M_n_cadlag_mg,lem:M_cal_converges_L1}
  By construction and \ref{lem:M_n_cadlag_mg}
\end{proof}

Let \begin{equation}\label{equation_DM_e6} M_t = \mathbb{E}[M\vert\mathcal{F}_t].\end{equation}

\begin{lemma}\label{lem:M_cadlag_mg}
  $M_t$ admits a martingale cadlag modification.
\end{lemma}
\begin{proof}
  \uses{lem:M_cal_converges_L1, lem:mg_is_cadlag}
  By construction $M_t$ is a martingale and thus by theorem \ref{lem:mg_is_cadlag} admits a cadlag martingale modification
  ($M_t$ is a version of $\mathbb{E}[M\vert\mathcal{F}_t]$ and thus passing to modification does not pose any problem).
\end{proof}

From this point onwards $M^n_t$ will be redefined as the modification from lemma \ref{lem:M_cadlag_mg}.
Define
\begin{itemize}
  \item Extend now $A^n$ as a left continuous process $A^n_s:=\sum_{t\in\mathcal{D}^T_n}A^n_t\mathbb{1}_{]t-2^{-n},t]}(s)$
  \item $\mathcal{A}^n=\lambda^n_nA^n+\cdots +\lambda^n_{N_n}A^{N_n}$
  \item $A_t=S_t-M_t$
\end{itemize}

\begin{lemma}\label{lem:M1_komlos}
  For every $t\in[0,T]$ we have $\mathcal{M}^n_t\stackrel{L^1}{\rightarrow}M_t$.
\end{lemma}
\begin{proof}
  \uses{lem:M_cal_converges_L1}
  We may notice that by Jensen's inequality, the tower lemma and lemma \ref{lem:M_cal_converges_L1}
  \begin{gather}\nonumber
    \mathbb{E}[|\mathcal{M}^n_t-M_t|]=\mathbb{E}[|\mathbb{E}[\mathcal{M}^n_T-M\vert\mathcal{F}_t]|]\leq \mathbb{E}[|\mathcal{M}^n_T-M|]\rightarrow0,\\
    \Rightarrow\mathcal{M}^n_t\stackrel{L^1}{\rightarrow} M_t,\quad \forall t\in[0,T].\label{equation_DM_e7}
  \end{gather}
\end{proof}

\begin{lemma}\label{lem:A_cal_conv_A_on_D_T}
  There exists a set $E\subseteq\Omega$, $P(E)=0$ and a subsequence $k_n$ such that $\lim_n\mathcal{A}^{k_n}_t(\omega)=A_t(\omega)$ for every $t\in\mathcal{D}^T,\omega\in\Omega\setminus E$.
\end{lemma}
\begin{proof}
  \uses{lem:M1_komlos}
  By Lemma \ref{lem:M1_komlos}
  $$
  \mathcal{A}^n_t=S_t-\mathcal{M}^n_t\stackrel{L^1}{\rightarrow}S_t-M_t=A_t,\quad\forall t\in\mathcal{D}^T.
  $$
  $\mathcal{D}^T$ is countable we can arrange the elements as $(t_n)_{n\in\mathbb{N}}$.
  Given $t_0\in\mathcal{D}^T$ there exists a subsequence $k^{0}_n$ for which $\mathcal{A}^{k^{0}_n}_{t_0}$ converges to $A_{t_0}$ over the set $\Omega\setminus E_{0}$ where $P(E_{0})=0$.
  Suppose we have a sequence $k^m_n$ for which $\mathcal{A}^{k^j_n}_{t_j}$ converges to $A_{t_j}$ over the set $\Omega\setminus E_{m}$ where $P(E_{m})=0$ for each $j=0,\cdots,m$.
  From this subsequence we may extract a new subsequence $k^{m+1}_n$ for which $\mathcal{A}^{k^{m+1}_n}_{t_{m+1}}$ converges to $A_{t_{m+1}}$ over the set $\Omega\setminus E_{m+1}$ where $P(E_{m+1})=0$.
  By construction over this subsequence the convergence for $t_0,\cdots,t_m$ still applies.
  With a diagonal argument we obtain the final result with $E=\bigcup_n E_n$.
\end{proof}

\begin{lemma}\label{lem:A_increasing}
  $(A_t)_{t\in[0,T]}$ is an increasing process.
\end{lemma}
\begin{proof}
  \uses{lem:A_cal_conv_A_on_D_T, lem:Predict_Part_Increasing, lem:M_cadlag_mg}
  Since $\mathcal{A}^n_t$ is increasing on $\mathcal{D}^T$ by lemma \ref{lem:A_cal_conv_A_on_D_T} also $A$ is almost surely increasing on $\mathcal{D}^T$.
  Since $S,M$ are cadlag also $A$ is cadlag (thus right-continuous). It follows that $A$ must be increasing on $[0,T]$.
\end{proof}

\begin{lemma}\label{lem:lim_Exp_A_n_tau_is_Exp_A_tau}
  Let $\tau$ be an $(\mathcal{F}_t)_{t\in[0,T]}$ stopping time. We have $\lim_n\mathbb{E}[A^n_\tau]=\mathbb{E}[A_\tau]$.
\end{lemma}
\begin{proof}
  \uses{lem:M_cadlag_mg, lem:M_n_cadlag_mg}
  Let $\sigma_n:=\inf\left(t\in\mathcal{D}^T_n\vert t>\tau\right)$. By construction of $A^n$ we have $A^n_\tau=A^n_{\sigma_n}$.
  Also $\sigma_n\searrow\tau$. Since $S$ is of class $D$ and cadlag we have
  \begin{align*}
    \mathbb{E}[A^n_\tau]&=\mathbb{E}[A^n_{\sigma_n}]=\mathbb{E}[S_{\sigma_n}]-\mathbb{E}[M^n_{\sigma_n}]=\mathbb{E}[S_{\sigma_n}]-\mathbb{E}[M^n_0]=\\
    &=\mathbb{E}[S_{\sigma_n}]-\mathbb{E}[S_0]\rightarrow \mathbb{E}[S_\tau]-\mathbb{E}[M_0]=\mathbb{E}[S_\tau]-\mathbb{E}[M_\tau]=\mathbb{E}[A_\tau].
  \end{align*}
\end{proof}

\begin{lemma}\label{lem:limsup_A_n_tau_is_A_tau_ae}
  Let $\tau$ be an $(\mathcal{F}_t)_{t\in[0,T]}$ stopping time. We have $\limsup_n \mathcal{A}_\tau^n = A_\tau$.
\end{lemma}
\begin{proof}
  \uses{lem:lim_Exp_A_n_tau_is_Exp_A_tau, lem:incr_fun_lim_right_cont_limsup_ineq, lem:Predict_Part_Increasing, lem:A_cal_conv_A_on_D_T}
  Firstly we notice that $\liminf_n \mathbb{E}[A_\tau^n]  \leq \limsup_n  \mathbb{E}  [\mathcal{A}_\tau^n  ]  \leq \mathbb{E}[\limsup_n  \mathcal{A}_\tau^n  ]  \leq \mathbb{E}[ A_\tau ]$,
  where the first inequality is justified by the definition of limsup and liminf and the fact that
  $$
  \sup_{k\geq n}\mathbb{E}[\mathcal{A}^k_\tau]\geq \sum_{m=k}^{N_k}\lambda^k_m\mathbb{E}[A^m_\tau]\geq \sum_{m=k}^{N_k}\lambda^k_m\inf_{j\geq n}\mathbb{E}[A^j_\tau]=\inf_{k\geq n}\mathbb{E}[A^k_\tau]
  $$
  the third inequality by \ref{lem:incr_fun_lim_right_cont_limsup_ineq}.
  Let's prove the second inequality: observe that
  $$
  \mathcal{A}^n_\tau= A_1+\mathcal{A}^n_\tau-A_1\leq A_1+(\mathcal{A}^n_\tau-A_1)_+,
  $$
  thus it follows that $\mathcal{A}^n_\tau - (\mathcal{A}^n_\tau-A_1)_+\leq A_1$; since $A_1$ is an integrable guardian the inverse Fatou Lemma may be applied to show together with limsup properties that
  \begin{align*}
    \limsup_n\mathbb{E}[\mathcal{A}^n_\tau]+0 &= \limsup_n\mathbb{E}[\mathcal{A}^n_\tau]+\liminf_n-\mathbb{E}[(\mathcal{A}^n_\tau-A_1)_+] \leq \limsup_n\mathbb{E}[\mathcal{A}^n_\tau-(\mathcal{A}^n_\tau-A_1)_+]\leq\\
    &\leq \mathbb{E}[\limsup_n\mathcal{A}^n_\tau-(\mathcal{A}^n_\tau-A_1)_+]\leq \mathbb{E}[\limsup_n\mathcal{A}^n_\tau]-\mathbb{E}[\liminf_n(\mathcal{A}^n_\tau-A_1)_+]\leq\mathbb{E}[\limsup_n\mathcal{A}^n_\tau],
    \end{align*}
  where the first equality is justified by the fact that $\mathcal{A}^n_\tau\leq\mathcal{A}^n_1\rightarrow A_1$ almost surely.
  Due to lemma \ref{lem:lim_Exp_A_n_tau_is_Exp_A_tau} and \ref{lem:incr_fun_lim_right_cont_limsup_ineq} the first sequence of inequalities is a sequence of equalities, thus
  we know that $A_\tau- \limsup_n \mathcal{A}_\tau^n $ is an a.s. nonnegative function with null expected value, and thus it must be almost everywhere null.
\end{proof}

\begin{theorem}\label{thm:Doob_Meyer}
  Let $S = (S_t )_{0\leq t\leq T}$ be a cadlag submartingale of class $D$.
  Then, $S$ can be written in a unique way in the form  $S = M + A$ where $M$ is a cadlag martingale and $A$ is a predictable increasing process starting at $0$.
\end{theorem}
\begin{proof}
  \uses{lem:A_increasing, lem:M_cadlag_mg, lem:limsup_A_n_tau_is_A_tau_ae, lem:incr_fun_lim_right_cont_lim_eq}
  By construction $M$ is a cadlag martingale and $A_0=0$ and by lemma \ref{lem:A_increasing} $A$ is increasing. It suffices to show that $A$ is predictable.
  $A^n,\mathcal{A}^n$ are left continuous and adapted, and thus they are predictable (measurable wrt the predictable sigma algebra (the one generated by left-cont adapted processes)).
  It is enough to show that $\omega-a.e.$, $\forall t\in[0,T]$, $\limsup_n\mathcal{A}^n_t(\omega)=A_t(\omega)$.

  By lemma \ref{lem:incr_fun_lim_right_cont_lim_eq} that is true for any continuity point of $A$. Since $A$ is increasing it can only have a finite amount of jumps larger than $1/k$ for any $k\in\mathbb{N}$.
  Consider now $\tau_{q,k}$ the family of stopping times equal to the $q$-th time that the process $A_t$ has a jump higher than $1/k$. This is a countable family.
  Given a time $t$ and a trajectory $\omega$ there are only two possibilities: either $A$ is continuous or not at time $t$ along $\omega$.
  If $A$ is continuous at time $t$ we have $\limsup_n\mathcal{A}^n_t(\omega)=A_t(\omega)$, if it jumps there exists $q(\omega),k(\omega)$ such that $t=\tau_{q(\omega),k(\omega)}(\omega)$.
  Due to lemma \ref{lem:limsup_A_n_tau_is_A_tau_ae} we know that $\limsup_n A^n_{\tau_{q,k}} = A_{\tau_{q,k}}$ for each $q,k$ almost surely. Thus, since it is an intersection of a countable amount of almost sure
  events $\forall\omega\in\Omega'$ with $P(\Omega')=1$, for each $q,k$ $\limsup_n A^n_{\tau_{q,k}}(\omega) = A_{\tau_{q,k}}(\omega)$ ($\omega$ does not depend upon $q,k$).
  Consequently, $\forall\omega\in\Omega'$ we have $\limsup_n\mathcal{A}^n_t(\omega)=\limsup_n\mathcal{A}^n_{\tau_{q(\omega),k(\omega)}}(\omega)=A_{\tau_{q(\omega),k(\omega)}}(\omega)=A_t(\omega)$
\end{proof}



\section{Local version of the Doob-Meyer decomposition}



\begin{lemma}\label{lem:IsLocalSubmartingale.local_doobMeyerClass}
  \uses{def:IsLocalSubmartingale, def:Doob_Meyer_class}
Every local submartingale $X$ with $X_0 = 0$ is locally of class D.
\end{lemma}

\begin{proof}
  \uses{lem:local_induction}
By Lemma~\ref{lem:local_induction}, it suffices to show that if $X$ is a submartingale with $X_0=0$ then it is locally of class D.

TODO
\end{proof}


\begin{theorem}[Doob-Meyer decomposition]\label{thm:local_doobMeyer}
  \uses{def:IsLocalSubmartingale, def:predictable, def:IsLocalMartingale}
An adapted process $X$ is a cadlag local submartingale iff $X = M + A$ where $M$ is a cadlag local martingale and $A$ is a predictable, cadlag, locally integrable and increasing process starting at $0$.
The processes $M$ and $A$ are uniquely determined by $X$ a.s.
\end{theorem}

\begin{proof}
  \uses{lem:IsLocalSubmartingale.local_doobMeyerClass, thm:Doob_Meyer}

\end{proof}
