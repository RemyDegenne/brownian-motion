\chapter{Gaussian distributions}
\label{chap:gaussian}

\section{Gaussian measures}
\label{sec:gaussian_measures}

\subsection{Real Gaussian measures}

\begin{definition}[Real Gaussian measure]\label{def:gaussianReal}
  \mathlibok
  \lean{ProbabilityTheory.gaussianReal}
  The real Gaussian measure with mean $\mu \in \mathbb{R}$ and variance $\sigma^2 > 0$ is the measure on $\mathbb{R}$ with density $\frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right)$ with respect to the Lebesgue measure.
  The real Gaussian measure with mean $\mu \in \mathbb{R}$ and variance $0$ is the Dirac measure $\delta_\mu$.
  We denote this measure by $\mathcal{N}(\mu, \sigma^2)$.
\end{definition}


\begin{lemma}\label{lem:charFun_gaussianReal}
  \uses{def:gaussianReal, def:charFun}
  \mathlibok
  \lean{ProbabilityTheory.charFun_gaussianReal}
The characteristic function of a real Gaussian measure with mean $\mu$ and variance $\sigma^2$ is given by
$x \mapsto \exp\left(i \mu x - \frac{\sigma^2 x^2}{2}\right)$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:centralMoment_two_mul_gaussianReal}
  \uses{def:gaussianReal}
  \leanok
  \lean{ProbabilityTheory.centralMoment_two_mul_gaussianReal}
The central moment of order $2n$ of a real Gaussian measure $\mathcal{N}(\mu, \sigma^2)$ is given by
\begin{align*}
  \mathbb{E}[(X - \mu)^{2n}] = \sigma^{2n} (2n - 1)!! \: ,
\end{align*}
in which $(2n - 1)!! = (2n - 1)(2n - 3) \cdots 3 \cdot 1$ is the double factorial of $2n - 1$.
\end{lemma}

\begin{proof}\leanok
\begin{align*}
	\mathbb{E}[(X - \mu)^{2n}] &= \int_{-\infty}^\infty (x - \mu)^{2n} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}} \mathrm dx \\
	&= \int_{-\infty}^\infty x^{2n} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{x^2}{2 \sigma^2}} \mathrm dx \\
	&= 2 \int_{0}^\infty x^{2n} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{x^2}{2 \sigma^2}} \mathrm dx \\
	&= 2 \int_{0}^\infty {\sqrt{2 \sigma^2 x}}^{2n} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-x)} \frac{\sigma^2}{\sqrt{2 \sigma^2 x'}} \mathrm dx \\
	&= \frac{\sigma^{2n} 2^n}{\sqrt{\pi}} \int_{0}^\infty x^{n - 1/2} e{-x} \mathrm dx \\
	&= \frac{\sigma^{2n} 2^n}{\sqrt{\pi}} \Gamma(n + 1/2) \\
	&= \frac{\sigma^{2n} 2^n}{\Gamma(1/2)} \left( \prod_{k=0}^{n-1} (k + 1/2) \right) \Gamma(1/2) \\
	&= \sigma^{2n} \prod_{k=0}^{n-1} (2k + 1) \\
	&= \sigma^{2n} (2n - 1)!!
\end{align*}
\end{proof}


\subsection{Gaussian measures on a Banach space}

That kind of generality is not needed for this project, but we happen to have results about Gaussian measures on a Banach space in Mathlib, so we will use them.
The main reference for this section is \cite{hairer2009introduction}.

Let $F$ be a separable Banach space.

\begin{definition}[Gaussian measure]\label{def:IsGaussian}
  \uses{def:gaussianReal}
  \mathlibok
  \lean{ProbabilityTheory.IsGaussian}
A measure $\mu$ on $F$ is Gaussian if for every continuous linear form $L \in F^*$, the pushforward measure $L_* \mu$ is a Gaussian measure on $\mathbb{R}$.
\end{definition}


\begin{lemma}\label{lem:IsGaussian.IsProbabilityMeasure}
  \uses{def:IsGaussian}
  \mathlibok
A Gaussian measure is a probability measure.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{theorem}\label{thm:isGaussian_iff_charFunDual_eq}
  \uses{def:IsGaussian, def:charFunDual}
  \mathlibok
  \lean{ProbabilityTheory.isGaussian_iff_charFunDual_eq}
A finite measure $\mu$ on $F$ is Gaussian if and only if for every continuous linear form $L \in F^*$, the characteristic function of $\mu$ at $L$ is
\begin{align*}
  \hat{\mu}(L) = \exp\left(i \mu[L] - \mathbb{V}_\mu[L] / 2\right) \: ,
\end{align*}
in which $\mathbb{V}_\mu[L]$ is the variance of $L$ with respect to $\mu$.
\end{theorem}

\begin{proof}\uses{thm:ext_of_charFunDual, lem:charFun_gaussianReal}\leanok

\end{proof}



\paragraph{Transformations of Gaussian measures}

\begin{lemma}\label{lem:isGaussian_map}
  \uses{def:IsGaussian}
  \mathlibok
  \lean{ProbabilityTheory.isGaussian_map}
Let $F, G$ be two Banach spaces, let $\mu$ be a Gaussian measure on $F$ and let $T : F \to G$ be a continuous linear map.
Then $T_*\mu$ is a Gaussian measure on $G$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:isGaussian_add_const}
  \uses{def:IsGaussian}
  \leanok
  % This is an instance without name in the code, hence we don't give a \lean{...}.
Let $\mu$ be a Gaussian measure on $F$ and let $c \in F$.
Then the measure $\mu$ translated by $c$ (the map of $\mu$ by $x \mapsto x + c$) is a Gaussian measure on $F$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:isGaussian_conv}
  \uses{def:IsGaussian}
  \mathlibok
  %\lean{ProbabilityTheory.isGaussian_conv} -- need a Mathlib update
The convolution of two Gaussian measures is a Gaussian measure.
\end{lemma}

\begin{proof}\leanok

\end{proof}



\paragraph{Fernique's theorem}


\begin{theorem}\label{thm:exists_integrable_exp_sq_of_map_rotation_eq_self}
  \leanok
  % In a Mathlib PR
Let $\mu$ be a finite measure on $F$ such that $\mu \times \mu$ is invariant under the rotation of angle $-\frac{\pi}{4}$.
Then there exists $C > 0$ such that the function $x \mapsto \exp (C \Vert x \Vert ^ 2)$ is integrable with respect to $\mu$.
\end{theorem}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:IsGaussian.map_rotation_eq_self}
  \uses{def:IsGaussian}
  \leanok
  % In a Mathlib PR
For a Gaussian measure $\mu$, $\mu \times \mu$ is invariant by rotation.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:isGaussian_conv}

\end{proof}


\begin{theorem}[Fernique's theorem]\label{thm:IsGaussian.exists_integrable_exp_sq}
  \uses{def:IsGaussian}
  \leanok
  \lean{ProbabilityTheory.IsGaussian.exists_integrable_exp_sq}
For a Gaussian measure, there exists $C > 0$ such that the function $x \mapsto \exp (C \Vert x \Vert ^ 2)$ is integrable.
\end{theorem}

\begin{proof}\leanok
  \uses{thm:isGaussian_iff_charFunDual_eq, lem:IsGaussian.IsProbabilityMeasure, thm:exists_integrable_exp_sq_of_map_rotation_eq_self, lem:IsGaussian.map_rotation_eq_self}

\end{proof}


\begin{lemma}\label{lem:IsGaussian.memLp_id}
  \uses{def:IsGaussian}
  \leanok
  \lean{ProbabilityTheory.IsGaussian.memLp_id}
A Gaussian measure $\mu$ has finite moments of all orders.
In particular, there is a well defined mean $m_\mu := \mu[\mathrm{id}]$, and for all $L \in F^*$, $\mu[L] = L(m_\mu)$.
\end{lemma}

\begin{proof}\leanok
  \uses{thm:IsGaussian.exists_integrable_exp_sq}

\end{proof}

A Gaussian measure has finite second moment by Lemma~\ref{lem:IsGaussian.memLp_id}, hence its covariance bilinear form is well defined.


\subsection{Gaussian measures on a finite dimensional Hilbert space}

We specialize directly from Banach space to finite dimensional Hilbert space since that's what we need in this project, although there are results for Gaussian measures on infinite dimensional Hilbert spaces that would worth stating.

\begin{lemma}\label{lem:isGaussian_iff_charFun_eq}
  \uses{def:IsGaussian, def:charFunDual, def:charFun}
  \leanok
  \lean{ProbabilityTheory.isGaussian_iff_charFun_eq}
A finite measure $\mu$ on a Hilbert space $E$ is Gaussian if and only if for every $t \in E$, the characteristic function of $\mu$ at $t$ is
\begin{align*}
  \hat{\mu}(t) =  \exp\left(i \mu[\langle t, \cdot \rangle] - \mathbb{V}_\mu[\langle t, \cdot \rangle] / 2\right) \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  \uses{thm:isGaussian_iff_charFunDual_eq}
By Theorem~\ref{thm:isGaussian_iff_charFunDual_eq}, $\mu$ is Gaussian iff for every continuous linear form $L \in E^*$, the characteristic function of $\mu$ at $L$ is
\begin{align*}
  \hat{\mu}(L) = \exp\left(i \mu[L] - \mathbb{V}_\mu[L] / 2\right) \: .
\end{align*}
Every continuous linear form $L \in E^*$ can be written as $L(x) = \langle t, x \rangle$ for some $t \in E$, hence we have that $\mu$ is Gaussian iff for every $t \in E$,
\begin{align*}
  \hat{\mu}(t) = \exp\left(i \mu[\langle t, \cdot \rangle] - \mathbb{V}_\mu[\langle t, \cdot \rangle] / 2\right) \: .
\end{align*}
\end{proof}

Let $E$ be a separable Hilbert space. We denote by $\langle \cdot, \cdot \rangle$ the inner product on $E$ and by $\Vert \cdot \Vert$ the associated norm.

\begin{lemma}\label{lem:IsGaussian.charFun_eq}
  \uses{def:IsGaussian, def:charFun, def:covInnerBilin}
  \leanok
  \lean{ProbabilityTheory.IsGaussian.charFun_eq}
The characteristic function of a Gaussian measure $\mu$ on $E$ is given by
\begin{align*}
  \hat{\mu}(t) = \exp\left(i \langle t, m_\mu \rangle - \frac{1}{2} C'_\mu(t, t)\right) \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  \uses{lem:isGaussian_iff_charFun_eq, lem:IsGaussian.memLp_id, lem:covarianceBilin_same_eq_variance}
By Lemma~\ref{lem:isGaussian_iff_charFun_eq}, for every $t \in E$,
\begin{align*}
  \hat{\mu}(t) = \exp\left(i \mu[\langle t, \cdot \rangle] - \mathbb{V}_\mu[\langle t, \cdot \rangle] / 2\right) \: .
\end{align*}
By Lemma~\ref{lem:IsGaussian.memLp_id}, $\mu$ has finite first moment and $\mu[\langle t, \cdot \rangle] = \langle t, m_\mu \rangle$. By the same lemma, $\mu$ has finite second moment and for any $t$ we have $\mathbb{V}_\mu[\langle t, \cdot\rangle] = C'_\mu(t, t)$.
\end{proof}

\begin{lemma}\label{lem:isGaussian_iff_gaussian_charFun}
  \uses{def:IsGaussian, def:charFun, def:covMatrix}
  \leanok
  \lean{ProbabilityTheory.isGaussian_iff_gaussian_charFun, ProbabilityTheory.gaussian_charFun_congr}
A finite measure $\mu$ on $E$ is Gaussian if and only if there exists $m \in E$ and $C$ positive semidefinite such that for all $t \in E$, the characteristic function of $\mu$ at $t$ is
\begin{align*}
  \hat{\mu}(t) = \exp\left(i \langle t, m \rangle - \frac{1}{2} C(t, t)\right) \: ,
\end{align*}
If that's the case, then $m = m_\mu$ and $C = C'_\mu$.
\end{lemma}

Note that this lemma does not say that there exists a Gaussian measure for any such $m$ and $C$.
We will prove that later.

\begin{proof}\leanok
  \uses{lem:IsGaussian.charFun_eq, lem:charFun_map_eq_charFunDual_smul, thm:ext_of_charFun}
Lemma~\ref{lem:IsGaussian.charFun_eq} states that the characteristic function of a Gaussian measure has the wanted form.

Suppose now that there exists $m \in E$ and $C$ positive semidefinite such that for all $t \in E$, $\hat{\mu}(t) = \exp\left(i \langle t, m \rangle - \frac{1}{2} C(t, t)\right)$.

We need to show that for all $L \in E^*$, $L_*\mu$ is a Gaussian measure on $\mathbb{R}$.
Such an $L$ can be written as $\langle u, \cdot \rangle$ for some $u \in E$.
Let then $u \in E$. We compute the characteristic function of $\langle u, \cdot\rangle_*\mu$ at $x \in \mathbb{R}$ with Lemma~\ref{lem:charFun_map_eq_charFunDual_smul}:
\begin{align*}
  \widehat{\langle u, \cdot\rangle_*\mu}(x)
  &= \hat{\mu}(x \cdot u)
  \\
  &= \exp\left(i x \langle u, m \rangle - \frac{1}{2} x^2 C(u, u)\right)
  \: .
\end{align*}
This is the characteristic function of a Gaussian measure on $\mathbb{R}$ with mean $\langle u, m \rangle$ and variance $C(u, u)$.
By Theorem~\ref{thm:ext_of_charFun}, $\langle u, \cdot\rangle_*\mu$ is Gaussian, hence $\mu$ is Gaussian.

By Lemma~\ref{lem:IsGaussian.charFun_eq}, we deduce that for any $t \in E$ we have
$$\exp\left(i\langle t, m \rangle - \frac{1}{2} C(t, t)\right) = \exp\left(i\langle t, m_\mu \rangle - \frac{1}{2} C'_\mu(t, t)\right).$$
In particular, for any $t$ there exists $n_t \in \mathbb{Z}$ such that
$$i\langle t, m \rangle - \frac{1}{2} C(t, t) = i\langle t, m_\mu \rangle - \frac{1}{2} C'_\mu(t, t) + 2i\pi n_t.$$
We deduce that $n$ is a continuous map from $E$ to $\mathbb{Z}$, and thus must be constant because $E$ is connected. By looking at the value at $t = 0$, we deduce that for any $t$, $n_t = 0$. Looking at real and imaginary parts we obtain that for any $t$,
$$\langle t, m \rangle = \langle t, m_\mu \rangle \quad \text{and} \quad C(t, t) = C'_\mu(t, t).$$
We immediately deduce that $m = m_\mu$. Moreover, because $C$ and $C'_\mu$ are symmetric, they are characterized by their values on the diagonal. Indeed, for any $x, y$,
$$C(x, y) = \frac{1}{2} (C(x + y, x + y) - C(x, x) - C(y, y)).$$
We deduce that $C = C'_\mu$.
\end{proof}

\begin{lemma}\label{lem:IsGaussian.ext_iff}
  \uses{def:IsGaussian, def:covInnerBilin}
  \leanok
  \lean{ProbabilityTheory.IsGaussian.ext, ProbabilityTheory.IsGaussian.ext_iff}
Two Gaussian measures $\mu$ and $\nu$ on a separable Hilbert space are equal if and only if they have same mean and same covariance.
\end{lemma}

\begin{proof}\leanok
  \uses{thm:ext_of_charFun, lem:IsGaussian.charFun_eq}
The forward direction is immediate.

For the converse direction, it is enough to show that $\mu$ and $\nu$ have the same characteristic function by Theorem~\ref{thm:ext_of_charFun}. As they are both Gaussian, their characteristic functions only depend on their mean and covariance by Lemma~\ref{lem:IsGaussian.charFun_eq}. Thus they are equal.
\end{proof}


\begin{definition}[Standard Gaussian measure]\label{def:stdGaussian}
  \uses{def:gaussianReal}
  \leanok
  \lean{ProbabilityTheory.stdGaussian}
Let $(e_1, \ldots, e_d)$ be an orthonormal basis of $E$ and let $\mu$ be the standard Gaussian measure on $\mathbb{R}$.
The standard Gaussian measure on $E$ is the pushforward measure of the product measure $\mu \times \ldots \times \mu$ by the map $x \mapsto \sum_{i=1}^d x_i \cdot e_i$.
\end{definition}

The fact that this definition does not depend on the choice of basis will be a consequence of the fact that its characteristic function does not depend on the basis.


\begin{lemma}\label{lem:integral_eval_pi}
  \leanok
  \lean{ProbabilityTheory.integral_eval_pi}
For $\mu_1, \ldots, \mu_d$ probability measures on $\mathbb{R}$ and $f : \mathbb{R} \to \mathbb{R}$ integrable with respect to $\mu_i$, we have
\begin{align*}
  \int_x f(x_i) \, d(\mu_1 \times \ldots \times \mu_d)(x)
  = \int_x f(x) \, d\mu_i
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
As $f$ is integrable, we can use Fubini theorem to obtain that
$$\int f(x_i) \, d(\mu_1 \times \ldots \times \mu_d)(x) = \int f(x) \, d\mu_i(x) \times \prod_{j \ne i} \int 1 \, d\mu_j(x) = \int f(x) \, d\mu_i(x)$$
because the $\mu_j$s are probability measures.
\end{proof}


\begin{lemma}\label{lem:isCentered_stdGaussian}
  \uses{def:stdGaussian}
  \leanok
  \lean{ProbabilityTheory.isCentered_stdGaussian}
The standard Gaussian measure on $E$ is centered, i.e., $\mu[L] = 0$ for every $L \in E^*$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:integral_eval_pi}

\end{proof}


\begin{lemma}\label{lem:isProbabilityMeasure_stdGaussian}
  \uses{def:stdGaussian}
  \leanok
  \lean{ProbabilityTheory.isProbabilityMeasure_stdGaussian}
The standard Gaussian measure is a probability measure.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:charFun_stdGaussian}
  \uses{def:stdGaussian, def:charFun}
  \leanok
  \lean{ProbabilityTheory.charFun_stdGaussian}
The characteristic function of the standard Gaussian measure on $E$ is given by
\begin{align*}
  \hat{\mu}(t) = \exp\left(-\frac{1}{2} \Vert t \Vert^2 \right) \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  \uses{lem:charFun_gaussianReal}
Denote by $\nu$ the standard Gaussian measure on $\mathbb{R}$. This is a straightforward computation:
\begin{align*}
  \hat{\mu}(t) = \int \exp\left(i\langle t, \sum_{j=1}^d x_j \cdot e_j \rangle\right) d(\nu \times \ldots \times \nu)(dx) &= \int \exp\left(\sum_{j=1}^d ix_j\langle t, e_j \rangle\right) d(\nu \times \ldots \times \nu)(dx) \\
  &= \int \prod_{j=1}^d \exp\left(ix_j\langle t, e_j \rangle\right) d(\nu \times \ldots \times \nu)(dx) \\
  &= \prod_{j=1}^d \int \exp\left(ix\langle t, e_j \rangle\right) d\nu(x) \\
  &= \prod_{j=1}^d \exp\left(-\frac{\langle t, e_j \rangle^2}{2}\right) \\
  &= \exp\left(-\frac{1}{2} \Vert t \Vert^2 \right).
\end{align*}
\end{proof}


\begin{lemma}\label{lem:isGaussian_stdGaussian}
  \uses{def:stdGaussian, def:IsGaussian}
  \leanok
  \lean{ProbabilityTheory.isGaussian_stdGaussian}
The standard Gaussian measure on $E$ is a Gaussian measure.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:isGaussian_iff_gaussian_charFun, lem:charFun_stdGaussian, lem:isProbabilityMeasure_stdGaussian}
Since the standard Gaussian is a probability measure (hence finite), we can apply Lemma~\ref{lem:isGaussian_iff_gaussian_charFun} that states that it suffices to show that the characteristic function has a particular form.
That form is given by Lemma~\ref{lem:charFun_stdGaussian}, taking $m=0$ and $C = \langle\cdot, \cdot\rangle$.
\end{proof}


\begin{lemma}\label{lem:integral_id_stdGaussian}
  \uses{def:stdGaussian}
  \leanok
  \lean{ProbabilityTheory.integral_id_stdGaussian}
The mean of the standard Gaussian measure is $0$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:integral_eval_pi}

\end{proof}


\begin{lemma}\label{lem:covMatrix_stdGaussian}
  \uses{def:stdGaussian, def:covMatrix}
  \leanok
  \lean{ProbabilityTheory.covMatrix_stdGaussian}
The covariance matrix of the standard Gaussian measure is the identity matrix.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:isGaussian_iff_gaussian_charFun, lem:charFun_stdGaussian}
From Lemma~\ref{lem:charFun_stdGaussian}, we know that for all $t \in \mathbb{R}$,
$$\hat{\mu}(t) = \exp\left(-\frac{\|t\|^2}{2}\right) = \exp\left(-\frac{\langle t, \mathrm{I}t\rangle}{2}\right).$$
As the identity is positive semidefinite, we deduce from Lemma~\ref{lem:isGaussian_iff_gaussian_charFun} that $\Sigma_\mu$ is the identity matrix.
\end{proof}


\begin{definition}[Multivariate Gaussian]\label{def:multivariateGaussian}
  \uses{def:stdGaussian}
  \leanok
  \lean{ProbabilityTheory.multivariateGaussian}
The multivariate Gaussian measure on $\mathbb{R}^d$ with mean $m \in \mathbb{R}^d$ and covariance matrix $\Sigma \in \mathbb{R}^{d \times d}$, with $\Sigma$ positive semidefinite, is the pushforward measure of the standard Gaussian measure on $\mathbb{R}^d$ by the map $x \mapsto m + \Sigma^{1/2} x$.
We denote this measure by $\mathcal{N}(m, \Sigma)$.
\end{definition}


\begin{lemma}\label{lem:integral_id_multivariateGaussian}
  \uses{def:multivariateGaussian}
  \leanok
  \lean{ProbabilityTheory.integral_id_multivariateGaussian}
The mean of the multivariate Gaussian measure $\mathcal{N}(m, \Sigma)$ is $m$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:integral_id_stdGaussian}

\end{proof}


\begin{lemma}\label{lem:covMatrix_multivariateGaussian}
  \uses{def:multivariateGaussian}
  \leanok
  \lean{ProbabilityTheory.covInnerBilin_multivariateGaussian}
The covariance matrix of the multivariate Gaussian measure $\mathcal{N}(m, \Sigma)$ is $\Sigma$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:covMatrix_stdGaussian}

\end{proof}


\begin{lemma}\label{lem:isGaussian_multivariateGaussian}
  \uses{def:multivariateGaussian, def:IsGaussian}
  \leanok
  \lean{ProbabilityTheory.isGaussian_multivariateGaussian}
A multivariate Gaussian measure is a Gaussian measure.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:isGaussian_stdGaussian, lem:isGaussian_add_const, lem:isGaussian_map}
The multivariate Gaussian measure is the pushforward of the standard Gaussian measure by an affine map, and is thus Gaussian by Lemma~\ref{lem:isGaussian_add_const} and Lemma~\ref{lem:isGaussian_map}.
\end{proof}


\begin{theorem}\label{thm:charFun_multivariateGaussian}
  \uses{def:multivariateGaussian, def:charFun}
  \leanok
  \lean{ProbabilityTheory.charFun_multivariateGaussian}
The characteristic function of a multivariate Gaussian measure $\mathcal{N}(m, \Sigma)$ is given by
\begin{align*}
  \hat{\mu}(t) = \exp\left(i \langle m, t \rangle - \frac{1}{2} \langle t, \Sigma t \rangle\right)
  \: .
\end{align*}
\end{theorem}

\begin{proof}\leanok
  \uses{lem:isGaussian_multivariateGaussian, lem:IsGaussian.charFun_eq, lem:integral_id_multivariateGaussian, lem:covMatrix_multivariateGaussian}
Since the multivariate Gaussian measure is a Gaussian measure, we can apply Lemma~\ref{lem:IsGaussian.charFun_eq} to it.
It suffices then to show that the mean and the covariance matrix of the multivariate Gaussian measure are equal to $m$ and $\Sigma$, respectively.
This is given by Lemma~\ref{lem:integral_id_multivariateGaussian} and Lemma~\ref{lem:covMatrix_multivariateGaussian}.
\end{proof}


\section{Gaussian processes}
\label{sec:gaussian_processes}

\begin{definition}[Gaussian process]\label{def:IsGaussianProcess}
  \uses{def:IsGaussian}
  \leanok
  \lean{ProbabilityTheory.IsGaussianProcess}
A process $X : T \to \Omega \to E$ is Gaussian if for every finite subset $t_1, \ldots, t_n \in T$, the random vector $(X_{t_1}, \ldots, X_{t_n})$ has a Gaussian distribution.
\end{definition}


\begin{lemma}\label{lem:isGaussianProcess_of_modification}
  \uses{def:IsGaussianProcess}
  \leanok
  \lean{ProbabilityTheory.IsGaussianProcess.modification}
Let $X, Y : T \to \Omega \to E$ be two stochastic processes that are modifications of each other (that is, for all $t \in T$, $X_t =_{a.e.} Y_t$).
If $X$ is a Gaussian process, then $Y$ is a Gaussian process as well.
\end{lemma}

\begin{proof}
  \uses{lem:map_eq_of_modification}
Being a Gaussian process is defined in terms of the distribution of finite-dimensional random vectors.
By Lemma~\ref{lem:map_eq_of_modification}, the random vector $(Y_{t_1}, \ldots, Y_{t_n})$ has the same distribution as the random vector $(X_{t_1}, \ldots, X_{t_n})$ for all $t_1, \ldots, t_n \in T$.
\end{proof}
